<!doctype html>



  


<html class="theme-next pisces use-motion">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.0.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Cloud, Container, Kubernetes, SDN, Docker" />





  <link rel="alternate" href="/atom.xml" title="Feisky's Blog" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.0.2" />






<meta name="description" content="Tensorflow图像处理Tensorflow封装了很多图像处理的操作，包括读取图像、图像处理、写图像到文件等等。在批量处理图像时，Tensorflow要求所有的图像都要有相同的Size，即(height,width,channels)。
读取图像%matplotlib inlineimport tensorflow as tfimport numpy as np#mil.use(&apos;svg&apos;)m">
<meta property="og:type" content="website">
<meta property="og:title" content="Tensorflow图像处理">
<meta property="og:url" content="http://feisky.xyz/ml/cnn/images.html">
<meta property="og:site_name" content="Feisky's Blog">
<meta property="og:description" content="Tensorflow图像处理Tensorflow封装了很多图像处理的操作，包括读取图像、图像处理、写图像到文件等等。在批量处理图像时，Tensorflow要求所有的图像都要有相同的Size，即(height,width,channels)。
读取图像%matplotlib inlineimport tensorflow as tfimport numpy as np#mil.use(&apos;svg&apos;)m">
<meta property="og:image" content="http://feisky.xyz/images/14800408340512.jpg">
<meta property="og:image" content="http://feisky.xyz/images/14800417667549.png">
<meta property="og:updated_time" content="2016-12-15T00:44:19.543Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Tensorflow图像处理">
<meta name="twitter:description" content="Tensorflow图像处理Tensorflow封装了很多图像处理的操作，包括读取图像、图像处理、写图像到文件等等。在批量处理图像时，Tensorflow要求所有的图像都要有相同的Size，即(height,width,channels)。
读取图像%matplotlib inlineimport tensorflow as tfimport numpy as np#mil.use(&apos;svg&apos;)m">
<meta name="twitter:image" content="http://feisky.xyz/images/14800408340512.jpg">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"always"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    }
  };
</script>




  <link rel="canonical" href="http://feisky.xyz/ml/cnn/images.html"/>


  <title>
  

  
    Tensorflow图像处理 | Feisky's Blog
  
</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body itemscope itemtype="//schema.org/WebPage" lang="zh-Hans">

  


<!-- hexo-inject:begin --><!-- hexo-inject:end --><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-69699206-1', 'auto');
  ga('send', 'pageview');
</script>









  
  
    
  

  <div class="container one-collumn sidebar-position-left  ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="//schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">Feisky's Blog</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle">Notes about anything.</p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-pages">
          <a href="/pages" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            笔记
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-commonweal">
          <a href="/404.html" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-heartbeat"></i> <br />
            
            公益404
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup">
 <span class="search-icon fa fa-search"></span>
 <input type="text" id="local-search-input">
 <div id="local-search-result"></div>
 <span class="popup-btn-close">close</span>
</div>


    </div>
  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    
    
      <h1 id="Tensorflow图像处理"><a href="#Tensorflow图像处理" class="headerlink" title="Tensorflow图像处理"></a>Tensorflow图像处理</h1><p>Tensorflow封装了很多图像处理的操作，包括读取图像、图像处理、写图像到文件等等。在批量处理图像时，Tensorflow要求所有的图像都要有相同的Size，即<script type="math/tex">(height,width,channels)</script>。</p>
<h2 id="读取图像"><a href="#读取图像" class="headerlink" title="读取图像"></a>读取图像</h2><figure class="highlight python"><table><tr><td class="code"><pre><div class="line">%matplotlib inline</div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="comment">#mil.use('svg')</span></div><div class="line">mil.use(<span class="string">"nbagg"</span>)</div><div class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot</div><div class="line"><span class="keyword">import</span> matplotlib <span class="keyword">as</span> mil</div><div class="line"></div><div class="line">sess = tf.InteractiveSession()</div><div class="line"></div><div class="line">image_filename = <span class="string">"n02113023_219.jpg"</span></div><div class="line">filename_queue = tf.train.string_input_producer(</div><div class="line">    tf.train.match_filenames_once(image_filename))</div><div class="line">image_reader = tf.WholeFileReader()</div><div class="line">_, image_file = image_reader.read(filename_queue)</div><div class="line">image = tf.image.decode_jpeg(image_file)</div><div class="line"></div><div class="line">sess.run(tf.initialize_all_variables())</div><div class="line">coord = tf.train.Coordinator()</div><div class="line">threads = tf.train.start_queue_runners(coord=coord)</div><div class="line"></div><div class="line"><span class="comment"># Rank 3 tensor in format:</span></div><div class="line"><span class="comment"># [batch_size, image_height, image_width, channels]</span></div><div class="line"><span class="keyword">print</span> sess.run(image)</div><div class="line"></div><div class="line">pyplot.imshow(sess.run(image), interpolation=<span class="string">'nearest'</span>)</div><div class="line">filename_queue.close(cancel_pending_enqueues=<span class="keyword">True</span>)</div><div class="line">coord.request_stop()</div><div class="line">coord.join(threads)</div></pre></td></tr></table></figure>
<p><img src="/images/14800408340512.jpg" alt=""></p>
<h2 id="图像格式"><a href="#图像格式" class="headerlink" title="图像格式"></a>图像格式</h2><p>Tensorflow支持JPEG和PNG格式（tf.image.decode_jpeg和tf.image.decode_png）。</p>
<p>ensorFlow has two image formats used to decode image data, one is tf.image.decode_jpeg and the other is tf.image.decode_png. These are common file formats in computer vision applications because they’re trivial to convert other formats to.</p>
<p>Something important to keep in mind, JPEG images don’t store any alpha channel information and PNG images do. This could be important if what you’re training on requires alpha information (transparency). An example usage scenario is one where you’ve manually cut out some pieces of an image, for example, irrelevant jester hats on dogs. Setting those pieces to black would make them seem of similar importance to other black colored items in the image. Setting the removed hat to have an alpha of 0 would help in distinguishing its removal.</p>
<p>When working with JPEG images, don’t manipulate them too much because it’ll leave artifacts. Instead, plan to take raw images and export them to JPEG while doing any manipulation needed. Try to manipulate images before loading them whenever possible to save time in training.</p>
<p>PNG images work well if manipulation is required. PNG format is lossless so it’ll keep all the information from the original file (unless they’ve been resized or downsampled). The downside to PNGs is that the files are larger than their JPEG counterpart.</p>
<h2 id="TFRecord"><a href="#TFRecord" class="headerlink" title="TFRecord"></a>TFRecord</h2><p>TensorFlow has a built-in file format designed to keep binary data and label (category for training) data in the same file. The format is called TFRecord and the format requires a preprocessing step to convert images to a TFRecord format before training. The largest benefit is keeping each input image in the same file as the label associated with it.</p>
<p>Technically, TFRecord files are protobuf formatted files. They are great for use as a preprocessed format because they aren’t compressed and can be loaded into memory quickly. In this example, an image is written to a new TFRecord formatted file and it’s label is stored as well.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><div class="line"><span class="comment"># Reuse the image from earlier and give it a fake label</span></div><div class="line">image_label = <span class="string">b'\x01'</span>  <span class="comment"># Assume the label data is in a one-hot representation (00000001)</span></div><div class="line"></div><div class="line"><span class="comment"># Convert the tensor into bytes, notice that this will load the entire image file</span></div><div class="line">image_loaded = sess.run(image)</div><div class="line">image_bytes = image_loaded.tobytes()</div><div class="line">image_height, image_width, image_channels = image_loaded.shape</div><div class="line"></div><div class="line"><span class="comment"># Export TFRecord</span></div><div class="line">writer = tf.python_io.TFRecordWriter(<span class="string">"training-image.tfrecord"</span>)</div><div class="line"></div><div class="line"><span class="comment"># Don't store the width, height or image channels in this Example file to save space but not required.</span></div><div class="line">example = tf.train.Example(features=tf.train.Features(feature=&#123;</div><div class="line">            <span class="string">'label'</span>: tf.train.Feature(bytes_list=tf.train.BytesList(value=[image_label])),</div><div class="line">            <span class="string">'image'</span>: tf.train.Feature(bytes_list=tf.train.BytesList(value=[image_bytes]))</div><div class="line">        &#125;))</div><div class="line"></div><div class="line"><span class="comment"># This will save the example to a text file tfrecord</span></div><div class="line">writer.write(example.SerializeToString())</div><div class="line">writer.close()</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><div class="line"><span class="comment"># Load TFRecord</span></div><div class="line">tf_record_filename_queue = tf.train.string_input_producer(</div><div class="line">    tf.train.match_filenames_once(<span class="string">"training-image.tfrecord"</span>))</div><div class="line"></div><div class="line"><span class="comment"># Notice the different record reader, this one is designed to work with TFRecord files which may</span></div><div class="line"><span class="comment"># have more than one example in them.</span></div><div class="line">tf_record_reader = tf.TFRecordReader()</div><div class="line">_, tf_record_serialized = tf_record_reader.read(tf_record_filename_queue)</div><div class="line"></div><div class="line"><span class="comment"># The label and image are stored as bytes but could be stored as int64 or float64 values in a</span></div><div class="line"><span class="comment"># serialized tf.Example protobuf.</span></div><div class="line">tf_record_features = tf.parse_single_example(</div><div class="line">    tf_record_serialized,</div><div class="line">    features=&#123;</div><div class="line">        <span class="string">'label'</span>: tf.FixedLenFeature([], tf.string),</div><div class="line">        <span class="string">'image'</span>: tf.FixedLenFeature([], tf.string),</div><div class="line">    &#125;)</div><div class="line"></div><div class="line"><span class="comment"># Using tf.uint8 because all of the channel information is between 0-255</span></div><div class="line">tf_record_image = tf.decode_raw(</div><div class="line">    tf_record_features[<span class="string">'image'</span>], tf.uint8)</div><div class="line"></div><div class="line"><span class="comment"># Reshape the image to look like the image saved, not required</span></div><div class="line">tf_record_image = tf.reshape(</div><div class="line">    tf_record_image,</div><div class="line">    [image_height, image_width, image_channels])</div><div class="line"><span class="comment"># Use real values for the height, width and channels of the image because it's required</span></div><div class="line"><span class="comment"># to reshape the input.</span></div><div class="line"></div><div class="line">tf_record_label = tf.cast(tf_record_features[<span class="string">'label'</span>], tf.string)</div><div class="line"></div><div class="line"><span class="comment"># Check that the image saved to disk is the same as the image which was loaded from TensorFlow.</span></div><div class="line">sess.close()</div><div class="line">sess = tf.InteractiveSession()</div><div class="line">sess.run(tf.initialize_all_variables())</div><div class="line">coord = tf.train.Coordinator()</div><div class="line">threads = tf.train.start_queue_runners(coord=coord)</div><div class="line"><span class="keyword">print</span> sess.run(tf.reduce_mean(tf.cast(tf.equal(tf_record_image, image),</div><div class="line">                                           tf.float32))) <span class="comment"># =&gt; 1.0</span></div><div class="line">tf_record_filename_queue.close(cancel_pending_enqueues=<span class="keyword">True</span>)</div><div class="line">coord.request_stop()</div><div class="line">coord.join(threads)</div></pre></td></tr></table></figure>
<h2 id="图像操作"><a href="#图像操作" class="headerlink" title="图像操作"></a>图像操作</h2><p>CNNs work well when they’re given a large amount of diverse quality training data. Images capture complex scenes in a way which visually communicates an intended subject. In the Stanford Dog’s Dataset, it’s important that the images visually highlight the importance of dogs in the picture. A picture with a dog clearly visible in the center is considered more valuable than one with a dog in the background.</p>
<p>Not all datasets have the most valuable images. The following are two images from the <a href="http://vision.stanford.edu/aditya86/ImageNetDogs/" target="_blank" rel="external">Stanford Dogs Dataset</a> which are supposed to highlight dog breeds. The image on the left <code>n02113978_3480.jpg</code> highlights important attributes of a typical Mexican Hairless Dog, while the image on the right <code>n02113978_1030.jpg</code> highlights the look of inebriated party goers scaring a Mexican Hairless Dog. The image on the right <code>n02113978_1030.jpg</code> is filled with irrelevant information which may train a CNN to categorize party goer faces instead of Mexican Hairless Dog breeds. Images like this may still include an image of a dog and could be manipulated to highlight the dog instead of people.</p>
<p><img src="/images/14800417667549.png" alt=""></p>
<p>Image manipulation is best done as a preprocessing step in most scenarios. An image can be cropped, resized and the color levels adjusted. On the other hand, there is an important use case for manipulating an image while training. After an image is loaded, it can be flipped or distorted to diversify the input training information used with the network. This step adds further processing time but helps with overfitting.</p>
<p>TensorFlow is not designed as an image manipulation framework. There are libraries available in Python which support more image manipulation than TensorFlow (<a href="http://www.pythonware.com/products/pil/" target="_blank" rel="external">PIL</a> and <a href="http://docs.opencv.org/3.0-beta/doc/py_tutorials/py_tutorials.html" target="_blank" rel="external">OpenCV</a>). For TensorFlow, we’ll summarize a few useful image manipulation features available which are useful in training CNNs.</p>
<h4 id="Cropping"><a href="#Cropping" class="headerlink" title="Cropping"></a>Cropping</h4><p>Cropping an image will remove certain regions of the image without keeping any information. Cropping is similar to <code>tf.slice</code> where a section of a tensor is cut out from the full tensor. Cropping an input image for a CNN can be useful if there is extra input along a dimension which isn’t required. For example, cropping dog pictures where the dog is in the center of the images to reduce the size of the input.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><div class="line">sess.run(tf.image.central_crop(image, <span class="number">0.1</span>))</div></pre></td></tr></table></figure>
<pre><code>array([[[  3, 108, 233]]], dtype=uint8)
</code></pre><p>The example code uses <code>tf.image.central_crop</code> to crop out 10% of the image and return it. This method always returns based on the center of the image being used.</p>
<p>Cropping is usually done in preprocessing but it can be useful when training if the background is useful. When the background is useful then cropping can be done while randomizing the center offset of where the crop begins.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><div class="line"><span class="comment"># This crop method only works on real value input.</span></div><div class="line">real_image = sess.run(image)</div><div class="line"></div><div class="line">bounding_crop = tf.image.crop_to_bounding_box(</div><div class="line">    real_image, offset_height=<span class="number">0</span>, offset_width=<span class="number">0</span>, target_height=<span class="number">2</span>, target_width=<span class="number">1</span>)</div><div class="line"></div><div class="line">sess.run(bounding_crop)</div></pre></td></tr></table></figure>
<pre><code>array([[[  0,   0,   0]],

       [[  0, 191,   0]]], dtype=uint8)
</code></pre><p>The example code uses <code>tf.image.crop_to_bounding_box</code> in order to crop the image starting at the upper left pixel located at <code>(0, 0)</code>. Currently, the function only works with a tensor which has a defined shape so an input image needs to be executed on the graph first.</p>
<h4 id="Padding"><a href="#Padding" class="headerlink" title="Padding"></a>Padding</h4><p>Pad an image with zeros in order to make it the same size as an expected image. This can be accomplished using <code>tf.pad</code> but TensorFlow has another function useful for resizing images which are too large or too small. The method will pad an image which is too small including zeros along the edges of the image. Often, this method is used to resize small images because any other method of resizing with distort the image.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><div class="line"><span class="comment"># This padding method only works on real value input.</span></div><div class="line">real_image = sess.run(image)</div><div class="line"></div><div class="line">pad = tf.image.pad_to_bounding_box(</div><div class="line">    real_image, offset_height=<span class="number">0</span>, offset_width=<span class="number">0</span>, target_height=<span class="number">4</span>, target_width=<span class="number">4</span>)</div><div class="line"></div><div class="line">sess.run(pad)</div></pre></td></tr></table></figure>
<pre><code>array([[[  0,   0,   0],
        [255, 255, 255],
        [254,   0,   0],
        [  0,   0,   0]],

       [[  0, 191,   0],
        [  3, 108, 233],
        [  0, 191,   0],
        [  0,   0,   0]],

       [[254,   0,   0],
        [255, 255, 255],
        [  0,   0,   0],
        [  0,   0,   0]],

       [[  0,   0,   0],
        [  0,   0,   0],
        [  0,   0,   0],
        [  0,   0,   0]]], dtype=uint8)
</code></pre><p>This example code increases the images height by one pixel and its width by a pixel as well. The new pixels are all set to 0. Padding in this manner is useful for scaling up an image which is too small. This can happen if there are images in the training set with a mix of aspect ratios. TensorFlow has a useful shortcut for resizing images which don’t match the same aspect ratio using a combination of <code>pad</code> and <code>crop</code>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><div class="line"><span class="comment"># This padding method only works on real value input.</span></div><div class="line">real_image = sess.run(image)</div><div class="line"></div><div class="line">crop_or_pad = tf.image.resize_image_with_crop_or_pad(</div><div class="line">    real_image, target_height=<span class="number">2</span>, target_width=<span class="number">5</span>)</div><div class="line"></div><div class="line">sess.run(crop_or_pad)</div></pre></td></tr></table></figure>
<pre><code>array([[[  0,   0,   0],
        [  0,   0,   0],
        [255, 255, 255],
        [254,   0,   0],
        [  0,   0,   0]],

       [[  0,   0,   0],
        [  0, 191,   0],
        [  3, 108, 233],
        [  0, 191,   0],
        [  0,   0,   0]]], dtype=uint8)
</code></pre><p>The <code>real_image</code> has been reduced in height to be 2 pixels tall and the width has been increased by padding the image with zeros. This function works based on the center of the image input.</p>
<h4 id="Flipping"><a href="#Flipping" class="headerlink" title="Flipping"></a>Flipping</h4><p>Flipping an image is exactly what it sounds like. Each pixel’s location is reversed horizontally or vertically. Technically speaking, flopping is the term used when flipping an image vertically. Terms aside, flipping images is useful with TensorFlow to give different perspectives of the same image for training. For example, a picture of an Australian Shepherd with crooked left ear could be flipped in order to allow matching of crooked right ears.</p>
<p>TensorFlow has functions to flip images vertically, horizontally and choose randomly. The ability to randomly flip an image is a useful method to keep from overfitting a model to flipped versions of images.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><div class="line">top_left_pixels = tf.slice(image, [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>])</div><div class="line"></div><div class="line">flip_horizon = tf.image.flip_left_right(top_left_pixels)</div><div class="line">flip_vertical = tf.image.flip_up_down(flip_horizon)</div><div class="line"></div><div class="line">sess.run([top_left_pixels, flip_vertical])</div></pre></td></tr></table></figure>
<pre><code>[array([[[  0,   0,   0],
         [255, 255, 255]],

        [[  0, 191,   0],
         [  3, 108, 233]]], dtype=uint8), array([[[  3, 108, 233],
         [  0, 191,   0]],

        [[255, 255, 255],
         [  0,   0,   0]]], dtype=uint8)]
</code></pre><p>This example code flips a subset of the image horizontally and then vertically. The subset is used with <code>tf.slice</code> because the original image flipped returns the same images (for this example only). The subset of pixels illustrates the change which occurs when an image is flipped. <code>tf.image.flip_left_right</code> and <code>tf.image.flip_up_down</code> both operate on tensors which are not limited to images. These will flip an image a single time, randomly flipping an image is done using a separate set of functions.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><div class="line">top_left_pixels = tf.slice(image, [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>])</div><div class="line"></div><div class="line">random_flip_horizon = tf.image.random_flip_left_right(top_left_pixels)</div><div class="line">random_flip_vertical = tf.image.random_flip_up_down(random_flip_horizon)</div><div class="line"></div><div class="line">sess.run(random_flip_vertical)</div></pre></td></tr></table></figure>
<pre><code>array([[[  3, 108, 233],
        [  0, 191,   0]],

       [[255, 255, 255],
        [  0,   0,   0]]], dtype=uint8)
</code></pre><p>This example does the same logic as the example before except that the output is random. Every time this runs, a different output is expected. There is a parameter named <code>seed</code> which may be used to control how random the flipping occurs.</p>
<h4 id="Saturation-and-Balance"><a href="#Saturation-and-Balance" class="headerlink" title="Saturation and Balance"></a>Saturation and Balance</h4><p>Images which are found on the internet are often edited in advance. For instance, many of the images found in the Stanford Dogs dataset have too much saturation (lots of color). When an edited image is used for training, it may mislead a CNN model into finding patterns which are related to the edited image and not the content in the image.</p>
<p>TensorFlow has useful functions which help in training on images by changing the saturation, hue, contrast and brightness. The functions allow for simple manipulation of these image attributes as well as randomly altering these attributes. The random altering is useful in training in for the same reason randomly flipping an image is useful. The random attribute changes help a CNN be able to accurately match a feature in images which have been edited or were taken under different lighting.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><div class="line">example_red_pixel = tf.constant([<span class="number">254.</span>, <span class="number">2.</span>, <span class="number">15.</span>])</div><div class="line">adjust_brightness = tf.image.adjust_brightness(example_red_pixel, <span class="number">0.2</span>)</div><div class="line"></div><div class="line">sess.run(adjust_brightness)</div></pre></td></tr></table></figure>
<pre><code>array([ 254.19999695,    2.20000005,   15.19999981], dtype=float32)
</code></pre><p>This example brightens a single pixel, which is primarily red, with a delta of <code>0.2</code>. Unfortunately, in the current version of TensorFlow 0.8, this method doesn’t work well with a <code>tf.uint8</code> input. It’s best to avoid using this when possible and preprocess brightness changes.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><div class="line">adjust_contrast = tf.image.adjust_contrast(image, <span class="number">-.5</span>)</div><div class="line"></div><div class="line">sess.run(tf.slice(adjust_contrast, [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>]))</div></pre></td></tr></table></figure>
<pre><code>array([[[170,  71, 124],
        [168, 112,   7],
        [170,  71, 124]]], dtype=uint8)
</code></pre><p>The example code changes the contrast by <code>-0.5</code> which makes the new version of the image fairly unrecognizable. Adjusting contrast is best done in small increments to keep from blowing out an image. Blowing out an image means the same thing as saturating a neuron, it reached its maximum value and can’t be recovered. With contrast changes, an image can become completely white and completely black from the same adjustment.</p>
<p>The <code>tf.slice</code> operation is for brevity, highlighting one of the pixels which has changed. It is not required when running this operation.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><div class="line">adjust_hue = tf.image.adjust_hue(image, <span class="number">0.7</span>)</div><div class="line"></div><div class="line">sess.run(tf.slice(adjust_hue, [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>]))</div></pre></td></tr></table></figure>
<pre><code>array([[[191,  38,   0],
        [ 62, 233,   3],
        [191,  38,   0]]], dtype=uint8)
</code></pre><p>The example code adjusts the hue found in the image to make it more colorful. The adjustment accepts a <code>delta</code> parameter which controls the amount of hue to adjust in the image.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><div class="line">adjust_saturation = tf.image.adjust_saturation(image, <span class="number">0.4</span>)</div><div class="line"></div><div class="line">sess.run(tf.slice(adjust_saturation, [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>]))</div></pre></td></tr></table></figure>
<pre><code>array([[[114, 191, 114],
        [141, 183, 233],
        [114, 191, 114]]], dtype=uint8)
</code></pre><p>The code is similar to adjusting the contrast. It is common to oversaturate an image in order to identify edges because the increased saturation highlights changes in colors.</p>
<h3 id="Colors"><a href="#Colors" class="headerlink" title="Colors"></a>Colors</h3><p>CNNs are commonly trained using images with a single color. When an image has a single color it is said to use a grayscale colorspace meaning it uses a single channel of colors. For most computer vision related tasks, using grayscale is reasonable because the shape of an image can be seen without all the colors. The reduction in colors equates to a quicker to train image. Instead of a 3 component rank 1 tensor to describe each color found with RGB, a grayscale image requires a single component rank 1 tensor to describe the amount of gray found in the image.</p>
<p>Although grayscale has benefits, it’s important to consider applications which require a distinction based on color. Color in images is challenging to work with in most computer vision because it isn’t easy to mathematically define the similarity of two RGB colors. In order to use colors in CNN training, it’s useful to convert the colorspace the image is natively in.</p>
<h4 id="Grayscale"><a href="#Grayscale" class="headerlink" title="Grayscale"></a>Grayscale</h4><p>Grayscale has a single component to it and has the same range of color as RGB <span class="math-tex" data-type="tex">\([0, 255]\)</span>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><div class="line">gray = tf.image.rgb_to_grayscale(image)</div><div class="line"></div><div class="line">sess.run(tf.slice(gray, [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>]))</div></pre></td></tr></table></figure>
<pre><code>array([[[  0],
        [255],
        [ 76]]], dtype=uint8)
</code></pre><p>This example converted the RGB image into grayscale. The <code>tf.slice</code> operation took the top row of pixels out to investigate how their color has changed. The grayscale conversion is done by averaging all the color values for a pixel and setting the amount of grayscale to be the average.</p>
<h4 id="HSV"><a href="#HSV" class="headerlink" title="HSV"></a>HSV</h4><p>Hue, saturation and value are what make up HSV colorspace. This space is represented with a 3 component rank 1 tensor similar to RGB. HSV is not similar to RGB in what it measures, it’s measuring attributes of an image which are closer to human perception of color than RGB. It is sometimes called HSB, where the B stands for brightness.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><div class="line">hsv = tf.image.rgb_to_hsv(tf.image.convert_image_dtype(image, tf.float32))</div><div class="line"></div><div class="line">sess.run(tf.slice(hsv, [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>]))</div></pre></td></tr></table></figure>
<pre><code>array([[[ 0.        ,  0.        ,  0.        ],
        [ 0.        ,  0.        ,  1.        ],
        [ 0.        ,  1.        ,  0.99607849]],

       [[ 0.33333334,  1.        ,  0.74901962],
        [ 0.59057975,  0.98712444,  0.91372555],
        [ 0.33333334,  1.        ,  0.74901962]],

       [[ 0.        ,  1.        ,  0.99607849],
        [ 0.        ,  0.        ,  1.        ],
        [ 0.        ,  0.        ,  0.        ]]], dtype=float32)
</code></pre><h4 id="RGB"><a href="#RGB" class="headerlink" title="RGB"></a>RGB</h4><p>RGB is the colorspace which has been used in all the example code so far. It’s broken up into a 3 component rank 1 tensor which includes the amount of red [0, 255], green [0, 255] and blue [0, 255]. Most images are already in RGB but TensorFlow has builtin functions in case the images are in another colorspace.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><div class="line">rgb_hsv = tf.image.hsv_to_rgb(hsv)</div><div class="line">rgb_grayscale = tf.image.grayscale_to_rgb(gray)</div></pre></td></tr></table></figure>
<p>The example code is straightforward except that the conversion from grayscale to RGB doesn’t make much sense. RGB expects three colors while grayscale only has one. When the conversion occurs, the RGB values are filled with the same value which is found in the grayscale pixel.</p>
<h4 id="Lab"><a href="#Lab" class="headerlink" title="Lab"></a>Lab</h4><p>Lab is not a colorspace which TensorFlow has native support for. It’s a useful colorspace because it can map to a larger number of perceivable colors than RGB. Although TensorFlow doesn’t support this natively, it is a colorspace which is often used in professional settings. Another Python library <a href="http://python-colormath.readthedocs.io/en/latest/" target="_blank" rel="external">python-colormath</a> has support for Lab conversion as well as other colorspaces not described here.</p>
<p>The largest benefit using a Lab colorspace is it maps closer to humans perception of the difference in colors than RGB or HSV. The euclidean distance between two colors in a Lab colorspace are somewhat representative of how different the colors look to a human.</p>
<h2 id="Casting-Images"><a href="#Casting-Images" class="headerlink" title="Casting Images"></a>Casting Images</h2><p>In these examples, <code>tf.to_float</code> is often used in order to illustrate changing an image’s type to another format. For examples, this works OK but TensorFlow has a built in function to properly scale values as they change types. <code>tf.image.convert_image_dtype(image, dtype, saturate=False)</code> is a useful shortcut to change the type of an image from <code>tf.uint8</code> to <code>tf.float</code>.</p>

    
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="//schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/qrcode.jpg"
               alt="Feisky" />
          <p class="site-author-name" itemprop="name">Feisky</p>
          <p class="site-description motion-element" itemprop="description">Notes about anything.</p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">93</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          
            <div class="site-state-item site-state-categories">
              <a href="/categories">
                <span class="site-state-item-count">14</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">29</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/feiskyer" target="_blank" title="Github">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                  Github
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://twitter.com/feisky" target="_blank" title="Twitter">
                  
                    <i class="fa fa-fw fa-twitter"></i>
                  
                  Twitter
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://weibo.com/371069890" target="_blank" title="Weibo">
                  
                    <i class="fa fa-fw fa-weibo"></i>
                  
                  Weibo
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://www.cnblogs.com/feisky/" target="_blank" title="博客园">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                  博客园
                </a>
              </span>
            
          
        </div>

        
        

        
        

      </section>

      
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">
            
              
            
            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Tensorflow图像处理"><span class="nav-number">1.</span> <span class="nav-text">Tensorflow图像处理</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#读取图像"><span class="nav-number">1.1.</span> <span class="nav-text">读取图像</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#图像格式"><span class="nav-number">1.2.</span> <span class="nav-text">图像格式</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#TFRecord"><span class="nav-number">1.3.</span> <span class="nav-text">TFRecord</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#图像操作"><span class="nav-number">1.4.</span> <span class="nav-text">图像操作</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Cropping"><span class="nav-number">1.4.0.1.</span> <span class="nav-text">Cropping</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Padding"><span class="nav-number">1.4.0.2.</span> <span class="nav-text">Padding</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Flipping"><span class="nav-number">1.4.0.3.</span> <span class="nav-text">Flipping</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Saturation-and-Balance"><span class="nav-number">1.4.0.4.</span> <span class="nav-text">Saturation and Balance</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Colors"><span class="nav-number">1.4.1.</span> <span class="nav-text">Colors</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Grayscale"><span class="nav-number">1.4.1.1.</span> <span class="nav-text">Grayscale</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#HSV"><span class="nav-number">1.4.1.2.</span> <span class="nav-text">HSV</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#RGB"><span class="nav-number">1.4.1.3.</span> <span class="nav-text">RGB</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Lab"><span class="nav-number">1.4.1.4.</span> <span class="nav-text">Lab</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Casting-Images"><span class="nav-number">1.5.</span> <span class="nav-text">Casting Images</span></a></li></ol></li></ol></div>
            
          </div>
        </section>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2016</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Feisky</span>
</div>

<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Pisces
  </a>
</div>

        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.0.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.0.2"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.0.2"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.0.2"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.2"></script>



  



  




  
  
  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length == 0) {
       search_path = "search.xml";
    }
    var path = "/" + search_path;
    // monitor main search box;

    function proceedsearch() {
      $("body").append('<div class="popoverlay">').css('overflow', 'hidden');
      $('.popup').toggle();

    }
    // search function;
    var searchFunc = function(path, search_id, content_id) {
    'use strict';
    $.ajax({
        url: path,
        dataType: "xml",
        async: true,
        success: function( xmlResponse ) {
            // get the contents from search data
            isfetched = true;
            $('.popup').detach().appendTo('.header-inner');
            var datas = $( "entry", xmlResponse ).map(function() {
                return {
                    title: $( "title", this ).text(),
                    content: $("content",this).text(),
                    url: $( "url" , this).text()
                };
            }).get();
            var $input = document.getElementById(search_id);
            var $resultContent = document.getElementById(content_id);
            $input.addEventListener('input', function(){
                var matchcounts = 0;
                var str='<ul class=\"search-result-list\">';
                var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                $resultContent.innerHTML = "";
                if (this.value.trim().length > 1) {
                // perform local searching
                datas.forEach(function(data) {
                    var isMatch = false;
                    var content_index = [];
                    var data_title = data.title.trim().toLowerCase();
                    var data_content = data.content.trim().replace(/<[^>]+>/g,"").toLowerCase();
                    var data_url = decodeURIComponent(data.url);
                    var index_title = -1;
                    var index_content = -1;
                    var first_occur = -1;
                    // only match artiles with not empty titles and contents
                    if(data_title != '') {
                        keywords.forEach(function(keyword, i) {
                            index_title = data_title.indexOf(keyword);
                            index_content = data_content.indexOf(keyword);
                            if( index_title >= 0 || index_content >= 0 ){
                                isMatch = true;
								if (i == 0) {
                                    first_occur = index_content;
                                }
                            } 
							
                        });
                    }
                    // show search results
                    if (isMatch) {
                        matchcounts += 1;
                        str += "<li><a href='"+ data_url +"' class='search-result-title'>"+ data_title +"</a>";
                        var content = data.content.trim().replace(/<[^>]+>/g,"");
                        if (first_occur >= 0) {
                            // cut out 100 characters
                            var start = first_occur - 20;
                            var end = first_occur + 80;
                            if(start < 0){
                                start = 0;
                            }
                            if(start == 0){
                                end = 50;
                            }
                            if(end > content.length){
                                end = content.length;
                            }
                            var match_content = content.substring(start, end);
                            // highlight all keywords
                            keywords.forEach(function(keyword){
                                var regS = new RegExp(keyword, "gi");
                                match_content = match_content.replace(regS, "<b class=\"search-keyword\">"+keyword+"</b>");
                            });

                            str += "<p class=\"search-result\">" + match_content +"...</p>"
                        }
                        str += "</li>";
                    }
                })};
                str += "</ul>";
                if (matchcounts == 0) { str = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>' }
                if (keywords == "") { str = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>' }
                $resultContent.innerHTML = str;
            });
            proceedsearch();
        }
    });}

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched == false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };

    });

    $('.popup-btn-close').click(function(e){
      $('.popup').hide();
      $(".popoverlay").remove();
      $('body').css('overflow', '');
    });
    $('.popup').click(function(e){
      e.stopPropagation();
    });
  </script>


  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
  </script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for (i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
    });
  </script>
  <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->


  


</body>
</html>
