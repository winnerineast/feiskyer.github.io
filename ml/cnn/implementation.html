<!doctype html>



  


<html class="theme-next pisces use-motion">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.0.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Cloud, Container, Kubernetes, SDN, Docker" />





  <link rel="alternate" href="/atom.xml" title="Feisky's Blog" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.0.2" />






<meta name="description" content="CNN ImplementationCNN ImplementationObject recognition and categorization using TensorFlow required a basic understanding of convolutions (for CNNs), common layers (non-linearity, pooling, fc), image">
<meta property="og:type" content="website">
<meta property="og:title" content="Feisky's Blog">
<meta property="og:url" content="http://feisky.xyz/ml/cnn/implementation.html">
<meta property="og:site_name" content="Feisky's Blog">
<meta property="og:description" content="CNN ImplementationCNN ImplementationObject recognition and categorization using TensorFlow required a basic understanding of convolutions (for CNNs), common layers (non-linearity, pooling, fc), image">
<meta property="og:image" content="http://feisky.xyz/images/14800437906342.png">
<meta property="og:image" content="http://feisky.xyz/images/14800438370921.png">
<meta property="og:image" content="http://feisky.xyz/images/14800438610335.png">
<meta property="og:updated_time" content="2016-11-29T00:06:54.301Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Feisky's Blog">
<meta name="twitter:description" content="CNN ImplementationCNN ImplementationObject recognition and categorization using TensorFlow required a basic understanding of convolutions (for CNNs), common layers (non-linearity, pooling, fc), image">
<meta name="twitter:image" content="http://feisky.xyz/images/14800437906342.png">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"always"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    }
  };
</script>




  <link rel="canonical" href="http://feisky.xyz/ml/cnn/implementation.html"/>


  <title>
  

  
     | Feisky's Blog
  
</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body itemscope itemtype="//schema.org/WebPage" lang="zh-Hans">

  


<!-- hexo-inject:begin --><!-- hexo-inject:end --><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-69699206-1', 'auto');
  ga('send', 'pageview');
</script>









  
  
    
  

  <div class="container one-collumn sidebar-position-left  ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="//schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">Feisky's Blog</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle">Notes about anything.</p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-pages">
          <a href="/pages" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            笔记
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-commonweal">
          <a href="/404.html" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-heartbeat"></i> <br />
            
            公益404
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup">
 <span class="search-icon fa fa-search"></span>
 <input type="text" id="local-search-input">
 <div id="local-search-result"></div>
 <span class="popup-btn-close">close</span>
</div>


    </div>
  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    
    
      <h1 id="CNN-Implementation"><a href="#CNN-Implementation" class="headerlink" title="CNN Implementation"></a>CNN Implementation</h1><h2 id="CNN-Implementation-1"><a href="#CNN-Implementation-1" class="headerlink" title="CNN Implementation"></a>CNN Implementation</h2><p>Object recognition and categorization using TensorFlow required a basic understanding of convolutions (for CNNs), common layers (non-linearity, pooling, fc), image loading, image manipulation and colorspaces. With these areas covered, it’s possible to build a CNN model for image recognition and classification using TensorFlow. In this case, the model is a dataset provided by Stanford which includes pictures of dogs and their corresponding breed. The network needs to train on these pictures then be judged on how well it can guess a dog’s breed based on a picture.</p>
<p>The network architecture follows a simplified version of <a href="https://code.google.com/p/cuda-convnet/" target="_blank" rel="external">Alex Krizhevsky’s AlexNet</a> without all of AlexNet’s layers. This architecture was described earlier in the chapter as the network which won ILSVRC’12 top challenge. The network uses layers and techniques familiar to this chapter which are similar to the <a href="https://www.tensorflow.org/versions/r0.8/tutorials/deep_cnn/index.html" target="_blank" rel="external">TensorFlow provided</a> tutorial on CNNs.</p>
<p><img src="/images/14800437906342.png" alt=""></p>
<p></p><p style="text-align: center;"><i>The network described in this section including the output TensorShape after each layer. The layers are read from left to right and top to bottom where related layers are grouped together. As the input progresses further into the network, its height and width are reduced while its depth is increased. The increase in depth reduces the computation required to use the network.</i></p><br><br><p></p>
<h3 id="Stanford-Dogs-Dataset"><a href="#Stanford-Dogs-Dataset" class="headerlink" title="Stanford Dogs Dataset"></a>Stanford Dogs Dataset</h3><p>The dataset used for training this model can be found on Stanford’s computer vision site <a href="http://vision.stanford.edu/aditya86/ImageNetDogs/" target="_blank" rel="external">http://vision.stanford.edu/aditya86/ImageNetDogs/</a>. Training the model requires downloading relevant data. After downloading the Zip archive of all the images, extract the archive into a new directory called <code>imagenet-dogs</code> in the same directory as the code building the model.</p>
<p>The Zip archive provided by Stanford includes pictures of dogs organized into 120 different breeds. The goal of this model is to train on 80% of the dog breed images and then test using the remaining 20%. If this were a production model, part of the raw data would be reserved for cross-validation of the results. Cross-validation is a useful step to validate the accuracy of a model but this model is designed to illustrate the process and not for competition.</p>
<p>The organization of the archive follows ImageNet’s practices. Each dog breed is a directory name similar to <code>n02085620-Chihuahua</code> where the second half of the directory name is the dog’s breed in English (<code>Chihuahua</code>). Within each directory there is a variable amount of images related to that breed. Each image is in JPEG format (RGB) and of varying sizes. The different sized images is a challenge because TensorFlow is expecting tensors of the same dimensionality.</p>
<h3 id="Convert-Images-to-TFRecords"><a href="#Convert-Images-to-TFRecords" class="headerlink" title="Convert Images to TFRecords"></a>Convert Images to TFRecords</h3><p>The raw images organized in a directory doesn’t work well for training because the images are not of the same size and their dog breed isn’t included in the file. Converting the images into TFRecord files in advance of training will help keep training fast and simplify matching the label of the image. Another benefit is that the training and testing related images can be separated in advance. Separated training and testing datasets allows continual testing of a model while training is occurring using checkpoint files.</p>
<p>Converting the images will require changing their colorspace into grayscale, resizing the images to be of uniform size and attaching the label to each image. This conversion should only happen once before training commences and likely will take a long time.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><div class="line"><span class="comment"># setup-only-ignore</span></div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"></div><div class="line">sess = tf.InteractiveSession()</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><div class="line"><span class="keyword">import</span> glob</div><div class="line"></div><div class="line">image_filenames = glob.glob(<span class="string">"./imagenet-dogs/n02*/*.jpg"</span>)</div><div class="line"></div><div class="line">image_filenames[<span class="number">0</span>:<span class="number">2</span>]</div></pre></td></tr></table></figure>
<pre><code>[&#39;./imagenet-dogs/n02085620-Chihuahua/n02085620_10074.jpg&#39;,
 &#39;./imagenet-dogs/n02085620-Chihuahua/n02085620_10131.jpg&#39;]
</code></pre><p>An example of how the archive is organized. The <code>glob</code> module allows directory listing which shows the structure of the files which exist in the dataset. The eight digit number is tied to the <a href="http://wordnet.princeton.edu/wordnet/documentation/" target="_blank" rel="external">WordNet ID</a> of each category used in ImageNet. ImageNet has a browser for image details which accepts the WordNet ID, for example the Chihuahua example can be accessed via <a href="http://www.image-net.org/synset?wnid=n02085620" target="_blank" rel="external">http://www.image-net.org/synset?wnid=n02085620</a>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><div class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> groupby</div><div class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</div><div class="line"></div><div class="line">training_dataset = defaultdict(list)</div><div class="line">testing_dataset = defaultdict(list)</div><div class="line"></div><div class="line"><span class="comment"># Split up the filename into its breed and corresponding filename. The breed is found by taking the directory name</span></div><div class="line">image_filename_with_breed = map(<span class="keyword">lambda</span> filename: (filename.split(<span class="string">"/"</span>)[<span class="number">2</span>], filename), image_filenames)</div><div class="line"></div><div class="line"><span class="comment"># Group each image by the breed which is the 0th element in the tuple returned above</span></div><div class="line"><span class="keyword">for</span> dog_breed, breed_images <span class="keyword">in</span> groupby(image_filename_with_breed, <span class="keyword">lambda</span> x: x[<span class="number">0</span>]):</div><div class="line">    <span class="comment"># Enumerate each breed's image and send ~20% of the images to a testing set</span></div><div class="line">    <span class="keyword">for</span> i, breed_image <span class="keyword">in</span> enumerate(breed_images):</div><div class="line">        <span class="keyword">if</span> i % <span class="number">5</span> == <span class="number">0</span>:</div><div class="line">            testing_dataset[dog_breed].append(breed_image[<span class="number">1</span>])</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            training_dataset[dog_breed].append(breed_image[<span class="number">1</span>])</div><div class="line"></div><div class="line">    <span class="comment"># Check that each breed includes at least 18% of the images for testing</span></div><div class="line">    breed_training_count = len(training_dataset[dog_breed])</div><div class="line">    breed_testing_count = len(testing_dataset[dog_breed])</div><div class="line"></div><div class="line">    <span class="keyword">assert</span> round(breed_testing_count / (breed_training_count + breed_testing_count), <span class="number">2</span>) &gt; <span class="number">0.18</span>, <span class="string">"Not enough testing images."</span></div></pre></td></tr></table></figure>
<p>This example code organized the directory and images (‘./imagenet-dogs/n02085620-Chihuahua/n02085620_10131.jpg’) into two dictionaries related to each breed including all the images for that breed. Now each dictionary would include Chihuahua images in the following format:</p>
<p><code>training_dataset[&quot;n02085620-Chihuahua&quot;] = [&quot;n02085620_10131.jpg&quot;, ...]</code></p>
<p>Organizing the breeds into these dictionaries simplifies the process of selecting each type of image and categorizing it. During preprocessing, all the image breeds can be iterated over and their images opened based on the filenames in the list.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">write_records_file</span><span class="params">(dataset, record_location)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Fill a TFRecords file with the images found in `dataset` and include their category.</div><div class="line"></div><div class="line">    Parameters</div><div class="line">    ----------</div><div class="line">    dataset : dict(list)</div><div class="line">      Dictionary with each key being a label for the list of image filenames of its value.</div><div class="line">    record_location : str</div><div class="line">      Location to store the TFRecord output.</div><div class="line">    """</div><div class="line">    writer = <span class="keyword">None</span></div><div class="line"></div><div class="line">    <span class="comment"># Enumerating the dataset because the current index is used to breakup the files if they get over 100</span></div><div class="line">    <span class="comment"># images to avoid a slowdown in writing.</span></div><div class="line">    current_index = <span class="number">0</span></div><div class="line">    <span class="keyword">for</span> breed, images_filenames <span class="keyword">in</span> dataset.items():</div><div class="line">        <span class="keyword">for</span> image_filename <span class="keyword">in</span> images_filenames:</div><div class="line">            <span class="keyword">if</span> current_index % <span class="number">100</span> == <span class="number">0</span>:</div><div class="line">                <span class="keyword">if</span> writer:</div><div class="line">                    writer.close()</div><div class="line"></div><div class="line">                record_filename = <span class="string">"&#123;record_location&#125;-&#123;current_index&#125;.tfrecords"</span>.format(</div><div class="line">                    record_location=record_location,</div><div class="line">                    current_index=current_index)</div><div class="line"></div><div class="line">                writer = tf.python_io.TFRecordWriter(record_filename)</div><div class="line">            current_index += <span class="number">1</span></div><div class="line"></div><div class="line">            image_file = tf.read_file(image_filename)</div><div class="line"></div><div class="line">            <span class="comment"># In ImageNet dogs, there are a few images which TensorFlow doesn't recognize as JPEGs. This</span></div><div class="line">            <span class="comment"># try/catch will ignore those images.</span></div><div class="line">            <span class="keyword">try</span>:</div><div class="line">                image = tf.image.decode_jpeg(image_file)</div><div class="line">            <span class="keyword">except</span>:</div><div class="line">                print(image_filename)</div><div class="line">                <span class="keyword">continue</span></div><div class="line"></div><div class="line">            <span class="comment"># Converting to grayscale saves processing and memory but isn't required.</span></div><div class="line">            grayscale_image = tf.image.rgb_to_grayscale(image)</div><div class="line">            resized_image = tf.image.resize_images(grayscale_image, <span class="number">250</span>, <span class="number">151</span>)</div><div class="line"></div><div class="line">            <span class="comment"># tf.cast is used here because the resized images are floats but haven't been converted into</span></div><div class="line">            <span class="comment"># image floats where an RGB value is between [0,1).</span></div><div class="line">            image_bytes = sess.run(tf.cast(resized_image, tf.uint8)).tobytes()</div><div class="line"></div><div class="line">            <span class="comment"># Instead of using the label as a string, it'd be more efficient to turn it into either an</span></div><div class="line">            <span class="comment"># integer index or a one-hot encoded rank one tensor.</span></div><div class="line">            <span class="comment"># https://en.wikipedia.org/wiki/One-hot</span></div><div class="line">            image_label = breed.encode(<span class="string">"utf-8"</span>)</div><div class="line"></div><div class="line">            example = tf.train.Example(features=tf.train.Features(feature=&#123;</div><div class="line">                <span class="string">'label'</span>: tf.train.Feature(bytes_list=tf.train.BytesList(value=[image_label])),</div><div class="line">                <span class="string">'image'</span>: tf.train.Feature(bytes_list=tf.train.BytesList(value=[image_bytes]))</div><div class="line">            &#125;))</div><div class="line"></div><div class="line">            writer.write(example.SerializeToString())</div><div class="line">    writer.close()</div><div class="line"></div><div class="line">write_records_file(testing_dataset, <span class="string">"./output/testing-images/testing-image"</span>)</div><div class="line">write_records_file(training_dataset, <span class="string">"./output/training-images/training-image"</span>)</div></pre></td></tr></table></figure>
<p>The example code is opening each image, converting it to grayscale, resizing it and then adding it to a TFRecord file. The logic isn’t different from earlier examples except that the operation <code>tf.image.resize_images</code> is used. The resizing operation will scale every image to be the same size even if it distorts the image. For example, if an image in portrait orientation and an image in landscape orientation were both resized with this code then the output of the landscape image would become distorted. These distortions are caused because <code>tf.image.resize_images</code> doesn’t take into account aspect ratio (the ratio of height to width) of an image. To properly resize a set of images, cropping or padding is a preferred method because it ignores the aspect ratio stopping distortions.</p>
<h3 id="Load-Images"><a href="#Load-Images" class="headerlink" title="Load Images"></a>Load Images</h3><p>Once the testing and training dataset have been transformed to TFRecord format, they can be read as TFRecords instead of as JPEG images. The goal is to load the images a few at a time with their corresponding labels.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><div class="line">filename_queue = tf.train.string_input_producer(</div><div class="line">    tf.train.match_filenames_once(<span class="string">"./output/training-images/*.tfrecords"</span>))</div><div class="line">reader = tf.TFRecordReader()</div><div class="line">_, serialized = reader.read(filename_queue)</div><div class="line"></div><div class="line">features = tf.parse_single_example(</div><div class="line">    serialized,</div><div class="line">    features=&#123;</div><div class="line">        <span class="string">'label'</span>: tf.FixedLenFeature([], tf.string),</div><div class="line">        <span class="string">'image'</span>: tf.FixedLenFeature([], tf.string),</div><div class="line">    &#125;)</div><div class="line"></div><div class="line">record_image = tf.decode_raw(features[<span class="string">'image'</span>], tf.uint8)</div><div class="line"></div><div class="line"><span class="comment"># Changing the image into this shape helps train and visualize the output by converting it to</span></div><div class="line"><span class="comment"># be organized like an image.</span></div><div class="line">image = tf.reshape(record_image, [<span class="number">250</span>, <span class="number">151</span>, <span class="number">1</span>])</div><div class="line"></div><div class="line">label = tf.cast(features[<span class="string">'label'</span>], tf.string)</div><div class="line"></div><div class="line">min_after_dequeue = <span class="number">10</span></div><div class="line">batch_size = <span class="number">3</span></div><div class="line">capacity = min_after_dequeue + <span class="number">3</span> * batch_size</div><div class="line">image_batch, label_batch = tf.train.shuffle_batch(</div><div class="line">    [image, label], batch_size=batch_size, capacity=capacity, min_after_dequeue=min_after_dequeue)</div></pre></td></tr></table></figure>
<p>This example code loads training images by matching all the TFRecord files found in the training directory. Each TFRecord includes multiple images but the <code>tf.parse_single_example</code> will take a single Example out of the file. The batching operation discussed earlier is used to train multiple images simultaneously. Batching multiple images is useful because these operations are designed to work with multiple images the same as with a single image. The primary requirement is that the system have enough memory to work with them all.</p>
<p>With the images available in memory, the next step is to create the model used for training and testing.</p>
<h3 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h3><p>The model used is similar to the <a href="https://github.com/tensorflow/tensorflow/blob/r0.8/tensorflow/models/image/mnist/convolutional.py" target="_blank" rel="external">mnist convolution example</a> which is often used in tutorials describing convolutional neural networks in TensorFlow. The architecture of this model is simple yet it performs well for illustrating different techniques used in image classification and recognition. An advanced model may borrow more from <a href="https://code.google.com/p/cuda-convnet/" target="_blank" rel="external">Alex Krizhevsky’s AlexNet</a> design which includes more convolution layers.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><div class="line"><span class="comment"># Converting the images to a float of [0,1) to match the expected input to convolution2d</span></div><div class="line">float_image_batch = tf.image.convert_image_dtype(image_batch, tf.float32)</div><div class="line"></div><div class="line">conv2d_layer_one = tf.contrib.layers.convolution2d(</div><div class="line">    float_image_batch,</div><div class="line">    num_output_channels=<span class="number">32</span>,     <span class="comment"># The number of filters to generate</span></div><div class="line">    kernel_size=(<span class="number">5</span>,<span class="number">5</span>),          <span class="comment"># It's only the filter height and width.</span></div><div class="line">    activation_fn=tf.nn.relu,</div><div class="line">    weight_init=tf.random_normal,</div><div class="line">    stride=(<span class="number">2</span>, <span class="number">2</span>),</div><div class="line">    trainable=<span class="keyword">True</span>)</div><div class="line">pool_layer_one = tf.nn.max_pool(conv2d_layer_one,</div><div class="line">    ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>],</div><div class="line">    strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>],</div><div class="line">    padding=<span class="string">'SAME'</span>)</div><div class="line"></div><div class="line"><span class="comment"># Note, the first and last dimension of the convolution output hasn't changed but the</span></div><div class="line"><span class="comment"># middle two dimensions have.</span></div><div class="line">conv2d_layer_one.get_shape(), pool_layer_one.get_shape()</div></pre></td></tr></table></figure>
<pre><code>(TensorShape([Dimension(3), Dimension(125), Dimension(76), Dimension(32)]),
 TensorShape([Dimension(3), Dimension(63), Dimension(38), Dimension(32)]))
</code></pre><p>The first layer in the model is created using the shortcut <code>tf.contrib.layers.convolution2d</code>. It’s important to note that the <code>weight_init</code> is set to be a random normal, meaning that the first set of filters are filled with random numbers following a normal distribution (this parameter is renamed in TensorFlow 0.9 to be <code>weights_initializer</code>). The filters are set as <code>trainable</code> so that as the network is fed information, these weights are adjusted to improve the accuracy of the model.</p>
<p>After a convolution is applied to the images, the output is downsized using a <code>max_pool</code> operation. After the operation, the output shape of the convolution is reduced in half due to the <code>ksize</code> used in the pooling and the <code>strides</code>. The reduction didn’t change the number of filters (output channels) or the size of the image batch. The components which were reduced dealt with the height and width of the image (filter).</p>
<figure class="highlight python"><table><tr><td class="code"><pre><div class="line">conv2d_layer_two = tf.contrib.layers.convolution2d(</div><div class="line">    pool_layer_one,</div><div class="line">    num_output_channels=<span class="number">64</span>,        <span class="comment"># More output channels means an increase in the number of filters</span></div><div class="line">    kernel_size=(<span class="number">5</span>,<span class="number">5</span>),</div><div class="line">    activation_fn=tf.nn.relu,</div><div class="line">    weight_init=tf.random_normal,</div><div class="line">    stride=(<span class="number">1</span>, <span class="number">1</span>),</div><div class="line">    trainable=<span class="keyword">True</span>)</div><div class="line"></div><div class="line">pool_layer_two = tf.nn.max_pool(conv2d_layer_two,</div><div class="line">    ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>],</div><div class="line">    strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>],</div><div class="line">    padding=<span class="string">'SAME'</span>)</div><div class="line"></div><div class="line">conv2d_layer_two.get_shape(), pool_layer_two.get_shape()</div></pre></td></tr></table></figure>
<pre><code>(TensorShape([Dimension(3), Dimension(63), Dimension(38), Dimension(64)]),
 TensorShape([Dimension(3), Dimension(32), Dimension(19), Dimension(64)]))
</code></pre><p>The second layer changes little from the first except the depth of the filters. The number of filters is now doubled while again reducing the size of the height and width of the image. The multiple convolution and pool layers are continuing to reduce the height and width of the input while adding further depth.</p>
<p>At this point, further convolution and pool steps could be taken. In many architectures there are over 5 different convolution and pooling layers. The most advanced architectures take longer to train and debug but they can match more sophisticated patterns. In this example, the two convolution and pooling layers are enough to illustrate the mechanics at work.</p>
<p>The tensor being operated on is still fairly complex tensor, the next step is to fully connect every point in each image with an output neuron. Since this example is using <code>softmax</code> later, the fully connected layer needs to be changed into a rank two tensor. The tensor’s first dimension will be used to separate each image while the second dimension is a rank one tensor of each input tensor.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><div class="line">flattened_layer_two = tf.reshape(</div><div class="line">    pool_layer_two,</div><div class="line">    [</div><div class="line">        batch_size,  <span class="comment"># Each image in the image_batch</span></div><div class="line">        <span class="number">-1</span>           <span class="comment"># Every other dimension of the input</span></div><div class="line">    ])</div><div class="line"></div><div class="line">flattened_layer_two.get_shape()</div></pre></td></tr></table></figure>
<pre><code>TensorShape([Dimension(3), Dimension(38912)])
</code></pre><p><code>tf.reshape</code> has a special value which can be used to signify, use all the dimensions remaining. In this example code, the <code>-1</code> is used to reshape the last pooling layer into a giant rank one tensor. With the pooling layer flattened out, it can be combined with two fully connected layers which associate the current network state to the breed of dog predicted.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><div class="line"><span class="comment"># The weight_init parameter can also accept a callable, a lambda is used here  returning a truncated normal</span></div><div class="line"><span class="comment"># with a stddev specified.</span></div><div class="line">hidden_layer_three = tf.contrib.layers.fully_connected(</div><div class="line">    flattened_layer_two,</div><div class="line">    <span class="number">512</span>,</div><div class="line">    weight_init=<span class="keyword">lambda</span> i, dtype: tf.truncated_normal([<span class="number">38912</span>, <span class="number">512</span>], stddev=<span class="number">0.1</span>),</div><div class="line">    activation_fn=tf.nn.relu</div><div class="line">)</div><div class="line"></div><div class="line"><span class="comment"># Dropout some of the neurons, reducing their importance in the model</span></div><div class="line">hidden_layer_three = tf.nn.dropout(hidden_layer_three, <span class="number">0.1</span>)</div><div class="line"></div><div class="line"><span class="comment"># The output of this are all the connections between the previous layers and the 120 different dog breeds</span></div><div class="line"><span class="comment"># available to train on.</span></div><div class="line">final_fully_connected = tf.contrib.layers.fully_connected(</div><div class="line">    hidden_layer_three,</div><div class="line">    <span class="number">120</span>,  <span class="comment"># Number of dog breeds in the ImageNet Dogs dataset</span></div><div class="line">    weight_init=<span class="keyword">lambda</span> i, dtype: tf.truncated_normal([<span class="number">512</span>, <span class="number">120</span>], stddev=<span class="number">0.1</span>)</div><div class="line">)</div></pre></td></tr></table></figure>
<p>This example code creates the final fully connected layer of the network where every pixel is associated with every breed of dog. Every step of this network has been reducing the size of the input images by converting them into filters which are then matched with a breed of dog (label). This technique has reduced the processing power required to train or test a network while generalizing the output.</p>
<h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><p>Once a model is ready to be trained, the last steps follow the same process discussed in earlier chapters of this book. The model’s loss is computed based on how accurately it guessed the correct labels in the training data which feeds into a training optimizer which updates the weights of each layer. This process continues one iteration at a time while attempting to increase the accuracy of each step.</p>
<p>An important note related to this model, during training most classification functions (<code>tf.nn.softmax</code>) require numerical labels. This was highlighted in the section describing loading the images from TFRecords. At this point, each label is a string similar to <code>n02085620-Chihuahua</code>. Instead of using <code>tf.nn.softmax</code> on this string, the label needs to be converted to be a unique number for each label. Converting these labels into an integer representation should be done in preprocessing.</p>
<p>For this dataset, each label will be converted into an integer which represents the index of each name in a list including all the dog breeds. There are many ways to accomplish this task, for this example a new TensorFlow utility operation will be used (<code>tf.map_fn</code>).</p>
<figure class="highlight python"><table><tr><td class="code"><pre><div class="line"><span class="keyword">import</span> glob</div><div class="line"></div><div class="line"><span class="comment"># Find every directory name in the imagenet-dogs directory (n02085620-Chihuahua, ...)</span></div><div class="line">labels = list(map(<span class="keyword">lambda</span> c: c.split(<span class="string">"/"</span>)[<span class="number">-1</span>], glob.glob(<span class="string">"./imagenet-dogs/*"</span>)))</div><div class="line"></div><div class="line"><span class="comment"># Match every label from label_batch and return the index where they exist in the list of classes</span></div><div class="line">train_labels = tf.map_fn(<span class="keyword">lambda</span> l: tf.where(tf.equal(labels, l))[<span class="number">0</span>,<span class="number">0</span>:<span class="number">1</span>][<span class="number">0</span>], label_batch, dtype=tf.int64)</div></pre></td></tr></table></figure>
<p>This example code uses two different forms of a <code>map</code> operation. The first form of <code>map</code> is used to create a list including only the dog breed name based on a list of directories. The second form of <code>map</code> is <code>tf.map_fn</code> which is a TensorFlow operation which will map a function over a tensor on the graph. The <code>tf.map_fn</code> is used to generate a rank one tensor including only the integer indexes where each label is located in the list of all the class labels. These unique integers can now be used with <code>tf.nn.softmax</code> to classify output predictions.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><div class="line"><span class="comment"># setup-only-ignore</span></div><div class="line">loss = tf.reduce_mean(</div><div class="line">    tf.nn.sparse_softmax_cross_entropy_with_logits(</div><div class="line">        final_fully_connected, train_labels))</div><div class="line"></div><div class="line">batch = tf.Variable(<span class="number">0</span>)</div><div class="line">learning_rate = tf.train.exponential_decay(</div><div class="line">    <span class="number">0.01</span>,</div><div class="line">    batch * <span class="number">3</span>,</div><div class="line">    <span class="number">120</span>,</div><div class="line">    <span class="number">0.95</span>,</div><div class="line">    staircase=<span class="keyword">True</span>)</div><div class="line"></div><div class="line">optimizer = tf.train.AdamOptimizer(</div><div class="line">    learning_rate, <span class="number">0.9</span>).minimize(</div><div class="line">    loss, global_step=batch)</div><div class="line"></div><div class="line">train_prediction = tf.nn.softmax(final_fully_connected)</div></pre></td></tr></table></figure>
<h3 id="Debug-the-Filters-with-Tensorboard"><a href="#Debug-the-Filters-with-Tensorboard" class="headerlink" title="Debug the Filters with Tensorboard"></a>Debug the Filters with Tensorboard</h3><p>CNNs have multiple moving parts which can cause issues during training resulting in poor accuracy. Debugging problems in a CNN often start with investigating how the filters (kernels) are changing every iteration. Each weight used in a filter is constantly changing as the network attempts to learn the most accurate set of weights to use based on the train method.</p>
<p>In a well designed CNN, when the first convolution layer is started, the initialized input weights are set to be random (in this case using <code>weight_init=tf.random_normal</code>). These weights activate over an image and the output of the activation (feature map) is random as well. Visualizing the feature map as if it were an image, the output looks like the original image with static applied. The static is caused by all the weights activating at random. Over many iterations, each filter becomes more uniform as the weights are adjusted to fit the training feedback. As the network converges, the filters resemble distinct small patterns which can be found in the image.</p>
<p><img src="/images/14800438370921.png" alt=""></p>
<p></p><p style="text-align: center;"><i>An original grayscale training image before it is passed through the first convolution layer.</i></p><br><br><p></p>
<p><img src="/images/14800438610335.png" alt=""></p>
<p></p><p style="text-align: center;"><i>A single feature map from the first convolution layer highlighting randomness in the output.</i></p><br><br><p></p>
<p>Debugging a CNN requires a familiarity working with these filters. Currently there isn’t any built in support in tensorboard to display filters or feature maps. A simple view of the filters can be done using a <code>tf.image_summary</code> operation on the filters being trained and the feature maps generated. Adding an image summary output to a graph gives a good overview of the filters being used and the feature map generated by applying them to the input images.</p>
<p>An in progress jupyter notebook extension worth mentioning is <a href="https://github.com/ericjang/tdb" target="_blank" rel="external">TensorDebugger</a> which is in an early state of development. The extension has a mode capable of viewing changes in filters as an animated gif over iterations.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><div class="line"><span class="comment"># setup-only-ignore</span></div><div class="line">filename_queue.close(cancel_pending_enqueues=<span class="keyword">True</span>)</div><div class="line">coord.request_stop()</div><div class="line">coord.join(threads)</div></pre></td></tr></table></figure>
<p><strong>参考</strong></p>
<ul>
<li>转自<a href="https://github.com/backstopmedia/tensorflowbook/blob/master/chapters/05_object_recognition_and_classification/Chapter%205%20-%2005%20CNN%20Implementation.ipynb" target="_blank" rel="external">https://github.com/backstopmedia/tensorflowbook/blob/master/chapters/05_object_recognition_and_classification/Chapter%205%20-%2005%20CNN%20Implementation.ipynb</a></li>
</ul>

    
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="//schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/qrcode.jpg"
               alt="Feisky" />
          <p class="site-author-name" itemprop="name">Feisky</p>
          <p class="site-description motion-element" itemprop="description">Notes about anything.</p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">90</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          
            <div class="site-state-item site-state-categories">
              <a href="/categories">
                <span class="site-state-item-count">14</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">29</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/feiskyer" target="_blank" title="Github">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                  Github
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://twitter.com/feisky" target="_blank" title="Twitter">
                  
                    <i class="fa fa-fw fa-twitter"></i>
                  
                  Twitter
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://weibo.com/371069890" target="_blank" title="Weibo">
                  
                    <i class="fa fa-fw fa-weibo"></i>
                  
                  Weibo
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://www.cnblogs.com/feisky/" target="_blank" title="博客园">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                  博客园
                </a>
              </span>
            
          
        </div>

        
        

        
        

      </section>

      
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">
            
              
            
            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#CNN-Implementation"><span class="nav-number">1.</span> <span class="nav-text">CNN Implementation</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#CNN-Implementation-1"><span class="nav-number">1.1.</span> <span class="nav-text">CNN Implementation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Stanford-Dogs-Dataset"><span class="nav-number">1.1.1.</span> <span class="nav-text">Stanford Dogs Dataset</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Convert-Images-to-TFRecords"><span class="nav-number">1.1.2.</span> <span class="nav-text">Convert Images to TFRecords</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Load-Images"><span class="nav-number">1.1.3.</span> <span class="nav-text">Load Images</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Model"><span class="nav-number">1.1.4.</span> <span class="nav-text">Model</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Training"><span class="nav-number">1.1.5.</span> <span class="nav-text">Training</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Debug-the-Filters-with-Tensorboard"><span class="nav-number">1.1.6.</span> <span class="nav-text">Debug the Filters with Tensorboard</span></a></li></ol></li></ol></li></ol></div>
            
          </div>
        </section>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2016</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Feisky</span>
</div>

<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Pisces
  </a>
</div>

        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.0.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.0.2"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.0.2"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.0.2"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.2"></script>



  



  




  
  
  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length == 0) {
       search_path = "search.xml";
    }
    var path = "/" + search_path;
    // monitor main search box;

    function proceedsearch() {
      $("body").append('<div class="popoverlay">').css('overflow', 'hidden');
      $('.popup').toggle();

    }
    // search function;
    var searchFunc = function(path, search_id, content_id) {
    'use strict';
    $.ajax({
        url: path,
        dataType: "xml",
        async: true,
        success: function( xmlResponse ) {
            // get the contents from search data
            isfetched = true;
            $('.popup').detach().appendTo('.header-inner');
            var datas = $( "entry", xmlResponse ).map(function() {
                return {
                    title: $( "title", this ).text(),
                    content: $("content",this).text(),
                    url: $( "url" , this).text()
                };
            }).get();
            var $input = document.getElementById(search_id);
            var $resultContent = document.getElementById(content_id);
            $input.addEventListener('input', function(){
                var matchcounts = 0;
                var str='<ul class=\"search-result-list\">';
                var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                $resultContent.innerHTML = "";
                if (this.value.trim().length > 1) {
                // perform local searching
                datas.forEach(function(data) {
                    var isMatch = false;
                    var content_index = [];
                    var data_title = data.title.trim().toLowerCase();
                    var data_content = data.content.trim().replace(/<[^>]+>/g,"").toLowerCase();
                    var data_url = decodeURIComponent(data.url);
                    var index_title = -1;
                    var index_content = -1;
                    var first_occur = -1;
                    // only match artiles with not empty titles and contents
                    if(data_title != '') {
                        keywords.forEach(function(keyword, i) {
                            index_title = data_title.indexOf(keyword);
                            index_content = data_content.indexOf(keyword);
                            if( index_title >= 0 || index_content >= 0 ){
                                isMatch = true;
								if (i == 0) {
                                    first_occur = index_content;
                                }
                            } 
							
                        });
                    }
                    // show search results
                    if (isMatch) {
                        matchcounts += 1;
                        str += "<li><a href='"+ data_url +"' class='search-result-title'>"+ data_title +"</a>";
                        var content = data.content.trim().replace(/<[^>]+>/g,"");
                        if (first_occur >= 0) {
                            // cut out 100 characters
                            var start = first_occur - 20;
                            var end = first_occur + 80;
                            if(start < 0){
                                start = 0;
                            }
                            if(start == 0){
                                end = 50;
                            }
                            if(end > content.length){
                                end = content.length;
                            }
                            var match_content = content.substring(start, end);
                            // highlight all keywords
                            keywords.forEach(function(keyword){
                                var regS = new RegExp(keyword, "gi");
                                match_content = match_content.replace(regS, "<b class=\"search-keyword\">"+keyword+"</b>");
                            });

                            str += "<p class=\"search-result\">" + match_content +"...</p>"
                        }
                        str += "</li>";
                    }
                })};
                str += "</ul>";
                if (matchcounts == 0) { str = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>' }
                if (keywords == "") { str = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>' }
                $resultContent.innerHTML = str;
            });
            proceedsearch();
        }
    });}

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched == false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };

    });

    $('.popup-btn-close').click(function(e){
      $('.popup').hide();
      $(".popoverlay").remove();
      $('body').css('overflow', '');
    });
    $('.popup').click(function(e){
      e.stopPropagation();
    });
  </script>


  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
  </script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for (i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
    });
  </script>
  <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->


  


</body>
</html>
