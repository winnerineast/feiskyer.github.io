<!doctype html>



  


<html class="theme-next pisces use-motion">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.0.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Cloud, Container, Kubernetes, SDN, Docker" />





  <link rel="alternate" href="/atom.xml" title="Feisky's Blog" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.0.2" />






<meta name="description" content="Getting Started in TensorFlowA look at a very simple neural network in TensorFlowThis is an introduction to working with TensorFlow. It works through an example of a very simple neural network, walkin">
<meta property="og:type" content="website">
<meta property="og:title" content="Feisky's Blog">
<meta property="og:url" content="http://feisky.xyz/ml/tensorflow/getting-started/index.html">
<meta property="og:site_name" content="Feisky's Blog">
<meta property="og:description" content="Getting Started in TensorFlowA look at a very simple neural network in TensorFlowThis is an introduction to working with TensorFlow. It works through an example of a very simple neural network, walkin">
<meta property="og:image" content="http://feisky.xyz/output_3_0.png">
<meta property="og:image" content="http://feisky.xyz/output_5_1.png">
<meta property="og:image" content="http://feisky.xyz/output_9_0.png">
<meta property="og:image" content="http://feisky.xyz/output_11_0.png">
<meta property="og:image" content="http://feisky.xyz/output_13_0.png">
<meta property="og:image" content="http://feisky.xyz/output_15_0.png">
<meta property="og:image" content="http://feisky.xyz/output_19_0.png">
<meta property="og:image" content="http://feisky.xyz/output_22_0.png">
<meta property="og:updated_time" content="2016-11-13T00:02:28.311Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Feisky's Blog">
<meta name="twitter:description" content="Getting Started in TensorFlowA look at a very simple neural network in TensorFlowThis is an introduction to working with TensorFlow. It works through an example of a very simple neural network, walkin">
<meta name="twitter:image" content="http://feisky.xyz/output_3_0.png">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"always"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    }
  };
</script>




  <link rel="canonical" href="http://feisky.xyz/ml/tensorflow/getting-started/"/>


  <title>
  

  
     | Feisky's Blog
  
</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body itemscope itemtype="//schema.org/WebPage" lang="zh-Hans">

  


<!-- hexo-inject:begin --><!-- hexo-inject:end --><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-69699206-1', 'auto');
  ga('send', 'pageview');
</script>









  
  
    
  

  <div class="container one-collumn sidebar-position-left  ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="//schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">Feisky's Blog</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle">Notes about anything.</p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-pages">
          <a href="/pages" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            笔记
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-commonweal">
          <a href="/404.html" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-heartbeat"></i> <br />
            
            公益404
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup">
 <span class="search-icon fa fa-search"></span>
 <input type="text" id="local-search-input">
 <div id="local-search-result"></div>
 <span class="popup-btn-close">close</span>
</div>


    </div>
  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    
    
      <h1 id="Getting-Started-in-TensorFlow"><a href="#Getting-Started-in-TensorFlow" class="headerlink" title="Getting Started in TensorFlow"></a>Getting Started in TensorFlow</h1><h2 id="A-look-at-a-very-simple-neural-network-in-TensorFlow"><a href="#A-look-at-a-very-simple-neural-network-in-TensorFlow" class="headerlink" title="A look at a very simple neural network in TensorFlow"></a>A look at a very simple neural network in TensorFlow</h2><p>This is an introduction to working with TensorFlow. It works through an example of a very simple neural network, walking through the steps of setting up the input, adding operators, setting up gradient descent, and running the computation graph. </p>
<h2 id="A-simple-neural-network"><a href="#A-simple-neural-network" class="headerlink" title="A simple neural network"></a>A simple neural network</h2><p>Let’s start with code. We’re going to construct a very simple neural network computing a linear regression between two variables, y and x. The function it tries to compute is the best $w_1$ and $w_2$ it can find for the function $y = w_2 x + w_1$ for the data. The data we’re going to give it is toy data, linear perturbed with random noise.</p>
<p>This is what the network looks like:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</div><div class="line"></div><div class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> Image</div><div class="line"><span class="keyword">import</span> base64</div><div class="line">Image(data=base64.decodestring(<span class="string">"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"</span>.encode(<span class="string">'utf-8'</span>)), embed=<span class="keyword">True</span>)</div></pre></td></tr></table></figure>
<p><img src="output_3_0.png" alt="png"></p>
<p>Here is the TensorFlow code for this simple neural network and the results of running this code:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><div class="line"><span class="comment">#@test &#123;"output": "ignore"&#125;</span></div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line">%matplotlib inline</div><div class="line"></div><div class="line"><span class="comment"># Set up the data with a noisy linear relationship between X and Y.</span></div><div class="line">num_examples = <span class="number">50</span></div><div class="line">X = np.array([np.linspace(<span class="number">-2</span>, <span class="number">4</span>, num_examples), np.linspace(<span class="number">-6</span>, <span class="number">6</span>, num_examples)])</div><div class="line">X += np.random.randn(<span class="number">2</span>, num_examples)</div><div class="line">x, y = X</div><div class="line">x_with_bias = np.array([(<span class="number">1.</span>, a) <span class="keyword">for</span> a <span class="keyword">in</span> x]).astype(np.float32)</div><div class="line"></div><div class="line">losses = []</div><div class="line">training_steps = <span class="number">50</span></div><div class="line">learning_rate = <span class="number">0.002</span></div><div class="line"></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">    <span class="comment"># Set up all the tensors, variables, and operations.</span></div><div class="line">    input = tf.constant(x_with_bias)</div><div class="line">    target = tf.constant(np.transpose([y]).astype(np.float32))</div><div class="line">    weights = tf.Variable(tf.random_normal([<span class="number">2</span>, <span class="number">1</span>], <span class="number">0</span>, <span class="number">0.1</span>))</div><div class="line"></div><div class="line">    tf.initialize_all_variables().run()</div><div class="line"></div><div class="line">    yhat = tf.matmul(input, weights)</div><div class="line">    yerror = tf.sub(yhat, target)</div><div class="line">    loss = tf.nn.l2_loss(yerror)</div><div class="line">  </div><div class="line">    update_weights = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)</div><div class="line">  </div><div class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(training_steps):</div><div class="line">        <span class="comment"># Repeatedly run the operations, updating the TensorFlow variable.</span></div><div class="line">        update_weights.run()</div><div class="line">        losses.append(loss.eval())</div><div class="line"></div><div class="line">    <span class="comment"># Training is done, get the final values for the graphs</span></div><div class="line">    betas = weights.eval()</div><div class="line">    yhat = yhat.eval()</div><div class="line"></div><div class="line"><span class="comment"># Show the fit and the loss over time.</span></div><div class="line">fig, (ax1, ax2) = plt.subplots(<span class="number">1</span>, <span class="number">2</span>)</div><div class="line">plt.subplots_adjust(wspace=<span class="number">.3</span>)</div><div class="line">fig.set_size_inches(<span class="number">10</span>, <span class="number">4</span>)</div><div class="line">ax1.scatter(x, y, alpha=<span class="number">.7</span>)</div><div class="line">ax1.scatter(x, np.transpose(yhat)[<span class="number">0</span>], c=<span class="string">"g"</span>, alpha=<span class="number">.6</span>)</div><div class="line">line_x_range = (<span class="number">-4</span>, <span class="number">6</span>)</div><div class="line">ax1.plot(line_x_range, [betas[<span class="number">0</span>] + a * betas[<span class="number">1</span>] <span class="keyword">for</span> a <span class="keyword">in</span> line_x_range], <span class="string">"g"</span>, alpha=<span class="number">0.6</span>)</div><div class="line">ax2.plot(range(<span class="number">0</span>, training_steps), losses)</div><div class="line">ax2.set_ylabel(<span class="string">"Loss"</span>)</div><div class="line">ax2.set_xlabel(<span class="string">"Training steps"</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<pre><code>/usr/local/lib/python2.7/dist-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.
  warnings.warn(&#39;Matplotlib is building the font cache using fc-list. This may take a moment.&#39;)
</code></pre><p><img src="output_5_1.png" alt="png"></p>
<p>In the remainder of this notebook, we’ll go through this example in more detail.</p>
<h2 id="From-the-beginning"><a href="#From-the-beginning" class="headerlink" title="From the beginning"></a>From the beginning</h2><p>Let’s walk through exactly what this is doing from the beginning. We’ll start with what the data looks like, then we’ll look at this neural network, what is executed when, what gradient descent is doing, and how it all works together.</p>
<h2 id="The-data"><a href="#The-data" class="headerlink" title="The data"></a>The data</h2><p>This is a toy data set here. We have 50 (x,y) data points. At first, the data is perfectly linear.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><div class="line"><span class="comment">#@test &#123;"output": "ignore"&#125;</span></div><div class="line">num_examples = <span class="number">50</span></div><div class="line">X = np.array([np.linspace(<span class="number">-2</span>, <span class="number">4</span>, num_examples), np.linspace(<span class="number">-6</span>, <span class="number">6</span>, num_examples)])</div><div class="line">plt.figure(figsize=(<span class="number">4</span>,<span class="number">4</span>))</div><div class="line">plt.scatter(X[<span class="number">0</span>], X[<span class="number">1</span>])</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="output_9_0.png" alt="png"></p>
<p>Then we perturb it with noise:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><div class="line"><span class="comment">#@test &#123;"output": "ignore"&#125;</span></div><div class="line">X += np.random.randn(<span class="number">2</span>, num_examples)</div><div class="line">plt.figure(figsize=(<span class="number">4</span>,<span class="number">4</span>))</div><div class="line">plt.scatter(X[<span class="number">0</span>], X[<span class="number">1</span>])</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="output_11_0.png" alt="png"></p>
<h2 id="What-we-want-to-do"><a href="#What-we-want-to-do" class="headerlink" title="What we want to do"></a>What we want to do</h2><p>What we’re trying to do is calculate the green line below:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><div class="line"><span class="comment">#@test &#123;"output": "ignore"&#125;</span></div><div class="line">weights = np.polyfit(X[<span class="number">0</span>], X[<span class="number">1</span>], <span class="number">1</span>)</div><div class="line">plt.figure(figsize=(<span class="number">4</span>,<span class="number">4</span>))</div><div class="line">plt.scatter(X[<span class="number">0</span>], X[<span class="number">1</span>])</div><div class="line">line_x_range = (<span class="number">-3</span>, <span class="number">5</span>)</div><div class="line">plt.plot(line_x_range, [weights[<span class="number">1</span>] + a * weights[<span class="number">0</span>] <span class="keyword">for</span> a <span class="keyword">in</span> line_x_range], <span class="string">"g"</span>, alpha=<span class="number">0.8</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="output_13_0.png" alt="png"></p>
<p>Remember that our simple network looks like this:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><div class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> Image</div><div class="line"><span class="keyword">import</span> base64</div><div class="line">Image(data=base64.decodestring(<span class="string">"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"</span>.encode(<span class="string">'utf-8'</span>)), embed=<span class="keyword">True</span>)</div></pre></td></tr></table></figure>
<p><img src="output_15_0.png" alt="png"></p>
<p>That’s equivalent to the function $\hat{y} = w_2 x + w_1$. What we’re trying to do is find the “best” weights $w_1$ and $w_2$. That will give us that green regression line above.</p>
<p>What are the best weights? They’re the weights that minimize the difference between our estimate $\hat{y}$ and the actual y. Specifically, we want to minimize the sum of the squared errors, so minimize $\sum{(\hat{y} - y)^2}$, which is known as the <em>L2 loss</em>. So, the best weights are the weights that minimize the L2 loss.</p>
<h2 id="Gradient-descent"><a href="#Gradient-descent" class="headerlink" title="Gradient descent"></a>Gradient descent</h2><p>What gradient descent does is start with random weights for $\hat{y} = w_2 x + w_1$ and gradually moves those weights toward better values.</p>
<p>It does that by following the downward slope of the error curves. Imagine that the possible errors we could get with different weights as a landscape. From whatever weights we have, moving in some directions will increase the error, like going uphill, and some directions will decrease the error, like going downhill. We want to roll downhill, always moving the weights toward lower error.</p>
<p>How does gradient descent know which way is downhill? It follows the partial derivatives of the L2 loss. The partial derivative is like a velocity, saying which way the error will change if we change the weight. We want to move in the direction of lower error. The partial derivative points the way.</p>
<p>So, what gradient descent does is start with random weights and gradually walk those weights toward lower error, using the partial derivatives to know which direction to go.</p>
<h2 id="The-code-again"><a href="#The-code-again" class="headerlink" title="The code again"></a>The code again</h2><p>Let’s go back to the code now, walking through it with many more comments in the code this time:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><div class="line"><span class="comment">#@test &#123;"output": "ignore"&#125;</span></div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line"><span class="comment"># Set up the data with a noisy linear relationship between X and Y.</span></div><div class="line">num_examples = <span class="number">50</span></div><div class="line">X = np.array([np.linspace(<span class="number">-2</span>, <span class="number">4</span>, num_examples), np.linspace(<span class="number">-6</span>, <span class="number">6</span>, num_examples)])</div><div class="line"><span class="comment"># Add random noise (gaussian, mean 0, stdev 1)</span></div><div class="line">X += np.random.randn(<span class="number">2</span>, num_examples)</div><div class="line"><span class="comment"># Split into x and y</span></div><div class="line">x, y = X</div><div class="line"><span class="comment"># Add the bias node which always has a value of 1</span></div><div class="line">x_with_bias = np.array([(<span class="number">1.</span>, a) <span class="keyword">for</span> a <span class="keyword">in</span> x]).astype(np.float32)</div><div class="line"></div><div class="line"><span class="comment"># Keep track of the loss at each iteration so we can chart it later</span></div><div class="line">losses = []</div><div class="line"><span class="comment"># How many iterations to run our training</span></div><div class="line">training_steps = <span class="number">50</span></div><div class="line"><span class="comment"># The learning rate. Also known has the step size. This changes how far</span></div><div class="line"><span class="comment"># we move down the gradient toward lower error at each step. Too large</span></div><div class="line"><span class="comment"># jumps risk inaccuracy, too small slow the learning.</span></div><div class="line">learning_rate = <span class="number">0.002</span></div><div class="line"></div><div class="line"><span class="comment"># In TensorFlow, we need to run everything in the context of a session.</span></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">    <span class="comment"># Set up all the tensors.</span></div><div class="line">    <span class="comment"># Our input layer is the x value and the bias node.</span></div><div class="line">    input = tf.constant(x_with_bias)</div><div class="line">    <span class="comment"># Our target is the y values. They need to be massaged to the right shape.</span></div><div class="line">    target = tf.constant(np.transpose([y]).astype(np.float32))</div><div class="line">    <span class="comment"># Weights are a variable. They change every time through the loop.</span></div><div class="line">    <span class="comment"># Weights are initialized to random values (gaussian, mean 0, stdev 0.1)</span></div><div class="line">    weights = tf.Variable(tf.random_normal([<span class="number">2</span>, <span class="number">1</span>], <span class="number">0</span>, <span class="number">0.1</span>))</div><div class="line"></div><div class="line">    <span class="comment"># Initialize all the variables defined above.</span></div><div class="line">    tf.initialize_all_variables().run()</div><div class="line"></div><div class="line">    <span class="comment"># Set up all operations that will run in the loop.</span></div><div class="line">    <span class="comment"># For all x values, generate our estimate on all y given our current</span></div><div class="line">    <span class="comment"># weights. So, this is computing y = w2 * x + w1 * bias</span></div><div class="line">    yhat = tf.matmul(input, weights)</div><div class="line">    <span class="comment"># Compute the error, which is just the difference between our </span></div><div class="line">    <span class="comment"># estimate of y and what y actually is.</span></div><div class="line">    yerror = tf.sub(yhat, target)</div><div class="line">    <span class="comment"># We are going to minimize the L2 loss. The L2 loss is the sum of the</span></div><div class="line">    <span class="comment"># squared error for all our estimates of y. This penalizes large errors</span></div><div class="line">    <span class="comment"># a lot, but small errors only a little.</span></div><div class="line">    loss = tf.nn.l2_loss(yerror)</div><div class="line"></div><div class="line">    <span class="comment"># Perform gradient descent. </span></div><div class="line">    <span class="comment"># This essentially just updates weights, like weights += grads * learning_rate</span></div><div class="line">    <span class="comment"># using the partial derivative of the loss with respect to the</span></div><div class="line">    <span class="comment"># weights. It's the direction we want to go to move toward lower error.</span></div><div class="line">    update_weights = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)</div><div class="line"></div><div class="line">    <span class="comment"># At this point, we've defined all our tensors and run our initialization</span></div><div class="line">    <span class="comment"># operations. We've also set up the operations that will repeatedly be run</span></div><div class="line">    <span class="comment"># inside the training loop. All the training loop is going to do is </span></div><div class="line">    <span class="comment"># repeatedly call run, inducing the gradient descent operation, which has the effect of</span></div><div class="line">    <span class="comment"># repeatedly changing weights by a small amount in the direction (the</span></div><div class="line">    <span class="comment"># partial derivative or gradient) that will reduce the error (the L2 loss).</span></div><div class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(training_steps):</div><div class="line">        <span class="comment"># Repeatedly run the operations, updating the TensorFlow variable.</span></div><div class="line">        sess.run(update_weights)</div><div class="line"></div><div class="line">        <span class="comment"># Here, we're keeping a history of the losses to plot later</span></div><div class="line">        <span class="comment"># so we can see the change in loss as training progresses.</span></div><div class="line">        losses.append(loss.eval())</div><div class="line"></div><div class="line">    <span class="comment"># Training is done, get the final values for the charts</span></div><div class="line">    betas = weights.eval()</div><div class="line">    yhat = yhat.eval()</div><div class="line"></div><div class="line"><span class="comment"># Show the results.</span></div><div class="line">fig, (ax1, ax2) = plt.subplots(<span class="number">1</span>, <span class="number">2</span>)</div><div class="line">plt.subplots_adjust(wspace=<span class="number">.3</span>)</div><div class="line">fig.set_size_inches(<span class="number">10</span>, <span class="number">4</span>)</div><div class="line">ax1.scatter(x, y, alpha=<span class="number">.7</span>)</div><div class="line">ax1.scatter(x, np.transpose(yhat)[<span class="number">0</span>], c=<span class="string">"g"</span>, alpha=<span class="number">.6</span>)</div><div class="line">line_x_range = (<span class="number">-4</span>, <span class="number">6</span>)</div><div class="line">ax1.plot(line_x_range, [betas[<span class="number">0</span>] + a * betas[<span class="number">1</span>] <span class="keyword">for</span> a <span class="keyword">in</span> line_x_range], <span class="string">"g"</span>, alpha=<span class="number">0.6</span>)</div><div class="line">ax2.plot(range(<span class="number">0</span>, training_steps), losses)</div><div class="line">ax2.set_ylabel(<span class="string">"Loss"</span>)</div><div class="line">ax2.set_xlabel(<span class="string">"Training steps"</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="output_19_0.png" alt="png"></p>
<p>This version of the code has a lot more comments at each step. Read through the code and the comments.</p>
<p>The core piece is the loop, which contains a single <code>run</code> call. <code>run</code> executes the operations necessary for the <code>GradientDescentOptimizer</code> operation. That includes several other operations, all of which are also executed each time through the loop. The <code>GradientDescentOptimizer</code> execution has a side effect of assigning to weights, so the variable weights changes each time in the loop.</p>
<p>The result is that, in each iteration of the loop, the code processes the entire input data set, generates all the estimates $\hat{y}$ for each $x$ given the current weights $w_i$, finds all the errors and L2 losses $(\hat{y} - y)^2$, and then changes the weights $w_i$ by a small amount in the direction of that will reduce the L2 loss.</p>
<p>After many iterations of the loop, the amount we are changing the weights gets smaller and smaller, and the loss gets smaller and smaller, as we narrow in on near optimal values for the weights. By the end of the loop, we should be near the lowest possible values for the L2 loss, and near the best possible weights we could have.</p>
<h2 id="The-details"><a href="#The-details" class="headerlink" title="The details"></a>The details</h2><p>This code works, but there are still a few black boxes that are worth diving into here. <code>l2_loss</code>? <code>GradientDescentOptimizer</code>? What exactly are those doing?</p>
<p>One way to understand exactly what those are doing is to do the same thing without using those functions. Here is equivalent code that calculates the gradients (derivatives), L2 loss (sum squared error), and <code>GradientDescentOptimizer</code> from scratch without using those functions.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><div class="line"><span class="comment">#@test &#123;"output": "ignore"&#125;</span></div><div class="line"></div><div class="line"><span class="comment"># Use the same input data and parameters as the examples above.</span></div><div class="line"><span class="comment"># We're going to build up a list of the errors over time as we train to display later.</span></div><div class="line">losses = []</div><div class="line"></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">    <span class="comment"># Set up all the tensors.</span></div><div class="line">    <span class="comment"># The input is the x values with the bias appended on to each x.</span></div><div class="line">    input = tf.constant(x_with_bias)</div><div class="line">    <span class="comment"># We're trying to find the best fit for the target y values.</span></div><div class="line">    target = tf.constant(np.transpose([y]).astype(np.float32))</div><div class="line">    <span class="comment"># Let's set up the weights randomly</span></div><div class="line">    weights = tf.Variable(tf.random_normal([<span class="number">2</span>, <span class="number">1</span>], <span class="number">0</span>, <span class="number">0.1</span>))</div><div class="line"></div><div class="line">    tf.initialize_all_variables().run()</div><div class="line"></div><div class="line">    <span class="comment"># learning_rate is the step size, so how much we jump from the current spot</span></div><div class="line">    learning_rate = <span class="number">0.002</span></div><div class="line"></div><div class="line">    <span class="comment"># The operations in the operation graph.</span></div><div class="line">    <span class="comment"># Compute the predicted y values given our current weights</span></div><div class="line">    yhat = tf.matmul(input, weights)</div><div class="line">    <span class="comment"># How much does this differ from the actual y?</span></div><div class="line">    yerror = tf.sub(yhat, target)</div><div class="line">    <span class="comment"># Change the weights by subtracting derivative with respect to that weight</span></div><div class="line">    loss = <span class="number">0.5</span> * tf.reduce_sum(tf.mul(yerror, yerror))</div><div class="line">    gradient = tf.reduce_sum(tf.transpose(tf.mul(input, yerror)), <span class="number">1</span>, keep_dims=<span class="keyword">True</span>)</div><div class="line">    update_weights = tf.assign_sub(weights, learning_rate * gradient)</div><div class="line">    </div><div class="line">    <span class="comment"># Repeatedly run the operation graph over the training data and weights.</span></div><div class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(training_steps):</div><div class="line">        sess.run(update_weights)</div><div class="line">    </div><div class="line">        <span class="comment"># Here, we're keeping a history of the losses to plot later</span></div><div class="line">        <span class="comment"># so we can see the change in loss as training progresses.</span></div><div class="line">        losses.append(loss.eval())</div><div class="line"></div><div class="line">    <span class="comment"># Training is done, compute final values for the graph.</span></div><div class="line">    betas = weights.eval()</div><div class="line">    yhat = yhat.eval()</div><div class="line"></div><div class="line"><span class="comment"># Show the results.</span></div><div class="line">fig, (ax1, ax2) = plt.subplots(<span class="number">1</span>, <span class="number">2</span>)</div><div class="line">plt.subplots_adjust(wspace=<span class="number">.3</span>)</div><div class="line">fig.set_size_inches(<span class="number">10</span>, <span class="number">4</span>)</div><div class="line">ax1.scatter(x, y, alpha=<span class="number">.7</span>)</div><div class="line">ax1.scatter(x, np.transpose(yhat)[<span class="number">0</span>], c=<span class="string">"g"</span>, alpha=<span class="number">.6</span>)</div><div class="line">line_x_range = (<span class="number">-4</span>, <span class="number">6</span>)</div><div class="line">ax1.plot(line_x_range, [betas[<span class="number">0</span>] + a * betas[<span class="number">1</span>] <span class="keyword">for</span> a <span class="keyword">in</span> line_x_range], <span class="string">"g"</span>, alpha=<span class="number">0.6</span>)</div><div class="line">ax2.plot(range(<span class="number">0</span>, training_steps), losses)</div><div class="line">ax2.set_ylabel(<span class="string">"Loss"</span>)</div><div class="line">ax2.set_xlabel(<span class="string">"Training steps"</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="output_22_0.png" alt="png"></p>
<p>This code looks very similar to the code above, but without using <code>l2_loss</code> or <code>GradientDescentOptimizer</code>. Let’s look at exactly what it is doing instead.</p>
<p>This code is the key difference:</p>
<blockquote>
<p><code>loss = 0.5 * tf.reduce_sum(tf.mul(yerror, yerror))</code></p>
<p><code>gradient = tf.reduce_sum(tf.transpose(tf.mul(input, yerror)), 1, keep_dims=True)</code></p>
<p><code>update_weights = tf.assign_sub(weights, learning_rate * gradient)</code></p>
</blockquote>
<p>The first line calculates the L2 loss manually. It’s the same as <code>l2_loss(yerror)</code>, which is half of the sum of the squared error, so $\frac{1}{2} \sum (\hat{y} - y)^2$. With this code, you can see exactly what the <code>l2_loss</code> operation does. It’s the total of all the squared differences between the target and our estimates. And minimizing the L2 loss will minimize how much our estimates of $y$ differ from the true values of $y$.</p>
<p>The second line calculates <script type="math/tex">\begin{bmatrix}\sum{(\hat{y} - y)*1}\sum{(\hat{y} - y)*x_i}\end{bmatrix}</script>. What is that? It’s the partial derivatives of the L2 loss with respect to $w_1$ and $w_2$, the same thing as what <code>gradients(loss, weights)</code> does in the earlier code. Not sure about that? Let’s look at it in more detail. The gradient calculation is going to get the partial derivatives of loss with respect to each of the weights so we can change those weights in the direction that will reduce the loss. L2 loss is <script type="math/tex">\frac{1}{2} \sum (\hat{y} - y)^2</script>, where <script type="math/tex">\hat{y} = w_2 x + w_1</script>. So, using the chain rule and substituting in for $\hat{y}$ in the derivative, $\frac{\partial}{\partial w_2} = \sum{(\hat{y} - y)\, <em>x_i}$ and $\frac{\partial}{\partial w_1} = \sum{(\hat{y} - y)\, </em>1}$. <code>GradientDescentOptimizer</code> does these calculations automatically for you based on the graph structure.</p>
<p>The third line is equivalent to <code>weights -= learning_rate * gradient</code>, so it subtracts a constant the gradient after scaling by the learning rate (to avoid jumping too far each time, which risks moving in the wrong direction). It’s also the same thing that <code>GradientDescentOptimizer(learning_rate).minimize(loss)</code> does in the earlier code. Gradient descent updates its first parameter based on the values in the second after scaling by the third, so it’s equivalent to the <code>assign_sub(weights, learning_rate * gradient)</code>.</p>
<p>Hopefully, this other code gives you a better understanding of what the operations we used previously are actually doing. In practice, you’ll want to use those high level operators most of the time rather than calculating things yourself. For this toy example and simple network, it’s not too bad to compute and apply the gradients yourself from scratch, but things get more complicated with larger networks.</p>

    
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="//schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/qrcode.jpg"
               alt="Feisky" />
          <p class="site-author-name" itemprop="name">Feisky</p>
          <p class="site-description motion-element" itemprop="description">Notes about anything.</p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">89</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          
            <div class="site-state-item site-state-categories">
              <a href="/categories">
                <span class="site-state-item-count">14</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">28</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/feiskyer" target="_blank" title="Github">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                  Github
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://twitter.com/feisky" target="_blank" title="Twitter">
                  
                    <i class="fa fa-fw fa-twitter"></i>
                  
                  Twitter
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://weibo.com/371069890" target="_blank" title="Weibo">
                  
                    <i class="fa fa-fw fa-weibo"></i>
                  
                  Weibo
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://www.cnblogs.com/feisky/" target="_blank" title="博客园">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                  博客园
                </a>
              </span>
            
          
        </div>

        
        

        
        

      </section>

      
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">
            
              
            
            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Getting-Started-in-TensorFlow"><span class="nav-number">1.</span> <span class="nav-text">Getting Started in TensorFlow</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#A-look-at-a-very-simple-neural-network-in-TensorFlow"><span class="nav-number">1.1.</span> <span class="nav-text">A look at a very simple neural network in TensorFlow</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#A-simple-neural-network"><span class="nav-number">1.2.</span> <span class="nav-text">A simple neural network</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#From-the-beginning"><span class="nav-number">1.3.</span> <span class="nav-text">From the beginning</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#The-data"><span class="nav-number">1.4.</span> <span class="nav-text">The data</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#What-we-want-to-do"><span class="nav-number">1.5.</span> <span class="nav-text">What we want to do</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Gradient-descent"><span class="nav-number">1.6.</span> <span class="nav-text">Gradient descent</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#The-code-again"><span class="nav-number">1.7.</span> <span class="nav-text">The code again</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#The-details"><span class="nav-number">1.8.</span> <span class="nav-text">The details</span></a></li></ol></li></ol></div>
            
          </div>
        </section>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2016</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Feisky</span>
</div>

<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Pisces
  </a>
</div>

        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.0.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.0.2"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.0.2"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.0.2"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.2"></script>



  



  




  
  
  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length == 0) {
       search_path = "search.xml";
    }
    var path = "/" + search_path;
    // monitor main search box;

    function proceedsearch() {
      $("body").append('<div class="popoverlay">').css('overflow', 'hidden');
      $('.popup').toggle();

    }
    // search function;
    var searchFunc = function(path, search_id, content_id) {
    'use strict';
    $.ajax({
        url: path,
        dataType: "xml",
        async: true,
        success: function( xmlResponse ) {
            // get the contents from search data
            isfetched = true;
            $('.popup').detach().appendTo('.header-inner');
            var datas = $( "entry", xmlResponse ).map(function() {
                return {
                    title: $( "title", this ).text(),
                    content: $("content",this).text(),
                    url: $( "url" , this).text()
                };
            }).get();
            var $input = document.getElementById(search_id);
            var $resultContent = document.getElementById(content_id);
            $input.addEventListener('input', function(){
                var matchcounts = 0;
                var str='<ul class=\"search-result-list\">';
                var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                $resultContent.innerHTML = "";
                if (this.value.trim().length > 1) {
                // perform local searching
                datas.forEach(function(data) {
                    var isMatch = false;
                    var content_index = [];
                    var data_title = data.title.trim().toLowerCase();
                    var data_content = data.content.trim().replace(/<[^>]+>/g,"").toLowerCase();
                    var data_url = decodeURIComponent(data.url);
                    var index_title = -1;
                    var index_content = -1;
                    var first_occur = -1;
                    // only match artiles with not empty titles and contents
                    if(data_title != '') {
                        keywords.forEach(function(keyword, i) {
                            index_title = data_title.indexOf(keyword);
                            index_content = data_content.indexOf(keyword);
                            if( index_title >= 0 || index_content >= 0 ){
                                isMatch = true;
								if (i == 0) {
                                    first_occur = index_content;
                                }
                            } 
							
                        });
                    }
                    // show search results
                    if (isMatch) {
                        matchcounts += 1;
                        str += "<li><a href='"+ data_url +"' class='search-result-title'>"+ data_title +"</a>";
                        var content = data.content.trim().replace(/<[^>]+>/g,"");
                        if (first_occur >= 0) {
                            // cut out 100 characters
                            var start = first_occur - 20;
                            var end = first_occur + 80;
                            if(start < 0){
                                start = 0;
                            }
                            if(start == 0){
                                end = 50;
                            }
                            if(end > content.length){
                                end = content.length;
                            }
                            var match_content = content.substring(start, end);
                            // highlight all keywords
                            keywords.forEach(function(keyword){
                                var regS = new RegExp(keyword, "gi");
                                match_content = match_content.replace(regS, "<b class=\"search-keyword\">"+keyword+"</b>");
                            });

                            str += "<p class=\"search-result\">" + match_content +"...</p>"
                        }
                        str += "</li>";
                    }
                })};
                str += "</ul>";
                if (matchcounts == 0) { str = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>' }
                if (keywords == "") { str = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>' }
                $resultContent.innerHTML = str;
            });
            proceedsearch();
        }
    });}

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched == false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };

    });

    $('.popup-btn-close').click(function(e){
      $('.popup').hide();
      $(".popoverlay").remove();
      $('body').css('overflow', '');
    });
    $('.popup').click(function(e){
      e.stopPropagation();
    });
  </script>


  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
  </script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for (i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
    });
  </script>
  <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->


  


</body>
</html>
