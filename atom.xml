<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Feisky&#39;s Blog</title>
  <subtitle>Notes about anything.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://feisky.xyz/"/>
  <updated>2017-03-17T01:40:56.314Z</updated>
  <id>http://feisky.xyz/</id>
  
  <author>
    <name>Feisky</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Kubernetes HA</title>
    <link href="http://feisky.xyz/2017/03/15/Kubernetes-HA/"/>
    <id>http://feisky.xyz/2017/03/15/Kubernetes-HA/</id>
    <published>2017-03-15T10:12:47.000Z</published>
    <updated>2017-03-17T01:40:56.314Z</updated>
    
    <content type="html"><![CDATA[<p>Kubernetes从1.5开始，通过<code>kops</code>或者<code>kube-up.sh</code>部署的集群会自动部署一个高可用的系统，包括</p>
<ul>
<li>etcd集群模式</li>
<li>apiserver负载均衡</li>
<li>controller manager、scheduler和cluster autoscaler自动选主（有且仅有一个运行实例）</li>
</ul>
<p>如下图所示</p>
<p><img src="/kubernetes/ha/ha.png" alt=""></p>
<h2 id="etcd集群"><a href="#etcd集群" class="headerlink" title="etcd集群"></a>etcd集群</h2><p>从<code>https://discovery.etcd.io/new?size=3</code>获取token后，把<a href="https://kubernetes.io/docs/admin/high-availability/etcd.yaml" target="_blank" rel="external">https://kubernetes.io/docs/admin/high-availability/etcd.yaml</a>放到每台机器的<code>/etc/kubernetes/manifests/etcd.yaml</code>，并替换掉<code>${DISCOVERY_TOKEN}</code>, <code>${NODE_NAME}</code>和<code>${NODE_IP}</code>，既可以由kubelet来启动一个etcd集群。</p>
<p>对于运行在kubelet外部的etcd，可以参考<a href="https://github.com/coreos/etcd/blob/master/Documentation/op-guide/clustering.md" target="_blank" rel="external">etcd clustering guide</a>来手动配置集群模式。</p>
<h2 id="apiserver"><a href="#apiserver" class="headerlink" title="apiserver"></a>apiserver</h2><p>把<a href="https://kubernetes.io/docs/admin/high-availability/kube-apiserver.yaml" target="_blank" rel="external">https://kubernetes.io/docs/admin/high-availability/kube-apiserver.yaml</a>放到每台Master节点的<code>/etc/kubernetes/manifests/</code>，并把相关的配置放到<code>/srv/kubernetes/</code>，即可由kubelet自动创建并启动apiserver:</p>
<ul>
<li>basic_auth.csv - basic auth user and password</li>
<li>ca.crt - Certificate Authority cert</li>
<li>known_tokens.csv - tokens that entities (e.g. the kubelet) can use to talk to the apiserver</li>
<li>kubecfg.crt - Client certificate, public key</li>
<li>kubecfg.key - Client certificate, private key</li>
<li>server.cert - Server certificate, public key</li>
<li>server.key - Server certificate, private key</li>
</ul>
<p>apiserver启动后，还需要为它们做负载均衡，可以使用云平台的弹性负载均衡服务或者使用haproxy/lvs等为master节点配置负载均衡。</p>
<h2 id="controller-manager和scheduler"><a href="#controller-manager和scheduler" class="headerlink" title="controller manager和scheduler"></a>controller manager和scheduler</h2><p>controller manager和scheduler需要保证任何时刻都只有一个实例运行，需要一个选主的过程，所以在启动时要设置<code>--leader-elect=true</code>，比如</p>
<figure class="highlight brainfuck"><table><tr><td class="code"><pre><div class="line"><span class="comment">kube</span><span class="literal">-</span><span class="comment">scheduler</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">master=127</span><span class="string">.</span><span class="comment">0</span><span class="string">.</span><span class="comment">0</span><span class="string">.</span><span class="comment">1:8080</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">v=2</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">leader</span><span class="literal">-</span><span class="comment">elect=true</span></div><div class="line"><span class="comment">kube</span><span class="literal">-</span><span class="comment">controller</span><span class="literal">-</span><span class="comment">manager</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">master=127</span><span class="string">.</span><span class="comment">0</span><span class="string">.</span><span class="comment">0</span><span class="string">.</span><span class="comment">1:8080</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">cluster</span><span class="literal">-</span><span class="comment">cidr=10</span><span class="string">.</span><span class="comment">245</span><span class="string">.</span><span class="comment">0</span><span class="string">.</span><span class="comment">0/16</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">allocate</span><span class="literal">-</span><span class="comment">node</span><span class="literal">-</span><span class="comment">cidrs=true</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">service</span><span class="literal">-</span><span class="comment">account</span><span class="literal">-</span><span class="comment">private</span><span class="literal">-</span><span class="comment">key</span><span class="literal">-</span><span class="comment">file=/srv/kubernetes/server</span><span class="string">.</span><span class="comment">key</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">v=2</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">leader</span><span class="literal">-</span><span class="comment">elect=true</span></div></pre></td></tr></table></figure>
<p>把<a href="https://kubernetes.io/docs/admin/high-availability/kube-scheduler.yaml" target="_blank" rel="external">kube-scheduler.yaml</a>和<a href="https://kubernetes.io/docs/admin/high-availability/kube-controller-manager.yaml" target="_blank" rel="external">kube-controller-manager.yaml</a>(非GCE平台需要适当修改) 放到每台master节点的<code>/etc/kubernetes/manifests/</code>即可。</p>
<h2 id="数据持久化"><a href="#数据持久化" class="headerlink" title="数据持久化"></a>数据持久化</h2><p>除了上面提到的这些配置，持久化存储也是高可用Kubernetes集群所必须的。</p>
<ul>
<li>对于公有云上部署的集群，可以考虑使用云平台提供的持久化存储，比如aws ebs或者gce persistent disk</li>
<li>对于物理机部署的集群，可以考虑使用iSCSI、NFS、Gluster或者Ceph等网络存储，也可以使用RAID</li>
</ul>
<h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><ul>
<li><a href="https://kubernetes.io/docs/admin/high-availability/" target="_blank" rel="external">https://kubernetes.io/docs/admin/high-availability/</a></li>
<li><a href="http://kubecloud.io/setup-ha-k8s-kops/" target="_blank" rel="external">http://kubecloud.io/setup-ha-k8s-kops/</a></li>
<li><a href="https://github.com/coreos/etcd/blob/master/Documentation/op-guide/clustering.md" target="_blank" rel="external">https://github.com/coreos/etcd/blob/master/Documentation/op-guide/clustering.md</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Kubernetes从1.5开始，通过&lt;code&gt;kops&lt;/code&gt;或者&lt;code&gt;kube-up.sh&lt;/code&gt;部署的集群会自动部署一个高可用的系统，包括&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;etcd集群模式&lt;/li&gt;
&lt;li&gt;apiserver负载均衡&lt;/li&gt;
&lt;li&gt;
    
    </summary>
    
    
      <category term="kubernetes" scheme="http://feisky.xyz/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>AWS S3故障回顾和总结</title>
    <link href="http://feisky.xyz/2017/03/03/AWS-S3%E6%95%85%E9%9A%9C%E5%9B%9E%E9%A1%BE%E5%92%8C%E6%80%BB%E7%BB%93/"/>
    <id>http://feisky.xyz/2017/03/03/AWS-S3故障回顾和总结/</id>
    <published>2017-03-03T14:27:50.000Z</published>
    <updated>2017-03-17T01:40:56.314Z</updated>
    
    <content type="html"><![CDATA[<h2 id="S3故障回顾"><a href="#S3故障回顾" class="headerlink" title="S3故障回顾"></a>S3故障回顾</h2><p>2月28日，AWS工程师在排查Northern Virginia (US-EAST-1) Region的一个S3计费问题时，因敲错了一条playbook的参数而误删了大量的s3控制服务引发了4小时的故障。这个误操作影响了两个S3的核心系统：</p>
<ul>
<li>Index系统，管理S3对象的元数据和位置，主要处理GET、LIST、PUT、DELETE请求</li>
<li>Placement系统，管理新存储的分配和，主要处理PUT请求</li>
</ul>
<p>由于S3的故障，一大批依赖于S3的AWS服务也发生故障（如EC2、EBS和Lambda等），进而也影响了近半的北美互联网服务。不过，这次故障只是影响了用户的访问，并没有丢失数据（可靠性还是保障的，S3有7个9的可靠性和4个9的可用性）。</p>
<p>虽然aws s3具有优秀的故障设计，在故障发生时一般会自动恢复。但是，由于s3极好的稳定性，Index和Placement系统已经多年未重启过了，这次重启重建index的时间超过预期，并且placement依赖于index系统，导致系统恢复花费了较长的时间。</p>
<h2 id="改进措施"><a href="#改进措施" class="headerlink" title="改进措施"></a>改进措施</h2><ul>
<li>完善工具，保证即便有人操作错误也不会引发故障（对事不对人）</li>
<li>让删除操作缓慢些（以便有时间反悔）</li>
<li>加上一个最小资源数限制的SafeGuard</li>
<li>拆分现有的服务为更小单元（factoring services into cells），减小服务故障影响面，缩短服务的恢复时间</li>
</ul>
<h2 id="教训"><a href="#教训" class="headerlink" title="教训"></a>教训</h2><ul>
<li>高可用很难，不仅包括系统架构的高可用，更包括运维的高可用</li>
<li>跨Region和跨Availability Zone的重要性（比如Amazon和Netflix的服务并未受影响，因为它们在设计之初就处理了Region失效的问题），可以考虑的方案包括<ul>
<li>跨Region的数据备份和恢复</li>
<li>跨Region的主从服务，包括Warm standby和Hot standby</li>
<li>跨Region的主主服务</li>
</ul>
</li>
<li>故障恢复的重要性，充分保证Recovery Time Objective (RTO) and Recovery Point Objective (RPO)</li>
<li>自动化，人总是会犯错的，应该用技术而不是管理来解决问题（敏感操作放慢进程，以便有时间反悔）</li>
<li>故障和恢复演练，比如Google SRE指出他们有一个定期的服务停机计划，以验证系统是否真的符合预期</li>
</ul>
<p><img src="/images/pinterest-ha.jpg" alt=""></p>
<p><em>一个高可用系统的参考架构，图片来自<a href="https://learningawsblog.com/2017/03/02/how-should-we-think-about-the-aws-outage/" target="_blank" rel="external">The Learning AWS Blog</a></em></p>
<h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul>
<li><a href="https://aws.amazon.com/cn/message/41926/" target="_blank" rel="external">Summary of the Amazon S3 Service Disruption in the Northern Virginia (US-EAST-1) Region</a></li>
<li><a href="https://learningawsblog.com/2017/03/02/how-should-we-think-about-the-aws-outage/" target="_blank" rel="external">How should we think about the aws outage</a></li>
<li><a href="https://news.ycombinator.com/item?id=13775667" target="_blank" rel="external">Hacker News讨论</a></li>
<li><a href="http://coolshell.cn/articles/17737.html" target="_blank" rel="external">AWS 的 S3 故障回顾和思考</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;S3故障回顾&quot;&gt;&lt;a href=&quot;#S3故障回顾&quot; class=&quot;headerlink&quot; title=&quot;S3故障回顾&quot;&gt;&lt;/a&gt;S3故障回顾&lt;/h2&gt;&lt;p&gt;2月28日，AWS工程师在排查Northern Virginia (US-EAST-1) Region的一个
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Gitlab故障回顾和总结</title>
    <link href="http://feisky.xyz/2017/03/03/Gitlab%E6%95%85%E9%9A%9C%E5%9B%9E%E9%A1%BE%E5%92%8C%E6%80%BB%E7%BB%93/"/>
    <id>http://feisky.xyz/2017/03/03/Gitlab故障回顾和总结/</id>
    <published>2017-03-03T14:27:37.000Z</published>
    <updated>2017-03-17T01:40:56.314Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Gitlab故障回顾"><a href="#Gitlab故障回顾" class="headerlink" title="Gitlab故障回顾"></a>Gitlab故障回顾</h2><p>1月31日，Giblab在修复一个PostgreSQL数据同步问题（DB Replication lagged too far behind）时，误将生产环境的数据删除（本来是计划删除db1上的数据，结果发现在错误的db2上操作了）。进而寻求从备份数据恢复，结果发现没有实时备份：</p>
<ul>
<li>LVM Snapshot每24小时备份一次，最新数据是6小时前的</li>
<li>常规备份由于pg_dump客户端版本问题失效</li>
<li>Azure Disk snapshot未启用</li>
<li>数据库同步会导致webhook删除，所以webhook只能从备份中恢复</li>
<li>S3 备份未生效，bucket为空</li>
<li>糟糕的备份流程，并且没有明确的文档</li>
</ul>
<p>最后，Gitlab只能从LVM snapshot上恢复6小时前的数据。由于备份机器性能极差，并且数据拷贝极慢，整个恢复过程也比较慢（18个小时）。</p>
<h2 id="改进措施"><a href="#改进措施" class="headerlink" title="改进措施"></a>改进措施</h2><p>故障发生后，Gitlab列出了一系列的改进措施，包括</p>
<ol>
<li><a href="https://gitlab.com/gitlab-com/infrastructure/issues/1094" target="_blank" rel="external">Update PS1 across all hosts to more clearly differentiate between hosts and environments (#1094)</a></li>
<li><a href="https://gitlab.com/gitlab-com/infrastructure/issues/1095" target="_blank" rel="external">Prometheus monitoring for backups (#1095)</a></li>
<li><a href="https://gitlab.com/gitlab-com/infrastructure/issues/1096" target="_blank" rel="external">Set PostgreSQL’s max_connections to a sane value (#1096)</a></li>
<li><a href="https://gitlab.com/gitlab-com/infrastructure/issues/1097" target="_blank" rel="external">Investigate Point in time recovery &amp; continuous archiving for PostgreSQL (#1097)</a></li>
<li><a href="https://gitlab.com/gitlab-com/infrastructure/issues/1098" target="_blank" rel="external">Hourly LVM snapshots of the production databases (#1098)</a></li>
<li><a href="https://gitlab.com/gitlab-com/infrastructure/issues/1099" target="_blank" rel="external">Azure disk snapshots of production databases (#1099)</a></li>
<li><a href="https://gitlab.com/gitlab-com/infrastructure/issues/1100" target="_blank" rel="external">Move staging to the ARM environment (#1100)</a></li>
<li><a href="https://gitlab.com/gitlab-com/infrastructure/issues/1101" target="_blank" rel="external">Recover production replica(s) (#1101)</a></li>
<li><a href="https://gitlab.com/gitlab-com/infrastructure/issues/1102" target="_blank" rel="external">Automated testing of recovering PostgreSQL database backups (#1102)</a></li>
<li><a href="https://gitlab.com/gitlab-com/infrastructure/issues/1103" target="_blank" rel="external">Improve PostgreSQL replication documentation/runbooks (#1103)</a></li>
<li><a href="https://gitlab.com/gitlab-com/infrastructure/issues/1105" target="_blank" rel="external">Investigate pgbarman for creating PostgreSQL backups (#1105)</a></li>
<li><a href="https://gitlab.com/gitlab-com/infrastructure/issues/494" target="_blank" rel="external">Investigate using WAL-E as a means of Database Backup and Realtime Replication (#494)</a></li>
<li><a href="https://gitlab.com/gitlab-com/infrastructure/issues/1152" target="_blank" rel="external">Build Streaming Database Restore</a></li>
<li><a href="https://gitlab.com/gitlab-com/infrastructure/issues/1163" target="_blank" rel="external">Assign an owner for data durability</a></li>
<li><a href="https://gitlab.com/gitlab-org/omnibus-gitlab/merge_requests/1251" target="_blank" rel="external">Bundle pgpool-II 3.6.1 (!1251)</a></li>
<li><a href="https://gitlab.com/gitlab-com/infrastructure/issues/259" target="_blank" rel="external">Connection pooling/load balancing for PostgreSQL (#259)</a></li>
</ol>
<h2 id="教训"><a href="#教训" class="headerlink" title="教训"></a>教训</h2><ul>
<li>PostgreSQL配置和使用错误，参见<a href="https://blog.2ndquadrant.com/dataloss-at-gitlab/" target="_blank" rel="external">Dataloss at Gitlab</a></li>
<li>自动化的必要性，人总是会犯错的，应该用技术而不是管理来解决问题（设计更合理的高可用系统而不是靠权限控制人肉操作）</li>
<li>备份和故障恢复系统需要定期演练，否则即便像Gitlab拥有这么多的备份系统依然会丢失数据</li>
</ul>
<h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul>
<li><a href="https://about.gitlab.com/2017/02/10/postmortem-of-database-outage-of-january-31/" target="_blank" rel="external">Postmortem of database outage of January 31</a></li>
<li><a href="https://docs.google.com/document/d/1GCK53YDcBWQveod9kfzW-VCxIABGiryG7_z_6jHdVik/pub" target="_blank" rel="external">GitLab.com Database Incident - 2017/01/31</a></li>
<li><a href="https://about.gitlab.com/2017/02/01/gitlab-dot-com-database-incident/" target="_blank" rel="external">GitLab.com Database Incident</a></li>
<li><a href="https://news.ycombinator.com/item?id=13537052" target="_blank" rel="external">Hacker News讨论</a></li>
<li><a href="http://coolshell.cn/articles/17680.html" target="_blank" rel="external">从GITLAB误删除数据库想到的</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Gitlab故障回顾&quot;&gt;&lt;a href=&quot;#Gitlab故障回顾&quot; class=&quot;headerlink&quot; title=&quot;Gitlab故障回顾&quot;&gt;&lt;/a&gt;Gitlab故障回顾&lt;/h2&gt;&lt;p&gt;1月31日，Giblab在修复一个PostgreSQL数据同步问题（DB R
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Debugging application in containers</title>
    <link href="http://feisky.xyz/2017/02/14/Debugging-application-in-containers/"/>
    <id>http://feisky.xyz/2017/02/14/Debugging-application-in-containers/</id>
    <published>2017-02-14T13:24:48.000Z</published>
    <updated>2017-03-17T01:40:56.314Z</updated>
    
    <content type="html"><![CDATA[<p>对于普通的服务器进程，我们可以很方便的使用宿主机上的各种工具来调试；但容器经常是仅包含必要的应用程序，一般不包含常用的调试工具，那如何在线调试容器中的进程呢？最简单的方法是再起一个新的包含了调试工具的容器。</p>
<p>来看一个最简单的web容器如何调试。</p>
<h3 id="webserver容器"><a href="#webserver容器" class="headerlink" title="webserver容器"></a>webserver容器</h3><p>用Go编写一个最简单的webserver：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><div class="line"><span class="comment">// go-examples/basic/webserver</span></div><div class="line"><span class="keyword">package</span> main</div><div class="line"></div><div class="line"><span class="keyword">import</span> <span class="string">"net/http"</span></div><div class="line"><span class="keyword">import</span> <span class="string">"fmt"</span></div><div class="line"><span class="keyword">import</span> <span class="string">"log"</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">func</span> <span class="title">index</span><span class="params">(w http.ResponseWriter, r *http.Request)</span></span> &#123;</div><div class="line">	fmt.Fprintln(w, <span class="string">"Hello World"</span>)</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</div><div class="line">	http.HandleFunc(<span class="string">"/"</span>, index)</div><div class="line">	err := http.ListenAndServe(<span class="string">":80"</span>, <span class="literal">nil</span>)</div><div class="line">	<span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</div><div class="line"></div><div class="line">		log.Println(err)</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>以linux平台方式编译</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><div class="line">GOOS=linux go build -o webserver</div></pre></td></tr></table></figure>
<p>然后用下面的Docker build一个docker镜像：</p>
<figure class="highlight dockerfile"><table><tr><td class="code"><pre><div class="line"><span class="keyword">FROM</span> scratch</div><div class="line"></div><div class="line"><span class="keyword">COPY</span><span class="bash"> ./webserver /</span></div><div class="line"><span class="keyword">CMD</span><span class="bash"> [<span class="string">"/webserver"</span>]</span></div></pre></td></tr></table></figure>
<figure class="highlight sh"><table><tr><td class="code"><pre><div class="line"><span class="comment"># docker build -t feisky/hello-world .</span></div><div class="line">Sending build context to Docker daemon 5.655 MB</div><div class="line">Step 1/3 : FROM scratch</div><div class="line"> ---&gt;</div><div class="line">Step 2/3 : COPY ./webserver /</div><div class="line"> ---&gt; 184eb7c074b5</div><div class="line">Removing intermediate container abf107844295</div><div class="line">Step 3/3 : CMD /webserver</div><div class="line"> ---&gt; Running <span class="keyword">in</span> fe9fa4841e70</div><div class="line"> ---&gt; dca5ec00b3e7</div><div class="line">Removing intermediate container fe9fa4841e70</div><div class="line">Successfully built dca5ec00b3e7</div></pre></td></tr></table></figure>
<p>最后启动webserver容器</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><div class="line">docker run -itd --name webserver -p 80:80 feisky/hello-world</div></pre></td></tr></table></figure>
<p>访问映射后的80端口，webserver容器正常返回”Hello World”</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><div class="line"><span class="comment"># curl http://$(hostname):80</span></div><div class="line">Hello World</div></pre></td></tr></table></figure>
<h3 id="新建一个容器调试webserver"><a href="#新建一个容器调试webserver" class="headerlink" title="新建一个容器调试webserver"></a>新建一个容器调试webserver</h3><p>用一个包含调试工具或者方便安装调试工具的镜像（如alpine）创建一个新的container，为了便于获取webserver进程的状态，新的容器共享webserver容器的pid namespace和net namespace，并增加必要的capability：</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><div class="line">docker run -it --rm --pid=container:webserver --net=container:webserver --cap-add sys_admin --cap-add sys_ptrace alpine sh</div><div class="line">/ <span class="comment"># ps -ef</span></div><div class="line">PID   USER     TIME   COMMAND</div><div class="line">    1 root       0:00 /webserver</div><div class="line">   13 root       0:00 sh</div><div class="line">   18 root       0:00 ps -ef</div></pre></td></tr></table></figure>
<p>这样，新的容器可以直接attach到webserver进程上来在线调试，比如strace到webserver进程</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><div class="line"><span class="comment"># 继续在刚创建的新容器sh中执行</span></div><div class="line">/ <span class="comment"># apk update &amp;&amp; apk add strace</span></div><div class="line">fetch http://dl-cdn.alpinelinux.org/alpine/v3.5/main/x86_64/APKINDEX.tar.gz</div><div class="line">fetch http://dl-cdn.alpinelinux.org/alpine/v3.5/community/x86_64/APKINDEX.tar.gz</div><div class="line">v3.5.1-34-g1d3b13bd53 [http://dl-cdn.alpinelinux.org/alpine/v3.5/main]</div><div class="line">v3.5.1-29-ga981b1f149 [http://dl-cdn.alpinelinux.org/alpine/v3.5/community]</div><div class="line">OK: 7958 distinct packages available</div><div class="line">(1/1) Installing strace (4.14-r0)</div><div class="line">Executing busybox-1.25.1-r0.trigger</div><div class="line">OK: 5 MiB <span class="keyword">in</span> 12 packages</div><div class="line">/ <span class="comment"># strace -p 1</span></div><div class="line">strace: Process 1 attached</div><div class="line">epoll_wait(4,</div><div class="line">^Cstrace: Process 1 detached</div><div class="line"> &lt;detached ...&gt;</div></pre></td></tr></table></figure>
<p>也可以获取webserver容器的网络状态</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><div class="line"><span class="comment"># 继续在刚创建的新容器sh中执行</span></div><div class="line">/ <span class="comment"># apk add lsof</span></div><div class="line">(1/1) Installing lsof (4.89-r0)</div><div class="line">Executing busybox-1.25.1-r0.trigger</div><div class="line">OK: 5 MiB <span class="keyword">in</span> 13 packages</div><div class="line">/ <span class="comment"># lsof -i TCP</span></div><div class="line">COMMAND   PID USER   FD   TYPE DEVICE SIZE/OFF NODE NAME</div><div class="line">webserver   1 root    3u  IPv6  14233      0t0  TCP *:http (LISTEN)</div></pre></td></tr></table></figure>
<p>当然，也可以访问webserver容器的文件系统</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><div class="line">/ <span class="comment"># ls -l /proc/1/root/</span></div><div class="line">total 5524</div><div class="line">drwxr-xr-x    5 root     root           360 Feb 14 13:16 dev</div><div class="line">drwxr-xr-x    2 root     root          4096 Feb 14 13:16 etc</div><div class="line">dr-xr-xr-x  128 root     root             0 Feb 14 13:16 proc</div><div class="line">dr-xr-xr-x   13 root     root             0 Feb 14 13:16 sys</div><div class="line">-rwxr-xr-x    1 root     root       5651357 Feb 14 13:15 webserver</div></pre></td></tr></table></figure>
<p>Kubernetes社区也在提议增加一个<code>kubectl debug</code>命令，用类似的方式在Pod中启动一个新容器来调试运行中的进程，可以参见<a href="https://github.com/kubernetes/kubernetes/pull/35584" target="_blank" rel="external">https://github.com/kubernetes/kubernetes/pull/35584</a>。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;对于普通的服务器进程，我们可以很方便的使用宿主机上的各种工具来调试；但容器经常是仅包含必要的应用程序，一般不包含常用的调试工具，那如何在线调试容器中的进程呢？最简单的方法是再起一个新的包含了调试工具的容器。&lt;/p&gt;
&lt;p&gt;来看一个最简单的web容器如何调试。&lt;/p&gt;
&lt;h3
    
    </summary>
    
    
      <category term="docker" scheme="http://feisky.xyz/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>Software Engineering at Google</title>
    <link href="http://feisky.xyz/2017/02/13/Software-Engineering-at-Google/"/>
    <id>http://feisky.xyz/2017/02/13/Software-Engineering-at-Google/</id>
    <published>2017-02-13T11:36:09.000Z</published>
    <updated>2017-03-17T01:40:56.314Z</updated>
    
    <content type="html"><![CDATA[<p>Google的Fergus Henderson在<a href="https://arxiv.org/pdf/1702.01715.pdf" target="_blank" rel="external">Software Engineering at Google</a>中介绍了Google的软件工程实践。</p>
<h2 id="软件开发"><a href="#软件开发" class="headerlink" title="软件开发"></a>软件开发</h2><p>源码仓库</p>
<ul>
<li>单一源代码仓库，除了核心配置和安全相关代码，任何工程师都可以访问任何代码，并可以根据需要修改</li>
<li>所有开发都基于master分支，发布的时候才创建发布分枝</li>
<li>代码的每个子树都有owner，任何修改都需要owner批准</li>
</ul>
<p>Blaze分布式构建系统</p>
<ul>
<li>构建和测试存储库中的任何软件通常非常简单和快捷</li>
<li>开发人员只需要编写BUILD文件，并且每个构建系统仅依赖BUILD文件所声明的文件</li>
<li>构建系统的优化：可靠，自动跟踪依赖关系，增量构建，缓存构建结果以便复用</li>
<li>自动代码检查和测试</li>
</ul>
<p>代码审查</p>
<ul>
<li>完善的代码审查工具，如可视化的Web界面、电子邮件集成、自动展示测试或静态分析的结果</li>
<li>每个变更都必须由至少另外一人审查，并将审查结果自动复制到项目维护者的邮件列表</li>
<li>鼓励小的变更，大的变更可以拆分为一系列较小的变更</li>
</ul>
<p>测试</p>
<ul>
<li>鼓励和广泛使用单元测试，Mocking非常普遍</li>
<li>广泛使用集成测试和回归测试</li>
<li>自动测量测试覆盖率</li>
<li>部署之前进行负载测试，显示关键的metrics，比如延迟、错误率以及它们随请求速率的变化情况</li>
</ul>
<p>Bug跟踪</p>
<ul>
<li>Google使用名为Buganizer的Bug跟踪系统</li>
<li>使用标签分类bug</li>
<li>每个bug都有一个默认的assignee和抄送邮件列表</li>
</ul>
<p>编程语言</p>
<ul>
<li>鼓励使用C++、Java、Python或Go之一，最小化不同编程语言的数量</li>
<li>每种语言都有Google风格指南，还有一个公司范围内的可读性培训</li>
<li>不同语言之前使用基于Protocol Buffers的RPC通信</li>
<li>为所有语言提供通用的开发工具，比如代码签出、编辑、构建、测试、审查、bug报告等</li>
</ul>
<p>调试和分析</p>
<ul>
<li>在通用框架中提供调试和代码跟踪工具</li>
<li>提供用于调试的网络接口检查RPC调用的时间、错误率和频率限制以及资源消耗、性能分析数据等</li>
</ul>
<p>发布</p>
<ul>
<li>频繁发布（比如每周或每两周），自动化发布任务，提高工程师积极性，允许更多迭代以加快整体速度</li>
<li>发布分支，将master的修改cherry-pick到发布分支</li>
<li>发布到staging服务器，测试部分生产流量的副本</li>
<li>发布到canary服务器，测试真实生产流量的一个子集</li>
<li>最后逐步发布到所有服务器</li>
</ul>
<p>Launch approval</p>
<ul>
<li>任何用户可见的更改或重大的设计变更都需要工程团队之外的很多人员的审查和批准，以确保这些变更满足符合法律、隐私、安全、可靠性以及业务需求</li>
<li>Google内部的Launch approval工具会跟踪这些审查和批准</li>
</ul>
<p>Post-mortems</p>
<ul>
<li>任何重大的生产故障都需要写一份事后的总结文档，描述事件的原因、影响以及如何解决</li>
<li>重点关注如何避免它们再次发生（而不是追究人员责任）</li>
</ul>
<p>频繁重写</p>
<ul>
<li>大部分软件每隔几年都会重写一次</li>
<li>减少了累计复杂性</li>
<li>有助于适应当前的最佳实践，鼓励新的想法</li>
<li>也是一种团队成员之间传递ownership的方式，</li>
<li>这是Google保持敏捷和长期成功的关键</li>
</ul>
<h2 id="项目管理"><a href="#项目管理" class="headerlink" title="项目管理"></a>项目管理</h2><p>20%时间</p>
<ul>
<li>允许工程师可以将20%时间花在喜欢的任何项目上</li>
<li>有助于新想法的原型开发和演示，提高员工积极性</li>
<li>鼓励创新企业文化</li>
</ul>
<p>OKR（Objectives and Key Results）</p>
<ul>
<li>个人和团队要明确记录目标并评估这些目标的进展情况，团队设置季度和年度目标</li>
<li>建立关键结果来量化OKR，用OKR score评估进展情况</li>
<li>设置野心勃勃的OKR指标，即设置期望为目标的65%</li>
<li>OKR是全公司透明的，是一种简化的沟通框架，使每个人都清晰了解公司的目标以及自己的位置</li>
</ul>
<p>项目审批</p>
<ul>
<li>Google没有明确的项目审批流程，一般通过自下而上的方式进行</li>
</ul>
<p>公司重组</p>
<ul>
<li>因项目取消而重组时工程师可以自由选择新的团队或角色</li>
<li>在很大程度上，技术驱动公司应该进行频繁的重组以避免组织效率低下</li>
</ul>
<h2 id="人员管理"><a href="#人员管理" class="headerlink" title="人员管理"></a>人员管理</h2><p>角色，技术角色与管理角色分开，项目由技术主管领导和决策，而经理负责管理技术主管，指导职业发展，并负责绩效评估</p>
<ul>
<li>高标准的软件工程师</li>
<li>研究科学家</li>
<li>SRE</li>
<li>产品经理</li>
<li>项目经理</li>
</ul>
<p>工作环境（Facilities）</p>
<ul>
<li>Google提供丰富的娱乐、运动和餐饮设施</li>
<li>开放式办公鼓励沟通</li>
<li>先进的视频会议设施方便不同团队的沟通</li>
</ul>
<p>培训</p>
<ul>
<li>新员工培训，每个新员工都有导师和伙伴（Buddy）</li>
<li>“Codelabs”和丰富的培训课程</li>
<li>也支持外部机构学习</li>
</ul>
<p>换岗</p>
<ul>
<li>鼓励在不同部门换岗，帮助公司内传播知识</li>
<li>允许12个月内表现良好的员工更换项目</li>
<li>鼓励临时性的参与其他项目</li>
</ul>
<p>绩效考核和奖励</p>
<ul>
<li>鼓励“peer bonuses”和“kudos”</li>
<li>明确详细的晋升过程，确保正确的人得到晋升</li>
<li>匿名反馈调查评估经理的绩效</li>
</ul>
<p>更多内容请参考英文原文<a href="https://arxiv.org/pdf/1702.01715.pdf" target="_blank" rel="external">https://arxiv.org/pdf/1702.01715.pdf</a>。更多SRE的内容请参考<a href="http://landing.google.com/sre/book/index.html" target="_blank" rel="external">SRE</a>以及<a href="http://feisky.xyz/SRE/">SRE笔记</a>。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Google的Fergus Henderson在&lt;a href=&quot;https://arxiv.org/pdf/1702.01715.pdf&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Software Engineering at Google&lt;/a&gt;中
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>使用docker dind创建swarm集群</title>
    <link href="http://feisky.xyz/2017/01/09/%E4%BD%BF%E7%94%A8docker-dind%E5%88%9B%E5%BB%BAswarm%E9%9B%86%E7%BE%A4/"/>
    <id>http://feisky.xyz/2017/01/09/使用docker-dind创建swarm集群/</id>
    <published>2017-01-09T13:01:18.000Z</published>
    <updated>2017-03-17T01:40:56.314Z</updated>
    
    <content type="html"><![CDATA[<p>在OS X系统上，由于Docker for Mac只能创建一台虚拟机，所以要创建多节点swarm集群的话，就需要额外启动其他的虚拟机，并手动安装和配置docker。不过借助dind (docker in docker)，不需要创建额外的虚拟机也可以启动一个swarm集群。</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><div class="line">docker swarm init</div><div class="line">SWARM_TOKEN=$(docker swarm join-token -q worker)</div><div class="line">NUM_WORKERS=3 </div><div class="line"></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> $(seq <span class="string">"<span class="variable">$&#123;NUM_WORKERS&#125;</span>"</span>); <span class="keyword">do</span></div><div class="line">    docker run <span class="_">-d</span> --privileged --name worker-<span class="variable">$&#123;i&#125;</span> --hostname=worker-<span class="variable">$&#123;i&#125;</span> --restart=always -p <span class="variable">$&#123;i&#125;</span>2375:2375 docker:1.12-dind</div><div class="line">    docker --host=localhost:<span class="variable">$&#123;i&#125;</span>2375 swarm join --token <span class="variable">$&#123;SWARM_TOKEN&#125;</span> <span class="variable">$&#123;SWARM_MASTER&#125;</span>:2377</div><div class="line"><span class="keyword">done</span></div></pre></td></tr></table></figure>
<p>这时，查询系统的node列表为:</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><div class="line">$ docker node ls</div><div class="line">ID                           HOSTNAME  STATUS  AVAILABILITY  MANAGER STATUS</div><div class="line">82rg1gpkhm5fajnexre6p0v34 *  moby      Ready   Active        Leader</div><div class="line">a0hhwgtqsxosx9gg6h6wqmx68    worker-3  Ready   Active</div><div class="line">bn93fte7yflatee3y88qq7ff0    worker-1  Ready   Active</div><div class="line">emy5y7qr2y26hk3dtqgkvnak3    worker-2  Ready   Active</div></pre></td></tr></table></figure>
<p>当然，也可以启动一个游Mano Marks创建的swarm集群可视化容器，更直观的查看集群的状态：</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><div class="line">docker run -it -d -<span class="selector-tag">p</span> <span class="number">8000</span>:<span class="number">8080</span> -v /var/run/docker<span class="selector-class">.sock</span>:/var/run/docker<span class="selector-class">.sock</span> manomarks/visualizer</div></pre></td></tr></table></figure>
<p><img src="/images/docker_visualizer.png" alt=""></p>
<p>参考文档<a href="http://blog.terranillius.com/post/swarm_dind/" target="_blank" rel="external">http://blog.terranillius.com/post/swarm_dind/</a>。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在OS X系统上，由于Docker for Mac只能创建一台虚拟机，所以要创建多节点swarm集群的话，就需要额外启动其他的虚拟机，并手动安装和配置docker。不过借助dind (docker in docker)，不需要创建额外的虚拟机也可以启动一个swarm集群。&lt;
    
    </summary>
    
    
      <category term="Docker" scheme="http://feisky.xyz/tags/Docker/"/>
    
  </entry>
  
  <entry>
    <title>Grumpy: 使用Go来运行Python程序</title>
    <link href="http://feisky.xyz/2017/01/05/Grumpy-%E4%BD%BF%E7%94%A8Go%E6%9D%A5%E8%BF%90%E8%A1%8CPython%E7%A8%8B%E5%BA%8F/"/>
    <id>http://feisky.xyz/2017/01/05/Grumpy-使用Go来运行Python程序/</id>
    <published>2017-01-05T09:11:09.000Z</published>
    <updated>2017-03-17T01:40:56.314Z</updated>
    
    <content type="html"><![CDATA[<p>Grumpy是Google近期开源（<a href="https://github.com/google/grumpy" target="_blank" rel="external">https://github.com/google/grumpy</a>）的把Python程序编译成Go程序的工具，主要是为了解决Python GIL（Global Interpreter Lock）锁的问题，把Python中的多线程转换成goroutine来避免锁的问题。注意它跟PyPy不一样，PyPy是一个Python解释器，而Grumpy不是，它只是把Python程序翻译成了Go程序，然后再编译运行。</p>
<p>Grumpy还在开发中，也还没有在Google的生产环境中使用，很多系统库还没有完成翻译，并且也不支持各种外部库和C扩展。虽然如此，Grumpy仍然是一个值得关注的有趣项目（Github已经有2700+的star）。</p>
<h2 id="简单使用"><a href="#简单使用" class="headerlink" title="简单使用"></a>简单使用</h2><figure class="highlight sh"><table><tr><td class="code"><pre><div class="line">$ git <span class="built_in">clone</span> https://github.com/google/grumpy.git</div><div class="line">$ <span class="built_in">cd</span> grumpy</div><div class="line">$ <span class="built_in">echo</span> <span class="string">"print 'hello, world'"</span> | make run</div><div class="line">hello, world</div></pre></td></tr></table></figure>
<p>当然，也可以把程序翻译成Go再运行:</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><div class="line"><span class="built_in">echo</span> <span class="string">'print "hello, world"'</span> &gt; hello.py</div><div class="line">make</div><div class="line"><span class="built_in">export</span> GOPATH=<span class="variable">$PWD</span>/build</div><div class="line"><span class="built_in">export</span> PYTHONPATH=<span class="variable">$PWD</span>/build/lib/python2.7/site-packages</div><div class="line"></div><div class="line">tools/grumpc hello.py &gt; hello.go</div><div class="line">go build -o hello hello.go</div><div class="line">./hello</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Grumpy是Google近期开源（&lt;a href=&quot;https://github.com/google/grumpy&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/google/grumpy&lt;/a&gt;）的把Python
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Kubernetes v1.5.0 release</title>
    <link href="http://feisky.xyz/2016/12/13/Kubernetes-v1-5-0-release/"/>
    <id>http://feisky.xyz/2016/12/13/Kubernetes-v1-5-0-release/</id>
    <published>2016-12-13T03:51:29.000Z</published>
    <updated>2017-03-17T01:40:56.314Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Update on 2016.12.14:</strong></p>
<p>Due to a serious security problem, kubernetes v1.5.0 is not recommanded. Kubernetes v1.5.1 has just released, so we should upgrade to v1.5.1 directly.</p>
<blockquote>
<p>The <code>--anonymous-auth=</code> flag in v1.5.0 is true by default (which may result in any users being able to access kubernetes API), but v1.5.1 turns it to false.</p>
</blockquote>
<h2 id="Kubernetes-v1-5-0"><a href="#Kubernetes-v1-5-0" class="headerlink" title="Kubernetes v1.5.0"></a>Kubernetes v1.5.0</h2><ul>
<li>StatefulSets (ex-PetSets)<ul>
<li>StatefulSets are beta now (fixes and stabilization)</li>
</ul>
</li>
<li>Improved Federation Support<ul>
<li>New command: <code>kubefed</code></li>
<li>DaemonSets</li>
<li>Deployments</li>
<li>ConfigMaps</li>
</ul>
</li>
<li>Simplified Cluster Deployment<ul>
<li>Improvements to <code>kubeadm</code></li>
<li>HA Setup for Master</li>
</ul>
</li>
<li>Node Robustness and Extensibility<ul>
<li>Windows Server Container support</li>
<li>CRI for pluggable container runtimes</li>
<li><code>kubelet</code> API supports authentication and authorization</li>
</ul>
</li>
</ul>
<h2 id="Features"><a href="#Features" class="headerlink" title="Features"></a>Features</h2><p>Features for this release were tracked via the use of the <a href="https://github.com/kubernetes/features" target="_blank" rel="external">kubernetes/features</a> issues repo.  Each Feature issue is owned by a Special Interest Group from <a href="https://github.com/kubernetes/community" target="_blank" rel="external">kubernetes/community</a></p>
<ul>
<li><strong>API Machinery</strong><ul>
<li>[beta] <code>kube-apiserver</code> support for the OpenAPI spec is moving from alpha to beta. The first <a href="https://github.com/kubernetes-incubator/client-python" target="_blank" rel="external">non-go client</a> is based on it (<a href="https://github.com/kubernetes/features/issues/53" target="_blank" rel="external">kubernetes/features#53</a>)</li>
</ul>
</li>
<li><strong>Apps</strong><ul>
<li>[stable] When replica sets cannot create pods, they will now report detail via the API about the underlying reason (<a href="https://github.com/kubernetes/features/issues/120" target="_blank" rel="external">kubernetes/features#120</a>)</li>
<li>[stable] <code>kubectl apply</code> is now able to delete resources you no longer need with <code>--prune</code> (<a href="https://github.com/kubernetes/features/issues/128" target="_blank" rel="external">kubernetes/features#128</a>)</li>
<li>[beta] Deployments that cannot make progress in rolling out the newest version will now indicate via the API they are blocked (<a href="http://kubernetes.io/docs/user-guide/deployments/#failed-deployment" target="_blank" rel="external">docs</a>) (<a href="https://github.com/kubernetes/features/issues/122" target="_blank" rel="external">kubernetes/features#122</a>)</li>
<li>[beta] StatefulSets allow workloads that require persistent identity or per-instance storage to be created and managed on Kubernetes. (<a href="http://kubernetes.io/docs/concepts/abstractions/controllers/statefulsets/" target="_blank" rel="external">docs</a>) (<a href="https://github.com/kubernetes/features/issues/137" target="_blank" rel="external">kubernetes/features#137</a>)</li>
<li>[beta] In order to preserve safety guarantees the cluster no longer force deletes pods on un-responsive nodes and users are now warned if they try to force delete pods via the CLI. (<a href="http://kubernetes.io/docs/tasks/manage-stateful-set/scale-stateful-set/" target="_blank" rel="external">docs</a>) (<a href="https://github.com/kubernetes/features/issues/119" target="_blank" rel="external">kubernetes/features#119</a>)</li>
</ul>
</li>
<li><strong>Auth</strong><ul>
<li>[alpha] Further polishing of the Role-based access control alpha API including a default set of cluster roles. (<a href="http://kubernetes.io/docs/admin/authorization/" target="_blank" rel="external">docs</a>) (<a href="https://github.com/kubernetes/features/issues/2" target="_blank" rel="external">kubernetes/features#2</a>)</li>
<li>[beta] Added ability to authenticate/authorize access to the Kubelet API (<a href="http://kubernetes.io/docs/admin/kubelet-authentication-authorization/" target="_blank" rel="external">docs</a>) (<a href="https://github.com/kubernetes/features/issues/89" target="_blank" rel="external">kubernetes/features#89</a>)</li>
</ul>
</li>
<li><strong>AWS</strong><ul>
<li>[stable] Roles should appear in kubectl get nodes (<a href="https://github.com/kubernetes/features/issues/113" target="_blank" rel="external">kubernetes/features#113</a>)</li>
</ul>
</li>
<li><strong>Cluster Lifecycle</strong><ul>
<li>[alpha] Improved UX and usability for the kubeadm binary that makes it easy to get a new cluster running. (<a href="http://kubernetes.io/docs/getting-started-guides/kubeadm/" target="_blank" rel="external">docs</a>) (<a href="https://github.com/kubernetes/features/issues/11" target="_blank" rel="external">kubernetes/features#11</a>)</li>
</ul>
</li>
<li><strong>Cluster Ops</strong><ul>
<li>[alpha] Added ability to create/remove clusters w/highly available (replicated) masters on GCE using kube-up/kube-down scripts. (<a href="http://kubernetes.io/docs/admin/ha-master-gce/" target="_blank" rel="external">docs</a>) (<a href="https://github.com/kubernetes/features/issues/48" target="_blank" rel="external">kubernetes/features#48</a>)</li>
</ul>
</li>
<li><strong>Federation</strong><ul>
<li>[alpha] Support for ConfigMaps in federation. (<a href="http://kubernetes.io/docs/user-guide/federation/configmap/" target="_blank" rel="external">docs</a>) (<a href="https://github.com/kubernetes/features/issues/105" target="_blank" rel="external">kubernetes/features#105</a>)</li>
<li>[alpha] Alpha level support for DaemonSets in federation. (<a href="http://kubernetes.io/docs/user-guide/federation/daemonsets/" target="_blank" rel="external">docs</a>) (<a href="https://github.com/kubernetes/features/issues/101" target="_blank" rel="external">kubernetes/features#101</a>)</li>
<li>[alpha] Alpha level support for Deployments in federation. (<a href="http://kubernetes.io/docs/user-guide/federation/deployment/" target="_blank" rel="external">docs</a>) (<a href="https://github.com/kubernetes/features/issues/100" target="_blank" rel="external">kubernetes/features#100</a>)</li>
<li>[alpha] Cluster federation: Added support for DeleteOptions.OrphanDependents for federation resources. (<a href="http://kubernetes.io/docs/user-guide/federation/#cascading-deletion" target="_blank" rel="external">docs</a>) (<a href="https://github.com/kubernetes/features/issues/99" target="_blank" rel="external">kubernetes/features#99</a>)</li>
<li>[alpha] Introducing <code>kubefed</code>, a new command line tool to simplify federation control plane kubernetes.io/docs/admin/federation/kubefed/)) (<a href="https://github.com/kubernetes/features/issues/97" target="_blank" rel="external">kubernetes/features#97</a>)</li>
</ul>
</li>
<li><strong>Network</strong><ul>
<li>[stable] Services can reference another service by DNS name, rather than being hosted in pods (<a href="https://github.com/kubernetes/features/issues/33" target="_blank" rel="external">kubernetes/features#33</a>)</li>
<li>[beta] Opt in source ip preservation for Services with Type NodePort or LoadBalancer (<a href="http://kubernetes.io/docs/tutorials/services/source-ip/" target="_blank" rel="external">docs</a>) (<a href="https://github.com/kubernetes/features/issues/27" target="_blank" rel="external">kubernetes/features#27</a>)</li>
<li>[stable] Enable DNS Horizontal Autoscaling with beta ConfigMap parameters support (<a href="http://kubernetes.io/docs/tasks/administer-cluster/dns-horizontal-autoscaling/" target="_blank" rel="external">docs</a>)</li>
</ul>
</li>
<li><strong>Node</strong><ul>
<li>[alpha] Added ability to preserve access to host userns when userns remapping is enabled in container runtime (<a href="https://github.com/kubernetes/features/issues/127" target="_blank" rel="external">kubernetes/features#127</a>)</li>
<li>[alpha] Introducing the v1alpha1 CRI API to allow pluggable container runtimes; an experimental docker-CRI integration is ready for testing and feedback. (<a href="https://github.com/kubernetes/community/blob/master/contributors/devel/container-runtime-interface.md" target="_blank" rel="external">docs</a>) (<a href="https://github.com/kubernetes/features/issues/54" target="_blank" rel="external">kubernetes/features#54</a>)</li>
<li>[alpha] Kubelet launches container in a per pod cgroup hiearchy based on quality of service tier (<a href="https://github.com/kubernetes/features/issues/126" target="_blank" rel="external">kubernetes/features#126</a>)</li>
<li>[beta] Kubelet integrates with memcg notification API to detect when a hard eviction threshold is crossed (<a href="https://github.com/kubernetes/features/issues/125" target="_blank" rel="external">kubernetes/features#125</a>)</li>
<li>[beta] Introducing the beta version containerized node conformance test gcr.io/google_containers/node-test:0.2 for users to verify node setup. (<a href="http://kubernetes.io/docs/admin/node-conformance/" target="_blank" rel="external">docs</a>) (<a href="https://github.com/kubernetes/features/issues/84" target="_blank" rel="external">kubernetes/features#84</a>)</li>
</ul>
</li>
<li><strong>Scheduling</strong><ul>
<li>[alpha] Added support for accounting opaque integer resources. (<a href="http://kubernetes.io/docs/user-guide/compute-resources/#opaque-integer-resources-alpha-feature" target="_blank" rel="external">docs</a>) (<a href="https://github.com/kubernetes/features/issues/76" target="_blank" rel="external">kubernetes/features#76</a>)</li>
<li>[beta] PodDisruptionBudget has been promoted to beta, can be used to safely drain nodes while respecting application SLO’s (<a href="http://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/" target="_blank" rel="external">docs</a>) (<a href="https://github.com/kubernetes/features/issues/85" target="_blank" rel="external">kubernetes/features#85</a>)</li>
</ul>
</li>
<li><strong>UI</strong><ul>
<li>[stable] Dashboard UI now shows all user facing objects and their resource usage. (<a href="http://kubernetes.io/docs/user-guide/ui/" target="_blank" rel="external">docs</a>) (<a href="https://github.com/kubernetes/features/issues/136" target="_blank" rel="external">kubernetes/features#136</a>)</li>
</ul>
</li>
<li><strong>Windows</strong><ul>
<li>[alpha] Added support for Windows Server 2016 nodes and scheduling Windows Server Containers (<a href="http://kubernetes.io/docs/getting-started-guides/windows/" target="_blank" rel="external">docs</a>) (<a href="https://github.com/kubernetes/features/issues/116" target="_blank" rel="external">kubernetes/features#116</a>)</li>
</ul>
</li>
</ul>
<h2 id="Known-Issues"><a href="#Known-Issues" class="headerlink" title="Known Issues"></a>Known Issues</h2><p>Populated via <a href="https://github.com/kubernetes/kubernetes/issues/37134" target="_blank" rel="external">v1.5.0 known issues / FAQ accumulator</a></p>
<ul>
<li>CRI <a href="https://github.com/kubernetes/community/blob/master/contributors/devel/container-runtime-interface.md#kubernetes-v15-release-cri-v1alpha1" target="_blank" rel="external">known issues and<br>limitations</a></li>
<li>getDeviceNameFromMount() function doesn’t return the volume path correctly when the volume path contains spaces <a href="https://github.com/kubernetes/kubernetes/issues/37712" target="_blank" rel="external"><a href="https://github.com/kubernetes/kubernetes/pull/37712" target="_blank" rel="external">#37712</a></a></li>
<li>Federation alpha features do not have feature gates defined and<br>are hence enabled by default. This will be fixed in a future release.<br><a href="https://github.com/kubernetes/kubernetes/issues/38593" target="_blank" rel="external"><a href="https://github.com/kubernetes/kubernetes/pull/38593" target="_blank" rel="external">#38593</a></a></li>
<li>Federation control plane can be upgraded by updating the image<br>fields in the <code>Deployment</code> specs of the control plane components.<br>However, federation control plane upgrades were not tested in this<br>release <a href="https://github.com/kubernetes/kubernetes/issues/38537" target="_blank" rel="external">38537</a></li>
</ul>
<h2 id="Notable-Changes-to-Existing-Behavior"><a href="#Notable-Changes-to-Existing-Behavior" class="headerlink" title="Notable Changes to Existing Behavior"></a>Notable Changes to Existing Behavior</h2><ul>
<li>Node controller no longer force-deletes pods from the api-server. (<a href="https://github.com/kubernetes/kubernetes/pull/35235" target="_blank" rel="external"><a href="https://github.com/kubernetes/kubernetes/pull/35235" target="_blank" rel="external">#35235</a></a>, <a href="https://github.com/foxish" target="_blank" rel="external"><a href="https://github.com/foxish" target="_blank" rel="external">@foxish</a></a>)<ul>
<li>For StatefulSet (previously PetSet), this change means creation of<br>replacement pods is blocked until old pods are definitely not running<br>(indicated either by the kubelet returning from partitioned state,<br>deletion of the Node object, deletion of the instance in the cloud provider,<br>or force deletion of the pod from the api-server).<br>This helps prevent “split brain” scenarios in clustered applications by<br>ensuring that unreachable pods will not be presumed dead unless some<br>“fencing” operation has provided one of the above indications.</li>
<li>For all other existing controllers except StatefulSet, this has no effect on<br>the ability of the controller to replace pods because the controllers do not<br>reuse pod names (they use generate-name).</li>
<li>User-written controllers that reuse names of pod objects should evaluate this change.</li>
<li>When deleting an object with <code>kubectl delete ... --grace-period=0</code>, the client will<br>begin a graceful deletion and wait until the resource is fully deleted.  To force<br>deletion immediately, use the <code>--force</code> flag. This prevents users from accidentally<br>allowing two Stateful Set pods to share the same persistent volume which could lead to data<br>corruption <a href="https://github.com/kubernetes/kubernetes/pull/37263" target="_blank" rel="external"><a href="https://github.com/kubernetes/kubernetes/pull/37263" target="_blank" rel="external">#37263</a></a></li>
</ul>
</li>
</ul>
<ul>
<li><p>Allow anonymous API server access, decorate authenticated users with system:authenticated group (<a href="https://github.com/kubernetes/kubernetes/pull/32386" target="_blank" rel="external"><a href="https://github.com/kubernetes/kubernetes/pull/32386" target="_blank" rel="external">#32386</a></a>, <a href="https://github.com/liggitt" target="_blank" rel="external"><a href="https://github.com/liggitt" target="_blank" rel="external">@liggitt</a></a>)</p>
<ul>
<li>kube-apiserver learned the ‘—anonymous-auth’ flag, which defaults to true. When enabled, requests to the secure port that are not rejected by other configured authentication methods are treated as anonymous requests, and given a username of ‘system:anonymous’ and a group of ‘system:unauthenticated’.</li>
<li>Authenticated users are decorated with a ‘system:authenticated’ group.</li>
<li>NOTE: anonymous access is enabled by default. If you rely on authentication alone to authorize access, change to use an authorization mode other than AlwaysAllow, or or set ‘—anonymous-auth=false’.</li>
</ul>
</li>
<li><p>kubectl get -o jsonpath=… will now throw an error if the path is to a field not present in the json, even if the path is for a field valid for the type.  This is a change from the pre-1.5 behavior, which would return the default value for some fields even if they were not present in the json. (<a href="https://github.com/kubernetes/kubernetes/issues/37991" target="_blank" rel="external"><a href="https://github.com/kubernetes/kubernetes/pull/37991" target="_blank" rel="external">#37991</a></a>, <a href="http://github.com/pwittrock" target="_blank" rel="external"><a href="https://github.com/pwittrock" target="_blank" rel="external">@pwittrock</a></a>)</p>
</li>
<li><p>The strategicmerge patchMergeKey for VolumeMounts was changed from “name” to “mountPath”.  This was necessary because the name field refers to the name of the Volume, and is not a unique key for the VolumeMount.  Multiple VolumeMounts will have the same Volume name if mounting the same volume more than once.  The “mountPath” is verified to be unique and can act as the mergekey.  (<a href="https://github.coma/kubernetes/kubernetes/pull/35071" target="_blank" rel="external"><a href="https://github.com/kubernetes/kubernetes/pull/35071" target="_blank" rel="external">#35071</a></a>, <a href="http://github.com/pwittrock" target="_blank" rel="external"><a href="https://github.com/pwittrock" target="_blank" rel="external">@pwittrock</a></a>)</p>
</li>
</ul>
<h2 id="Deprecations"><a href="#Deprecations" class="headerlink" title="Deprecations"></a>Deprecations</h2><ul>
<li>extensions/v1beta1.Jobs is deprecated, use batch/v1.Job instead (<a href="https://github.com/kubernetes/kubernetes/pull/36355" target="_blank" rel="external"><a href="https://github.com/kubernetes/kubernetes/pull/36355" target="_blank" rel="external">#36355</a></a>, <a href="https://github.com/soltysh" target="_blank" rel="external"><a href="https://github.com/soltysh" target="_blank" rel="external">@soltysh</a></a>)</li>
<li>The kubelet —reconcile-cdir flag is deprecated because it has no function anymore. (<a href="https://github.com/kubernetes/kubernetes/pull/35523" target="_blank" rel="external"><a href="https://github.com/kubernetes/kubernetes/pull/35523" target="_blank" rel="external">#35523</a></a>, <a href="https://github.com/luxas" target="_blank" rel="external"><a href="https://github.com/luxas" target="_blank" rel="external">@luxas</a></a>)</li>
<li>Notice of deprecation for recycler <a href="https://github.com/kubernetes/kubernetes/pull/36760" target="_blank" rel="external"><a href="https://github.com/kubernetes/kubernetes/pull/36760" target="_blank" rel="external">#36760</a></a></li>
</ul>
<h2 id="Action-Required-Before-Upgrading"><a href="#Action-Required-Before-Upgrading" class="headerlink" title="Action Required Before Upgrading"></a>Action Required Before Upgrading</h2><ul>
<li>batch/v2alpha1.ScheduledJob has been renamed, use batch/v2alpha1.CronJob instead (<a href="https://github.com/kubernetes/kubernetes/pull/36021" target="_blank" rel="external"><a href="https://github.com/kubernetes/kubernetes/pull/36021" target="_blank" rel="external">#36021</a></a>, <a href="https://github.com/soltysh" target="_blank" rel="external"><a href="https://github.com/soltysh" target="_blank" rel="external">@soltysh</a></a>)</li>
<li>PetSet has been renamed to StatefulSet.<br>If you have existing PetSets, <strong>you must perform extra migration steps</strong> both<br>before and after upgrading to convert them to StatefulSets. (<a href="http://kubernetes.io/docs/tasks/manage-stateful-set/upgrade-pet-set-to-stateful-set/" target="_blank" rel="external">docs</a>) (<a href="https://github.com/kubernetes/kubernetes/pull/35663" target="_blank" rel="external"><a href="https://github.com/kubernetes/kubernetes/pull/35663" target="_blank" rel="external">#35663</a></a>, <a href="https://github.com/janetkuo" target="_blank" rel="external"><a href="https://github.com/janetkuo" target="_blank" rel="external">@janetkuo</a></a>)</li>
<li>If you are upgrading your Cluster Federation components from v1.4.x, please update your <code>federation-apiserver</code> and <code>federation-controller-manager</code> manifests to the new version (<a href="https://github.com/kubernetes/kubernetes/pull/30601" target="_blank" rel="external"><a href="https://github.com/kubernetes/kubernetes/pull/30601" target="_blank" rel="external">#30601</a></a>, <a href="https://github.com/madhusudancs" target="_blank" rel="external"><a href="https://github.com/madhusudancs" target="_blank" rel="external">@madhusudancs</a></a>)</li>
<li>The deprecated kubelet —configure-cbr0 flag has been removed, and with that the “classic” networking mode as well.  If you depend on this mode, please investigate whether the other network plugins <code>kubenet</code> or <code>cni</code> meet your needs. (<a href="https://github.com/kubernetes/kubernetes/pull/34906" target="_blank" rel="external"><a href="https://github.com/kubernetes/kubernetes/pull/34906" target="_blank" rel="external">#34906</a></a>, <a href="https://github.com/luxas" target="_blank" rel="external"><a href="https://github.com/luxas" target="_blank" rel="external">@luxas</a></a>)</li>
<li>New client-go structure, refer to kubernetes/client-go for versioning policy (<a href="https://github.com/kubernetes/kubernetes/pull/34989" target="_blank" rel="external"><a href="https://github.com/kubernetes/kubernetes/pull/34989" target="_blank" rel="external">#34989</a></a>, <a href="https://github.com/caesarxuchao" target="_blank" rel="external"><a href="https://github.com/caesarxuchao" target="_blank" rel="external">@caesarxuchao</a></a>)</li>
<li>The deprecated kube-scheduler —bind-pods-qps and —bind-pods burst flags have been removed, use —kube-api-qps and —kube-api-burst instead (<a href="https://github.com/kubernetes/kubernetes/pull/34471" target="_blank" rel="external"><a href="https://github.com/kubernetes/kubernetes/pull/34471" target="_blank" rel="external">#34471</a></a>, <a href="https://github.com/timothysc" target="_blank" rel="external"><a href="https://github.com/timothysc" target="_blank" rel="external">@timothysc</a></a>)</li>
<li>If you used the <a href="http://kubernetes.io/docs/admin/disruptions/" target="_blank" rel="external">PodDisruptionBudget</a> feature in 1.4 (i.e. created <code>PodDisruptionBudget</code> objects), then <strong>BEFORE</strong>  upgrading from 1.4 to 1.5, you must delete all <code>PodDisruptionBudget</code> objects (<code>policy/v1alpha1/PodDisruptionBudget</code>) that you have created. It is not possible to delete these objects after you upgrade, and their presence will prevent you from using the beta PodDisruptionBudget feature in 1.5 (which uses <code>policy/v1beta1/PodDisruptionBudget</code>). If you have already upgraded, you will need to downgrade the master to 1.4 to delete the <code>policy/v1alpha1/PodDisruptionBudget</code> objects.</li>
</ul>
<h2 id="External-Dependency-Version-Information"><a href="#External-Dependency-Version-Information" class="headerlink" title="External Dependency Version Information"></a>External Dependency Version Information</h2><p>Continuous integration builds have used the following versions of external dependencies, however, this is not a strong recommendation and users should consult an appropriate installation or upgrade guide before deciding what versions of etcd, docker or rkt to use.</p>
<ul>
<li>Docker versions 1.10.3 - 1.12.3<ul>
<li>Docker version 1.11.2 known issues<ul>
<li>Kernel crash with Aufs storage driver on Debian Jessie (<a href="https://github.com/kubernetes/kubernetes/issues/27885" target="_blank" rel="external"><a href="https://github.com/kubernetes/kubernetes/pull/27885" target="_blank" rel="external">#27885</a></a>)<br>which can be identified by the <a href="http://kubernetes.io/docs/admin/node-problem/" target="_blank" rel="external">node problem detector</a></li>
<li>Leaked File descriptors (<a href="https://github.com/docker/containerd/issues/275" target="_blank" rel="external">#275</a>)</li>
<li>Additional memory overhead per container (<a href="https://github.com/docker/docker/issues/21737" target="_blank" rel="external"><a href="https://github.com/kubernetes/kubernetes/pull/21737" target="_blank" rel="external">#21737</a></a>)</li>
</ul>
</li>
<li>Docker version 1.12.1 <a href="https://github.com/kubernetes/kubernetes/issues/28698" target="_blank" rel="external">has been validated</a> through the Kubernetes docker automated validation framework as has Docker version 1.12.3</li>
<li>Docker 1.10.3 contains <a href="https://github.com/docker/docker/compare/v1.10.3...runcom:docker-1.10.3-stable" target="_blank" rel="external">backports provided by RedHat</a> for known issues</li>
<li>Docker versions as old as may 1.9.1 work with <a href="CHANGELOG.md#191">known issues</a> but this is not guaranteed</li>
</ul>
</li>
<li>rkt version 1.21.0<ul>
<li>known issues with the rkt runtime are <a href="http://kubernetes.io/docs/getting-started-guides/rkt/notes/" target="_blank" rel="external">listed here</a></li>
</ul>
</li>
<li><p>etcd version 2.2.1</p>
<ul>
<li>etcd version 3.0.14 <a href="https://k8s-gubernator.appspot.com/builds/kubernetes-jenkins/logs/ci-kubernetes-e2e-gce-etcd3/" target="_blank" rel="external">has also been validated</a> but does require <a href="https://coreos.com/blog/migrating-applications-etcd-v3.html" target="_blank" rel="external">specific configuration steps</a></li>
</ul>
<p><a href="https://docs.k8s.io" target="_blank" rel="external">Documentation</a> &amp; <a href="https://releases.k8s.io/release-1.5/examples" target="_blank" rel="external">Examples</a></p>
</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;Update on 2016.12.14:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Due to a serious security problem, kubernetes v1.5.0 is not recommanded. Kubernetes v1.5.1 
    
    </summary>
    
    
      <category term="kubernetes" scheme="http://feisky.xyz/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Weekly reading list</title>
    <link href="http://feisky.xyz/2016/12/08/Weekly-reading-list/"/>
    <id>http://feisky.xyz/2016/12/08/Weekly-reading-list/</id>
    <published>2016-12-08T06:00:22.000Z</published>
    <updated>2017-03-17T01:40:56.314Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Docker收购Infinit-PDF"><a href="#Docker收购Infinit-PDF" class="headerlink" title="Docker收购Infinit PDF"></a><a href="https://blog.docker.com/2016/12/docker-acquires-infinit/" target="_blank" rel="external">Docker收购Infinit</a> <a href="/assets/infinit.pdf">PDF</a></h1><p><a href="https://infinit.sh/" target="_blank" rel="external">Infinit</a>为容器提供了分布式存储，其特点包括</p>
<ul>
<li>基于软件：可以部署在任何硬件之上，从遗留设备到消费级实体机、虚拟机，甚至容器。</li>
<li>可编程：开发者可以轻松地完成多个存储基础设施的自动化创建和部署，并且每个都能借助基于策略的能力进行自定义，适配上层应用的需求。</li>
<li>可伸缩：通过依靠一个去中心化的架构（即点对点），Infinit没有使用leader/follower模型，因而不会有瓶颈和单点失效的问题。</li>
<li>自愈合：Infinit的再平衡策略能让系统适应各种故障，包括拜占庭将军问题。</li>
<li>多用途：Infinit平台提供了块、对象和文件存储的接口：NFS、SMB、AWS S3、OpenStack Swift、iSCSI和FUSE等等。</li>
</ul>
<p><img src="/images/14811775178610.png" alt=""></p>
<p>基本原理：</p>
<p>• Federate all nodes in an overlay network for lookup and routing.<br>• Store data as blocks in a distributed hashtable (key-value store) with a per-block consensus.<br>• Use cryptographic access control to dispense from any leader.<br>• Use symmetrical operations to ensure resilience and flexibility.</p>
<p><img src="/images/14811780770073.jpg" alt=""></p>
<p><img src="/images/14811782445452.jpg" alt=""></p>
<p>Infinit以volume的形式挂载到docker容器中：</p>
<p><img src="/images/14811779192673.jpg" alt=""></p>
<h2 id="Docker-Alibaba——超大规模Docker化的实战经验"><a href="#Docker-Alibaba——超大规模Docker化的实战经验" class="headerlink" title="Docker@Alibaba——超大规模Docker化的实战经验"></a><a href="https://yq.aliyun.com/articles/64256" target="_blank" rel="external">Docker@Alibaba——超大规模Docker化的实战经验</a></h2><ul>
<li>AliDocker，并非阿里云，主要是阿里内部业务在用</li>
<li>Swarm改造支持单swarm实例health nodes 2w+<ul>
<li>优化连接管理、最小化锁粒度、修改diff算法减少node刷新时的开销、减少连接线程</li>
</ul>
</li>
<li>Docker Engine增强<ul>
<li>磁盘配额、固定IP、Hooks、解决hyperd重启容器销毁问题等</li>
</ul>
</li>
<li>镜像分发：流式分发、镜像分批预热、P2P等</li>
<li>swarm-proxy HA</li>
</ul>
<p><img src="/images/14811788194621.jpg" alt="">7</p>
<h2 id="微服务在微信后台的架构实践"><a href="#微服务在微信后台的架构实践" class="headerlink" title="微服务在微信后台的架构实践"></a><a href="http://ppt.geekbang.org/slide/show/607" target="_blank" rel="external">微服务在微信后台的架构实践</a></h2><ul>
<li>多地自治，园区互备：城市间RTT 30ms-400ms，园区间小于2ms</li>
<li>RPC：protobuf/libco</li>
<li>调度：Yard，双层调度器（Mesos+Yard）</li>
<li>过载保护：轻重分离、队列式、组合命令式</li>
</ul>
<p><img src="/images/14811817178100.jpg" alt=""></p>
<ul>
<li>数据存储：PaxosStore，同步复制、多主多写</li>
</ul>
<p><img src="/images/14811818656014.jpg" alt=""></p>
<h2 id="Go-微服务架构的基石"><a href="#Go-微服务架构的基石" class="headerlink" title="Go:微服务架构的基石"></a><a href="http://ppt.geekbang.org/slide/show/615" target="_blank" rel="external">Go:微服务架构的基石</a></h2><ul>
<li>负载均衡：seesaw、caddy</li>
<li>服务网关：tyk、fabio、vulcand</li>
<li>进程间通信：RESTful、RPC、自定义<ul>
<li>REST框架：beego、gin、Iris、micro、go-kit、goa</li>
<li>RPC框架：grpc、thrift、hprose</li>
<li>自定义：协议，编解码</li>
</ul>
</li>
<li>服务发现：etcd、consul、serf</li>
<li>调度系统：kubernetes、swarm、mesos</li>
<li>消息队列：NSQ、Nats</li>
<li>APM（应用性能监控）：appdash、Cloudinsight、opentracing</li>
<li>配置管理：etcd、consul、mgmt</li>
<li>日志分析：Beats、Heka</li>
<li>服务监控：open-falcon、prometheus</li>
<li>CI/CD：Drone</li>
<li>熔断器：gateway、Hystrix-go</li>
</ul>
<h2 id="基于万节点Kubernetes支撑大规模云应用实践"><a href="#基于万节点Kubernetes支撑大规模云应用实践" class="headerlink" title="基于万节点Kubernetes支撑大规模云应用实践"></a><a href="http://ppt.geekbang.org/slide/show/586" target="_blank" rel="external">基于万节点Kubernetes支撑大规模云应用实践</a></h2><ul>
<li>基于OpenStack的IaaS：裁剪KVM镜像、优化启动流程，Openvswitch SDN、Ceph存储</li>
<li>容器与虚拟机共用一套虚拟化网络</li>
<li>Ceph存储直接挂载到容器</li>
<li>统一的日志收集、分析、搜索</li>
<li>Kubernetes调度器优化：将原来的串行队列改为多个优先级队列</li>
<li>etcd集群扩展：将Pod/Node/ReplicationController拆分到不同的etcd集群</li>
</ul>
<p><img src="/images/14811811876614.jpg" alt=""></p>
<p><img src="/images/14811813670269.jpg" alt=""></p>
<h1 id="20-天持续压测，告诉你云存储性能哪家强-📈"><a href="#20-天持续压测，告诉你云存储性能哪家强-📈" class="headerlink" title="20 天持续压测，告诉你云存储性能哪家强 📈"></a><a href="https://www.v2ex.com/t/326038?from=timeline&amp;isappinstalled=0#reply7" target="_blank" rel="external">20 天持续压测，告诉你云存储性能哪家强</a> <a href="http://www.codingpy.com/specials/cbs_test/" target="_blank" rel="external">📈</a></h1><p>对阿里云和腾讯云两种云存储产品（云盘）的性能和价格对比：</p>
<ul>
<li>测试方法：SNIA发布的<a href="http://snia.org/sites/default/files/SSS_PTS_Enterprise_v1.1.pdf" target="_blank" rel="external">企业级SSD评测规范</a>及<a href="https://github.com/cloudharmony/block-storage" target="_blank" rel="external">实现</a></li>
<li>阿里云 SSD 云盘必须搭配 I/O 优化实例才能给发挥最大性能</li>
<li>腾讯云高效云盘的读写操作可同时达到预期性能峰值（数据块 16KB 以下），而阿里云方面，读写无法同时达到预期性能峰值</li>
<li>腾讯云达到了预期的性能；阿里云部分没有达到， 400GB 容量的时延过高</li>
<li>腾讯云高效云盘的时延在 1ms 以下， IOPS 、吞吐量的优势更加突出</li>
<li>两家高效云盘的 IOPS 表现均比较稳定，几乎呈一条直线，只有阿里云的 400GB 云盘有些略微波动</li>
<li>容量越大，似乎闲置时间对性能恢复的影响越明显；阿里云 400GB 高效云盘的性能波动受闲置时间影响较明显</li>
</ul>
<p><img src="/images/14811840736214.jpg" alt=""></p>
<h2 id="外卖的背后-饿了么基础架构从0到1的演进"><a href="#外卖的背后-饿了么基础架构从0到1的演进" class="headerlink" title="外卖的背后-饿了么基础架构从0到1的演进"></a><a href="http://ppt.geekbang.org/slide/show/619" target="_blank" rel="external">外卖的背后-饿了么基础架构从0到1的演进</a></h2><ul>
<li>负载均衡<ul>
<li>最初：HAProxy部署在客户端本地，不需要考虑HAProxy的高可用；问题是部署规模大，维护客户列表复杂，并且配置不统一</li>
<li>解决：<ul>
<li>RPC+内置LB SDK，服务自注册/自发现，配置少，部署简单</li>
<li>Redis/DB解决方案GoProxy: DAL/Corvus作为服务自注册，GoProxy订阅注册事件并自带服务发现的Haproxy</li>
<li>健康检查：心跳检测，进程在、端口活不代表服务可用</li>
</ul>
</li>
</ul>
</li>
<li>无损升级<ul>
<li>最初：发布前通知客户端停止请求，服务端将正在处理的请求处理完毕才能升级</li>
<li>解决：<ul>
<li>RPC调用：服务发现机制会在注销时通知客户端，直连情况下客户端从可用列表中剔除准备下线的服务</li>
<li>数据库访问，客户端限制连接存活时间，DAL侧重连</li>
</ul>
</li>
</ul>
</li>
<li>基础架构：开放式架构<ul>
<li>基于组件，给业务开发团队最大的自由</li>
<li>基于运行时，给业务开发团队最大限度的自由</li>
<li>开放封闭原则：基础框架应该是可以扩展的，但是不可以“选择”的</li>
</ul>
</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Docker收购Infinit-PDF&quot;&gt;&lt;a href=&quot;#Docker收购Infinit-PDF&quot; class=&quot;headerlink&quot; title=&quot;Docker收购Infinit PDF&quot;&gt;&lt;/a&gt;&lt;a href=&quot;https://blog.docker.
    
    </summary>
    
      <category term="Readings" scheme="http://feisky.xyz/categories/Readings/"/>
    
    
  </entry>
  
  <entry>
    <title>Weekly reading list</title>
    <link href="http://feisky.xyz/2016/12/04/Weekly-reading-list/"/>
    <id>http://feisky.xyz/2016/12/04/Weekly-reading-list/</id>
    <published>2016-12-04T23:59:01.000Z</published>
    <updated>2017-03-17T01:40:56.314Z</updated>
    
    <content type="html"><![CDATA[<h2 id="分布式后台毫秒服务引擎"><a href="#分布式后台毫秒服务引擎" class="headerlink" title="分布式后台毫秒服务引擎"></a><a href="http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2650994968&amp;idx=1&amp;sn=6713bb3b59e1fb38c70f7178de136cfc&amp;scene=0#wechat_redirect" target="_blank" rel="external">分布式后台毫秒服务引擎</a></h2><blockquote>
<p>腾讯QQ团队于12月4日开源了一个服务开发运营框架，叫做毫秒服务引擎（Mass Service Engine in Cluster，MSEC），它集RPC、名字发现服务、负载均衡、业务监控、灰度发布、容量管理、日志管理、Key-Value存储于一体，目的是提高开发与运营的效率和质量。</p>
</blockquote>
<p><img src="/images/14809236732969.jpg" alt=""></p>
<p><img src="/images/14809236897310.jpg" alt=""></p>
<ul>
<li>服务发现与负载均衡<ul>
<li>集中管理每个服务（包括异构服务）的IP地址</li>
<li>服务之间RPC调用：服务名+接口名</li>
<li>路由的同时统计过去一段时间的成功率和时延</li>
</ul>
</li>
<li>支持多种编程语言（通过Protocol buffer生成不同语言的接口），如C/C++、Java、PHP等</li>
<li>Web化的管理界面(Tomcat)</li>
<li>存储：Redis cluster</li>
<li>官方网站：<a href="http://haomiao.qq.com" target="_blank" rel="external">http://haomiao.qq.com</a></li>
<li>Github：<a href="https://github.com/Tencent/MSEC" target="_blank" rel="external">https://github.com/Tencent/MSEC</a></li>
</ul>
<h2 id="Understanding-SELinux-Roles"><a href="#Understanding-SELinux-Roles" class="headerlink" title="Understanding SELinux Roles"></a><a href="http://danwalsh.livejournal.com/75683.html" target="_blank" rel="external">Understanding SELinux Roles</a></h2><p>SELinux label包含4个部分<code>user_u:role_r:type_t:level</code>，每个用户可以访问的角色：</p>
<figure class="highlight armasm"><table><tr><td class="code"><pre><div class="line"><span class="symbol">semanage</span> user -l</div><div class="line"></div><div class="line">         Labeling   <span class="keyword">MLS/ </span>      <span class="keyword">MLS/ </span>                       </div><div class="line"><span class="keyword">SELinux </span>User    Prefix     MCS Level  MCS Range       <span class="keyword">SELinux </span>Roles</div><div class="line"></div><div class="line"><span class="symbol">guest_u</span>             user       <span class="built_in">s0</span>         <span class="built_in">s0</span>                             guest_r</div><div class="line"><span class="symbol">root</span>                    user       <span class="built_in">s0</span>         <span class="built_in">s0</span>-<span class="built_in">s0</span>:<span class="built_in">c0</span>.c1023        staff_r sysadm_r system_r unconfined_r</div><div class="line"><span class="symbol">staff_u</span>               user       <span class="built_in">s0</span>         <span class="built_in">s0</span>-<span class="built_in">s0</span>:<span class="built_in">c0</span>.c1023        staff_r sysadm_r system_r unconfined_r</div><div class="line"><span class="symbol">sysadm_u</span>         user       <span class="built_in">s0</span>         <span class="built_in">s0</span>-<span class="built_in">s0</span>:<span class="built_in">c0</span>.c1023        sysadm_r</div><div class="line"><span class="symbol">system_u</span>          user       <span class="built_in">s0</span>         <span class="built_in">s0</span>-<span class="built_in">s0</span>:<span class="built_in">c0</span>.c1023        system_r unconfined_r</div><div class="line"><span class="symbol">unconfined_u</span>    user       <span class="built_in">s0</span>         <span class="built_in">s0</span>-<span class="built_in">s0</span>:<span class="built_in">c0</span>.c1023        system_r unconfined_r</div><div class="line"><span class="symbol">user_u</span>               user       <span class="built_in">s0</span>         <span class="built_in">s0</span>                             user_r</div><div class="line"><span class="symbol">xguest_u</span>           user       <span class="built_in">s0</span>         <span class="built_in">s0</span>                             xguest_r</div></pre></td></tr></table></figure>
<ul>
<li>system_r role is the default role for all processes started at boot</li>
<li>You can not assign an SELinux user a role that is not listed</li>
<li>object_r is not really a role, but more of a place holder.  Roles only make sense for processes, not for files</li>
<li>on the file system.  But the SELinux label requires a role for all labels.  object_r is the role that we use to fill the objects on disks role.  Changing a process to run as object_r or trying to assign a different role to a file will always be denied by the kernel.</li>
</ul>
<h2 id="Kompose-a-tool-to-go-from-Docker-compose-to-Kubernetes"><a href="#Kompose-a-tool-to-go-from-Docker-compose-to-Kubernetes" class="headerlink" title="Kompose: a tool to go from Docker-compose to Kubernetes"></a><a href="http://blog.kubernetes.io/2016/11/kompose-tool-go-from-docker-compose-to-kubernetes.html" target="_blank" rel="external">Kompose: a tool to go from Docker-compose to Kubernetes</a></h2><ul>
<li>把docker-compose.yml或dab转换为kubernetes service+deployment</li>
<li>Github: <a href="https://github.com/kubernetes-incubator/kompose" target="_blank" rel="external">https://github.com/kubernetes-incubator/kompose</a></li>
</ul>
<figure class="highlight applescript"><table><tr><td class="code"><pre><div class="line">$ kompose <span class="comment">--bundle docker-compose-bundle.dab convert</span></div><div class="line">WARN[<span class="number">0000</span>]: Unsupported key networks - <span class="keyword">ignoring</span></div><div class="line"><span class="built_in">file</span> <span class="string">"redis-svc.json"</span> created</div><div class="line"><span class="built_in">file</span> <span class="string">"web-svc.json"</span> created</div><div class="line"><span class="built_in">file</span> <span class="string">"web-deployment.json"</span> created</div><div class="line"><span class="built_in">file</span> <span class="string">"redis-deployment.json"</span> created</div><div class="line"></div><div class="line">$ kompose -f docker-compose.yml convert</div><div class="line">WARN[<span class="number">0000</span>]: Unsupported key networks - <span class="keyword">ignoring</span></div><div class="line"><span class="built_in">file</span> <span class="string">"redis-svc.json"</span> created</div><div class="line"><span class="built_in">file</span> <span class="string">"web-svc.json"</span> created</div><div class="line"><span class="built_in">file</span> <span class="string">"web-deployment.json"</span> created</div><div class="line"><span class="built_in">file</span> <span class="string">"redis-deployment.json"</span> created</div></pre></td></tr></table></figure>
<h2 id="2016年网络虚拟化趋势"><a href="#2016年网络虚拟化趋势" class="headerlink" title="2016年网络虚拟化趋势"></a><a href="http://www.sdnlab.com/18153.html" target="_blank" rel="external">2016年网络虚拟化趋势</a></h2><ul>
<li>市场持续升温：NV的市场已经是一个数十亿美元的市场，Cisco、Juniper、Nuage、VMware是NV市场的四大巨头，他们占据了NV市场的绝大多数收入</li>
<li>思科和VMware公布的数据显示其与NV相关的投资组合在2016年将近30亿美元</li>
<li>容器化：思科收购ContainerX，VMWare推出vSphere集成容器（VIC）</li>
</ul>
<p><img src="/images/14809260496846.jpg" alt=""></p>
<h2 id="Amazon发布一大波新产品"><a href="#Amazon发布一大波新产品" class="headerlink" title="Amazon发布一大波新产品"></a>Amazon发布一大波新产品</h2><ul>
<li><a href="https://amazonlightsail.com/" target="_blank" rel="external">Amazon Lightsail</a>：廉价VPS，价格跟LightSale, DO, VULTR, Linode相同。</li>
<li><a href="https://aws.amazon.com/cn/blogs/aws/developer-preview-ec2-instances-f1-with-programmable-hardware/" target="_blank" rel="external">F1 instance with FPGA</a>：VHDL和Verilog终于有出路了</li>
<li>今年是机器学习大火的一年，Amazon也随大流（微软、Google）推出了AI服务：<ul>
<li>Amazon Rekognition图像处理和分析</li>
<li>Amazon Lex自然语言处理</li>
<li>Amazon Polly文本到语音的转换</li>
</ul>
</li>
<li><a href="https://aws.amazon.com/cn/snowmobile/" target="_blank" rel="external">AWS Snowmobile</a>：带宽从来都不是问题<br>  <img src="/images/14809265679834.jpg" alt=""></li>
</ul>
<h2 id="Linux-bcc-BPF-tcplife"><a href="#Linux-bcc-BPF-tcplife" class="headerlink" title="Linux bcc/BPF tcplife"></a><a href="http://www.brendangregg.com/blog/2016-11-30/linux-bcc-tcplife.html" target="_blank" rel="external">Linux bcc/BPF tcplife</a></h2><figure class="highlight tap"><table><tr><td class="code"><pre><div class="line"><span class="comment"># ./tcplife -D 80</span></div><div class="line">PID   COMM       LADDR           LPORT RADDR           RPORT TX_KB RX_KB MS</div><div class="line">27448 curl       100.66.11.247  <span class="number"> 54146 </span>54.154.224.174 <span class="number"> 80 </span>      <span class="number"> 0 </span>   <span class="number"> 1 </span>263.85</div><div class="line">27450 curl       100.66.11.247  <span class="number"> 20618 </span>54.154.164.22  <span class="number"> 80 </span>      <span class="number"> 0 </span>   <span class="number"> 1 </span>243.62</div><div class="line">27452 curl       100.66.11.247  <span class="number"> 11480 </span>54.154.43.103  <span class="number"> 80 </span>      <span class="number"> 0 </span>   <span class="number"> 1 </span>231.16</div><div class="line">27454 curl       100.66.11.247  <span class="number"> 31382 </span>54.154.15.7    <span class="number"> 80 </span>      <span class="number"> 0 </span>   <span class="number"> 1 </span>249.95</div><div class="line">27456 curl       100.66.11.247  <span class="number"> 33416 </span>52.210.59.223  <span class="number"> 80 </span>      <span class="number"> 0 </span>   <span class="number"> 1 </span>545.72</div><div class="line">27458 curl       100.66.11.247  <span class="number"> 16406 </span>52.30.140.35   <span class="number"> 80 </span>      <span class="number"> 0 </span>   <span class="number"> 1 </span>222.29</div><div class="line">27460 curl       100.66.11.247  <span class="number"> 11634 </span>52.30.133.135  <span class="number"> 80 </span>      <span class="number"> 0 </span>   <span class="number"> 1 </span>217.52</div><div class="line">27462 curl       100.66.11.247  <span class="number"> 25660 </span>52.30.126.182  <span class="number"> 80 </span>      <span class="number"> 0 </span>   <span class="number"> 1 </span>250.81</div><div class="line">[...]</div><div class="line"></div><div class="line"><span class="comment"># ./tcplife -h</span></div><div class="line">usage: tcplife [-h] [-T] [-t] [-w] [-s] [-p PID] [-L LOCALPORT]</div><div class="line">               [-D REMOTEPORT]</div><div class="line"></div><div class="line">Trace the lifespan of TCP sessions and summarize</div><div class="line"></div><div class="line">optional arguments:</div><div class="line">  -h, --help            show this help message and exit</div><div class="line">  -T, --time            include time column on output (HH:MM:SS)</div><div class="line">  -t, --timestamp       include timestamp on output (seconds)</div><div class="line">  -w, --wide            wide column output (fits IPv6 addresses)</div><div class="line">  -s, --csv             comma seperated values output</div><div class="line">  -p PID, --pid PID     trace this PID only</div><div class="line">  -L LOCALPORT, --localport LOCALPORT</div><div class="line">                        comma-separated list of local ports to trace.</div><div class="line">  -D REMOTEPORT, --remoteport REMOTEPORT</div><div class="line">                        comma-separated list of remote ports to trace.</div><div class="line"></div><div class="line">examples:</div><div class="line">    ./tcplife           <span class="comment"># trace all TCP connect()s</span></div><div class="line">    ./tcplife -t        <span class="comment"># include time column (HH:MM:SS)</span></div><div class="line">    ./tcplife -w        <span class="comment"># wider colums (fit IPv6)</span></div><div class="line">    ./tcplife -stT      <span class="comment"># csv output, with times &amp; timestamps</span></div><div class="line">    ./tcplife -p<span class="number"> 181 </span>   <span class="comment"># only trace PID 181</span></div><div class="line">    ./tcplife -L<span class="number"> 80 </span>    <span class="comment"># only trace local port 80</span></div><div class="line">    ./tcplife -L 80,81  <span class="comment"># only trace local ports 80 and 81</span></div><div class="line">    ./tcplife -D<span class="number"> 80 </span>    <span class="comment"># only trace remote port 80</span></div></pre></td></tr></table></figure>
<h2 id="cgroup-namespace"><a href="#cgroup-namespace" class="headerlink" title="cgroup namespace"></a><a href="http://hustcat.github.io/cgroup-namespace/" target="_blank" rel="external">cgroup namespace</a></h2><p>之前，在一个容器查看/proc/$PID/cgroup，或者在容器挂载cgroup时，会看到整个系统的cgroup信息；在内核从4.6开始，支持cgroup namespace （<a href="https://lwn.net/Articles/618873/" target="_blank" rel="external">https://lwn.net/Articles/618873/</a>）。</p>
<blockquote>
<p>(1)可以限制容器的cgroup filesytem视图，使得在容器中也可以安全的使用cgroup；<br>(2)此外，会使容器迁移更加容易；在迁移时，/proc/self/cgroup需要复制到目标机器，这要求容器的cgroup路径是唯一的，否则可能会与目标机器冲突。有了cgroupns，每个容器都有自己的cgroup filesystem视图，不用担心这种冲突。</p>
</blockquote>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;分布式后台毫秒服务引擎&quot;&gt;&lt;a href=&quot;#分布式后台毫秒服务引擎&quot; class=&quot;headerlink&quot; title=&quot;分布式后台毫秒服务引擎&quot;&gt;&lt;/a&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4
    
    </summary>
    
      <category term="Readings" scheme="http://feisky.xyz/categories/Readings/"/>
    
    
  </entry>
  
  <entry>
    <title>KubeCon/CloudNativeCon 2016见闻</title>
    <link href="http://feisky.xyz/2016/11/14/KubeCon-2016%E8%A7%81%E9%97%BB/"/>
    <id>http://feisky.xyz/2016/11/14/KubeCon-2016见闻/</id>
    <published>2016-11-14T01:49:52.000Z</published>
    <updated>2017-03-17T01:40:56.314Z</updated>
    
    <content type="html"><![CDATA[<p>题记：上周去西雅图参加了<a href="http://events.linuxfoundation.org/events/kubecon" target="_blank" rel="external">KubeCon&amp;CloudNativeCon 2016</a>，不仅见到Dawn、Brendan、Tim以及Sig Node的各路大神，还参加了不少有趣的session。</p>
<h2 id="Compiling-to-Containers-Brendan-Burns-Microsoft"><a href="#Compiling-to-Containers-Brendan-Burns-Microsoft" class="headerlink" title="Compiling to Containers - Brendan Burns, Microsoft"></a><a href="https://cnkc16.sched.org/event/8K8y/compiling-to-containers-brendan-burns-microsoft?iframe=no&amp;w=100%&amp;sidebar=yes&amp;bg=no" target="_blank" rel="external">Compiling to Containers</a> - Brendan Burns, Microsoft</h2><p>Containers可以看作是现代分布式系统的“汇编语言”，这样分布式系统的管理实际上就成了开发“Container汇编语言”。Brendan还以JavaScript为例，演示了如何基于<a href="https://github.com/brendandburns/metaparticle" target="_blank" rel="external">Metaparticle</a>来支持不同的service pattern:</p>
<ul>
<li>Simple Service: simply exposes a function as an HTTP service.</li>
<li>Scatter/Gather: fans all requests out to all leaf nodes (scatter phase), all responses are then aggregated in a root node (gather phase), this aggregate response is returned to the caller.</li>
<li>Shard: select a shard (replica) for a request.</li>
<li>Spread: similar to Shard, but uses a randomized shardingFunction.</li>
</ul>
<h2 id="Unik-Unikernel-Runtime-for-Kubernetes-Idit-Levine-EMC"><a href="#Unik-Unikernel-Runtime-for-Kubernetes-Idit-Levine-EMC" class="headerlink" title="Unik: Unikernel Runtime for Kubernetes - Idit Levine, EMC"></a><a href="https://cnkc16.sched.org/event/8K8v/unik-unikernel-runtime-for-kubernetes-idit-levine-emc?iframe=no&amp;w=100%&amp;sidebar=yes&amp;bg=no" target="_blank" rel="external">Unik: Unikernel Runtime for Kubernetes</a> - Idit Levine, EMC</h2><p><a href="https://github.com/emc-advanced-dev/unik" target="_blank" rel="external">Unik</a>是一个将应用编译成unikernel的工具，支持rump/OSv/IncludeOS/MirageOS等，编译的结果可以直接跑在公有云或者本地的虚拟机中。</p>
<p>Unik在Kubernetes中实现了一个特殊的<a href="https://github.com/emc-advanced-dev/kubernetes/tree/master/pkg/kubelet/unik" target="_blank" rel="external">runtime</a>，并可以通过kubernetes来管理unik：</p>
<ul>
<li>镜像管理：实际上通过rkt来操作镜像</li>
<li>Pod管理：one Pod == one VM == one Container，通过调用unik daemon来操作</li>
</ul>
<h2 id="Technical-View-Comparison-of-Container-Orchestration-and-Management-Systems-Lei-Zhang-HyperHQ"><a href="#Technical-View-Comparison-of-Container-Orchestration-and-Management-Systems-Lei-Zhang-HyperHQ" class="headerlink" title="Technical View: Comparison of Container Orchestration and Management Systems - Lei Zhang, HyperHQ"></a><a href="https://cnkc16.sched.org/event/8K3x/technical-view-comparison-of-container-orchestration-and-management-systems-lei-zhang-hyperhq?iframe=no&amp;w=100%&amp;sidebar=yes&amp;bg=no" target="_blank" rel="external">Technical View: Comparison of Container Orchestration and Management Systems</a> - Lei Zhang, HyperHQ</h2><p>我司张磊同学的大作，从架构、控制平面、服务发现与负载均衡、调度等各个角度对比Kubernetes/Swarmkit/Mesos等常见容器编排系统，并以<a href="https://github.com/hyperhq/hypernetes" target="_blank" rel="external">hypernetes</a>和<a href="http://hyper.sh/" target="_blank" rel="external">Hyper Container Service</a>为例说明为什么kubernetes是一个更好的选择。</p>
<p><img src="/images/1-4.png" alt="1"><br><img src="/images/2-3.png" alt="2"></p>
<h2 id="Everything-You-Ever-Wanted-to-Know-About-Resource-Scheduling-But-Were-Afraid-to-Ask-Tim-Hockin-Google"><a href="#Everything-You-Ever-Wanted-to-Know-About-Resource-Scheduling-But-Were-Afraid-to-Ask-Tim-Hockin-Google" class="headerlink" title="Everything You Ever Wanted to Know About Resource Scheduling, But Were Afraid to Ask - Tim Hockin, Google"></a><a href="https://cnkc16.sched.org/event/8K8l/everything-you-ever-wanted-to-know-about-resource-scheduling-but-were-afraid-to-ask-tim-hockin-google?iframe=no&amp;w=100%&amp;sidebar=yes&amp;bg=no" target="_blank" rel="external">Everything You Ever Wanted to Know About Resource Scheduling, But Were Afraid to Ask</a> - Tim Hockin, Google</h2><p>Kubernetes is fundamentally about resource management:</p>
<ul>
<li>CPU/Memory/Disk</li>
<li>Network/Ports/IP addresses</li>
<li>PIDs</li>
<li>GPUs</li>
<li>Storage</li>
<li>Power</li>
</ul>
<p>关于资源管理的三方面:</p>
<ul>
<li>Isolation：保证应用可以获取想要的资源并且不影响其他应用，目前基于Requests/Limits只支持cpu和内存</li>
<li>Sizing：应该给用户分配多少资源呢，没有统一的方法，只能靠benchmark，但精确的benchmark很难，所以Kubernetes提供了Horizontal Pod Autoscaler，未来可能还会有VerticalPodAutoscaler</li>
<li>Utilization：资源的实际使用情况是啥样的，涉及Priority调度、Quota管理、Overcommit等等。</li>
</ul>
<p><img src="/images/3-4.png" alt="3"></p>
<p>Slide见<a href="https://speakerdeck.com/thockin/everything-you-ever-wanted-to-know-about-resource-scheduling-dot-dot-dot-almost" target="_blank" rel="external">这里</a>.</p>
<h2 id="Self-hosted-Scale-and-Federation-with-Kubernetes-v1-4-and-Beyond-Brandon-Philips-CoreOS"><a href="#Self-hosted-Scale-and-Federation-with-Kubernetes-v1-4-and-Beyond-Brandon-Philips-CoreOS" class="headerlink" title="Self-hosted, Scale, and Federation with Kubernetes v1.4 and Beyond - Brandon Philips, CoreOS"></a><a href="https://cnkc16.sched.org/event/8K3v/self-hosted-scale-and-federation-with-kubernetes-v14-and-beyond-brandon-philips-coreos-inc?iframe=yes&amp;w=100%&amp;sidebar=yes&amp;bg=no#" target="_blank" rel="external">Self-hosted, Scale, and Federation with Kubernetes v1.4 and Beyond</a> - Brandon Philips, CoreOS</h2><p>基本的idea是用Kubernetes来管理Kubernetes的部署和升级，这也是一个孵化状态的项目<a href="https://github.com/kubernetes-incubator/bootkube" target="_blank" rel="external">bootkube</a>。由于kubelet初始化需要控制平面的配合，bootkube会在一开始的时候会启动一个暂时的控制平面（api-server, scheduler, controller manager），部署完成后再替换回来。</p>
<p>部署前将bootkube作为控制平面：</p>
<p><img src="/images/bootkube-1.png" alt="bootkube-1"></p>
<p>部署后替换成真正的控制平面：</p>
<p><img src="/images/bootkube-2.png" alt="bootkube-2"></p>
<p>相关文档</p>
<ul>
<li><a href="https://github.com/kubernetes-incubator/bootkube" target="_blank" rel="external">Incubator BootKube</a></li>
<li><a href="https://speakerdeck.com/philips/kubecon-2016-self-hosted-scale-and-federation-with-kubernetes-v1-dot-4-and-beyond" target="_blank" rel="external">Slide</a></li>
<li><a href="https://docs.google.com/document/d/1VNp4CMjPPHevh2_JQGMl-hpz9JSLq3s7HlI87CTjl-8/edit" target="_blank" rel="external">Design of bootkube</a></li>
<li><a href="https://coreos.com/blog/self-hosted-kubernetes.html" target="_blank" rel="external">Blog of self-booted kubernetes</a></li>
<li><a href="https://docs.google.com/document/d/1_I6xT0XHCoOqZUT-dtpxzwvYpTR5JmFQY0S4gL2PPkU/edit#heading=h.yeahbhtih70g" target="_blank" rel="external">Kubelet as a Container and Self-Hosted Kubernetes</a></li>
</ul>
<h2 id="F2F"><a href="#F2F" class="headerlink" title="F2F"></a>F2F</h2><p><strong><a href="https://docs.google.com/document/d/1ZVQIzLuHsBFDCw-QLO30rqRlSNSc1xynfs9GAFrjeVc/edit" target="_blank" rel="external">Sig-node F2F</a></strong></p>
<ul>
<li>Performance-Sensitive Application Platform<ul>
<li>高性能应用的场景，比如telecom, HFT等，需要NUMA、GPU、sysctls、hugepage、cpuset等等的支持。</li>
<li>一个idea是通过daemonset并配合NodeAllocatable</li>
</ul>
</li>
<li>CRI blockers and progress<ul>
<li>1.5的feature基本完成，只剩下一些bug fixes</li>
<li>per-pod cgroups</li>
<li>monitoring</li>
<li>api versioning</li>
<li>hostport network</li>
<li>metrics</li>
<li>CRI validation tests</li>
<li>auth</li>
</ul>
</li>
<li>Runtime agnostic debugging tools<ul>
<li>CLI for CRI?</li>
</ul>
</li>
<li>NodeSpec standardization</li>
<li>Packaging</li>
</ul>
<p><strong><a href="https://docs.google.com/document/d/1wtJeXhiVOL7qdDK_zouZPjskTIrsOLmD-9Ij478y7_Y/edit" target="_blank" rel="external">OCI F2F</a></strong></p>
<p>最主要的是<code>Runtime CLI Spec</code>和<code>Image Spec rc3</code>。比较有趣的是<a href="https://github.com/vbatts/nspawn-oci" target="_blank" rel="external">systemd wrapper / rkt wrapper for OCI runtime-spec CLI</a>，rkt也要加入OCI的大营。</p>
<h2 id="Developer-Summit"><a href="#Developer-Summit" class="headerlink" title="Developer Summit"></a>Developer Summit</h2><p><strong><a href="https://docs.google.com/presentation/d/1SD6a6eJl47t0qyTFE8GzaiytW4T_crdWgYAMCaLy1W8" target="_blank" rel="external">Scaling the Kubernetes CodeBase</a></strong></p>
<p>Kubernetes使用github来管理代码库，但现在碰到了明显的瓶颈：从2015年下半年开始<a href="http://velodrome.k8s.io/dashboard/db/kubernetes-developer-velocity" target="_blank" rel="external">merge时间明显加长</a>，open issues和PRs一直再增长，大量无关紧要的github通知等等。未来计划将Kubernetes的代码拆分到多个repo中，kuberentes代码库只保留核心代码，并通过Extension mechanism来支持各种功能：</p>
<ul>
<li>Apiserver federation</li>
<li>Authorization hooks</li>
<li>Admission-control hooks</li>
<li>Initializers and finalizers</li>
<li>ThirdPartyResource</li>
<li>Kubectl extensions</li>
<li>Service Broker, Operators</li>
<li>Controller pattern</li>
<li>External cloudproviders</li>
<li>CRI, network, and storage plugins</li>
<li>Cluster addons: UI, monitoring, logging</li>
<li>Feature gates, feature discovery, dependency management</li>
</ul>
<p>更多记录见<a href="https://docs.google.com/document/d/1zN2DWKerXwbzxZTO52wBRqp_uHMdLp8P52xYOmp5WZ4/edit" target="_blank" rel="external">这里</a>.</p>
<p><strong>其他的Summit简介</strong></p>
<ul>
<li><a href="https://docs.google.com/presentation/d/1dFfN3_9VM4cRKknZB9_0wsM_1YLJToRTEx7dX6BoEhI" target="_blank" rel="external">Kubernetes 2017 Features &amp; Roadmaps</a> <a href="https://docs.google.com/spreadsheets/d/154cAee2mOn3LoQDgpgG2ZzAIdIlQ_KnMlghGjnGwQ1w" target="_blank" rel="external">Roadmap</a><ul>
<li>Scale the project (tablestakes)</li>
<li>Reference architectures for application workflows</li>
<li>Secure multi-tenancy with service catalog</li>
<li>Production ready cluster lifecycle</li>
<li>Multi-cloud support for AWS / Azure / on premises</li>
</ul>
</li>
<li><a href="https://docs.google.com/document/d/11kK39Zz3zxIY9-GFa8buqqWYOrctFQ7hvOw5NBFwJL0/edit#heading=h.n0vlwrfu65r1" target="_blank" rel="external">Cluster Lifecycle Deployment &amp; Upgrade Roadmap</a><ul>
<li>HA</li>
<li>Upgrades</li>
<li>Config Management</li>
<li>Toolbox vs. guided flow</li>
<li>Documentation</li>
<li>Conformance Testing</li>
<li>PKI</li>
</ul>
</li>
<li><a href="https://docs.google.com/presentation/d/1GFuKgN-1kMmcg41T9HsasP8YEWtMAipEibYnwGkQuHo" target="_blank" rel="external">Documentation</a> and <a href="https://docs.google.com/document/d/1VYoVl63Iq2QSKxpoV7HGXmWasxU5EQCAuoIItj-ODvM/edit" target="_blank" rel="external">here</a></li>
<li><a href="https://docs.google.com/document/d/1cJhpjRcXpTmTWQswEfBqSbrfqrawtD9ZlWW30Q3BWv4/edit" target="_blank" rel="external">K8s Dev Summit: Outstanding Issues and PRs</a><ul>
<li>Issues 4600+, PRs 600+，还在不断增长中</li>
<li><a href="https://github.com/kubernetes/community/tree/master/sig-contribx" target="_blank" rel="external">Sig Contribx</a>会跟进处理这个问题</li>
</ul>
</li>
<li><a href="https://docs.google.com/document/d/1WmRwT3nhLXTYRLKlj9rjZmEXv3BEu7TFX29nuaO3enA/edit#heading=h.w90umf5dyer3" target="_blank" rel="external">Multi-tenancy</a></li>
<li><a href="https://docs.google.com/document/d/1X5i-Z3GsFyknxq4LyB3cObXEm5ds7DTUYOKwHAMHK5A/edit" target="_blank" rel="external">Azure &amp; AWS Kubernetes Discussion</a></li>
<li><a href="https://docs.google.com/document/d/1klHgGFKtSPHGNlG24LDb5-ornBUtZLcUnEjF8XKXDes/edit" target="_blank" rel="external">Contrib notes</a> and <a href="https://docs.google.com/document/d/19B2vcK6Y3xE3JO7sd4n6lPF0m9bBVZpNucvcR3MwEXA/edit" target="_blank" rel="external">On developer onboarding</a></li>
<li><a href="https://docs.google.com/document/d/1p7scsTPzPyouktBFTxu4RhRwW8yUn5Lj7VGY9SaOo-8/edit" target="_blank" rel="external">Compute resource management</a></li>
<li><a href="https://docs.google.com/document/d/1K2hh7nQ9glYzGE-5J7oKBB7oK3S_MKqwCISXZK-sB2Q/edit" target="_blank" rel="external">Logging volumes</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;题记：上周去西雅图参加了&lt;a href=&quot;http://events.linuxfoundation.org/events/kubecon&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;KubeCon&amp;amp;CloudNativeCon 2016&lt;/a&gt;
    
    </summary>
    
    
      <category term="kubernetes" scheme="http://feisky.xyz/tags/kubernetes/"/>
    
      <category term="kubecon" scheme="http://feisky.xyz/tags/kubecon/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes container runtime interface</title>
    <link href="http://feisky.xyz/2016/09/24/Kubernetes-container-runtime-interface/"/>
    <id>http://feisky.xyz/2016/09/24/Kubernetes-container-runtime-interface/</id>
    <published>2016-09-24T22:59:31.000Z</published>
    <updated>2017-03-17T01:40:56.314Z</updated>
    
    <content type="html"><![CDATA[<p>题记：最近一段时间在做Kubernetes容器引擎接口（Container Runtime Interface， CRI）的重构，并支持以插件的方式引入外部容器引擎。CRI还在紧张有序的开发中，预计在v1.5发布第一个alpha版。</p>
<h2 id="什么是CRI"><a href="#什么是CRI" class="headerlink" title="什么是CRI"></a>什么是CRI</h2><p>CRI是Kubelet（负责管理容器生命周期的服务）与容器引擎之间的接口。为了适应多种不同的容器引擎，Kubelet在加入rkt的时候就已经在docker API的基础上抽象了一个Runtime接口，只是由于一些特定的缺陷，在这个接口上不太容易引入其他新的容器引擎：</p>
<ul>
<li>Runtime接口的抽象度太高，导致一些原本该在Kubelet控制的逻辑被放到了Runtime实现里面。比如在当前的实现中，rkt和docker的<code>SyncPod</code>（负责Pod创建的接口）存在大量重复的逻辑，每次修改docker部分的时，都有可能需要同时修改rkt部分。这样，如果再加入新的容器引擎的话，同时修改多个Runtime部分的代码是没法维护的。</li>
<li>Runtime接口是集成在Kubelet内部的，集成容器引擎相关的代码需要放到Kubernetes代码库里面，这同样带来了维护的问题：代码维护麻烦，任何一个容器引擎修改了代码都需要发布新的kubelet；集成测试麻烦，要为每个不同的容器引擎部署不同的集成测试环境。</li>
<li>没有提供容器创建的接口，无法直接在Kubelet里面做到对容器的精细控制。</li>
<li>耦合了镜像和容器管理，而它们的生命周期本来就是独立的。</li>
</ul>
<p>既然Runtime接口有很多问题，并且有很多容器引擎想要集成到Kubernetes中，所以有必要重新定义CRI，并且提供一种插件机制，允许容器引擎以外部独立进程的方式接入。所以，Brendan Burns在<a href="http://hypercontainer.io" target="_blank" rel="external">Hyper</a>集成的时候就提供了一种以客户端/服务器方式接入外部容器引擎的思路。在大量的社区讨论后，Node team重新抽象了容器引擎接口（也就是CRI），并决定以gRPC的方式接入外部容器引擎。</p>
<h2 id="CRI是如何工作的"><a href="#CRI是如何工作的" class="headerlink" title="CRI是如何工作的"></a>CRI是如何工作的</h2><p>CRI比Runtime接口提供了更细粒度的抽象，解耦了镜像管理和容器管理，并为Pod和Container提供了独立的操作接口。CRI以gRPC的方式接入，Kubelet是gPRC API的客户端，而容器引擎则是gRPC API的服务端。gRPC已经自动实现了它们之间交互的细节，容器引擎只需要实现每个具体的API。</p>
<p>一个典型的启动Pod的流程为</p>
<p><img src="/images/createpod.png" alt="createpod"></p>
<p>而停止Pod的流程为</p>
<p><img src="/images/killpod.png" alt="killpod"></p>
<h2 id="CRI带了什么"><a href="#CRI带了什么" class="headerlink" title="CRI带了什么"></a>CRI带了什么</h2><p>CRI解决了上述提到的Runtime接口的问题，使得新的容器引擎可以更方便的集成到Kubernetes中来，这必将给Kubernetes社区带来新一轮的变革，并促进Kubernetes走入更多的应用场景中。比如，Redhat借助OCI容器引擎runc摆脱对docker依赖，Hyper以虚拟化的方式解决多租户场景下的容器隔离问题，甚至Mirantis直接用Kubernetes来管理原生的虚拟机。</p>
<p>CRI也解耦了容器和镜像的管理，可以方便的扩展其他镜像格式，比如ACI等。</p>
<p>CRI还在着力解决一些很有挑战的问题，比如</p>
<ul>
<li>容器日志的管理，包括日志格式化规范、日志文件rotate、日志文件磁盘IO控制以及日志的统一收集处理等。</li>
<li>解除streaming API（exec、attach、logs等）对kubelet的网络压力。当前所有的streaming API都是从<code>apiserver-&gt;kubelet-&gt;runtime</code>，apiserver是无状态的，可以水平扩展，但kubelet和runtime则是每台机器只能有一个，streaming API有可能会给他们带来处理的瓶颈。所以在CRI中，将会考虑使用一个独立进程（需要对apiserver开放端口）来单独处理这些请求<code>apiserver-&gt;newStreamProcess</code>，释放kubelet来做更核心的事情。</li>
<li>更灵活的网络配置，将Pod网络的配置完全交给容器引擎，而Kubernetes只需要最终的网络状态。</li>
</ul>
<h2 id="CRI的未来"><a href="#CRI的未来" class="headerlink" title="CRI的未来"></a>CRI的未来</h2><p>虽然CRI还在持续开发中（目前还没有任何release），但已经有很多厂商已经开始了引入新容器引擎的进程：</p>
<ul>
<li>Frakti：为解决多租户场景下的容器隔离问题，Hyper以虚拟化的方式运行容器。关于frakti的更多细节见<a href="https://github.com/kubernetes/frakti。" target="_blank" rel="external">https://github.com/kubernetes/frakti。</a></li>
<li>OCI-O：为解耦对docker的依赖，Redhat提供对OCI容器引擎的支持（目前主要是runc）。关于oci-o的更多细节见<a href="https://github.com/kubernetes-incubator/oci-o。" target="_blank" rel="external">https://github.com/kubernetes-incubator/oci-o。</a></li>
<li>Rktlet：为了加速rkt容器引擎的开发维护，CoreOS提议将rkt集成的代码独立出Kubelet（vendor到kubelet，同集成到kubelet内部便于发布），并重构rkt以适应CRI的变化。关于rktlet的更多细节见<a href="https://github.com/kubernetes-incubator/rktlet。" target="_blank" rel="external">https://github.com/kubernetes-incubator/rktlet。</a></li>
<li>Virtlet：为了支持原生的虚拟机管理，Mirantis提议直接用Kubernetes来管理原生的虚拟机（需要将docker镜像替换成qcow2镜像）。关于virtlet的更多细节见<a href="https://github.com/Mirantis/virtlet。" target="_blank" rel="external">https://github.com/Mirantis/virtlet。</a></li>
<li>当然，docker相关的代码还会继续保留在kubelet内部，只不过要重构到CRI上面来。</li>
</ul>
<p>CRI预计在Kubernetes v1.5发布第一个alpha版。届时，上面各个容器引擎的实现也将会发布第一个release。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;题记：最近一段时间在做Kubernetes容器引擎接口（Container Runtime Interface， CRI）的重构，并支持以插件的方式引入外部容器引擎。CRI还在紧张有序的开发中，预计在v1.5发布第一个alpha版。&lt;/p&gt;
&lt;h2 id=&quot;什么是CRI&quot;&gt;
    
    </summary>
    
    
      <category term="kubernetes" scheme="http://feisky.xyz/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes中的服务发现与负载均衡</title>
    <link href="http://feisky.xyz/2016/09/11/Kubernetes%E4%B8%AD%E7%9A%84%E6%9C%8D%E5%8A%A1%E5%8F%91%E7%8E%B0%E4%B8%8E%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/"/>
    <id>http://feisky.xyz/2016/09/11/Kubernetes中的服务发现与负载均衡/</id>
    <published>2016-09-11T01:48:09.000Z</published>
    <updated>2017-03-17T01:40:56.314Z</updated>
    
    <content type="html"><![CDATA[<p>Kubernetes在设计之初就充分考虑了针对容器的服务发现与负载均衡机制，提供了Service资源，并通过kube-proxy配合cloud provider来适应不同的应用场景。随着kubernetes用户的激增，用户场景的不断丰富，又产生了一些新的负载均衡机制。目前，kubernetes中的负载均衡大致可以分为以下几种机制，每种机制都有其特定的应用场景：</p>
<ul>
<li>Service：直接用Service提供cluster内部的负载均衡，并借助cloud provider提供的LB提供外部访问</li>
<li>Ingress Controller：还是用Service提供cluster内部的负载均衡，但是通过自定义LB提供外部访问</li>
<li>Service Load Balancer：把load balancer直接跑在容器中，实现Bare Metal的Service Load Balancer</li>
<li>Custom Load Balancer：自定义负载均衡，并替代kube-proxy，一般在物理部署Kubernetes时使用，方便接入公司已有的外部服务</li>
</ul>
<h2 id="Service"><a href="#Service" class="headerlink" title="Service"></a>Service</h2><p><img src="/images/k8s-service.png" alt=""></p>
<p>Service是对一组提供相同功能的Pods的抽象，并为它们提供一个统一的入口。借助Service，应用可以方便的实现服务发现与负载均衡，并实现应用的零宕机升级。Service通过标签来选取服务后端，一般配合Replication Controller或者Deployment来保证后端容器的正常运行。</p>
<p>Service有三种类型：</p>
<ul>
<li>ClusterIP：默认类型，自动分配一个仅cluster内部可以访问的虚拟IP</li>
<li>NodePort：在ClusterIP基础上为Service在每台机器上绑定一个端口，这样就可以通过<code>&lt;NodeIP&gt;:NodePort</code>来访问改服务</li>
<li>LoadBalancer：在NodePort的基础上，借助cloud provider创建一个外部的负载均衡器，并将请求转发到<code>&lt;NodeIP&gt;:NodePort</code></li>
</ul>
<p>另外，也可以讲已有的服务以Service的形式加入到Kubernetes集群中来，只需要在创建Service的时候不指定Label selector，而是在Service创建好后手动为其添加endpoint。</p>
<h2 id="Ingress-Controller"><a href="#Ingress-Controller" class="headerlink" title="Ingress Controller"></a>Ingress Controller</h2><p>Service虽然解决了服务发现和负载均衡的问题，但它在使用上还是有一些限制，比如</p>
<p>－ 只支持4层负载均衡，没有7层功能<br>－ 对外访问的时候，NodePort类型需要在外部搭建额外的负载均衡，而LoadBalancer要求kubernetes必须跑在支持的cloud provider上面</p>
<p>Ingress就是为了解决这些限制而引入的新资源，主要用来将服务暴露到cluster外面，并且可以自定义服务的访问策略。比如想要通过负载均衡器实现不同子域名到不同服务的访问：</p>
<figure class="highlight gherkin"><table><tr><td class="code"><pre><div class="line">foo.bar.com --|<span class="string">                 </span>|<span class="string">-&gt; foo.bar.com s1:80</span></div><div class="line">              |<span class="string"> 178.91.123.132  </span>|</div><div class="line">bar.foo.com --|<span class="string">                 </span>|<span class="string">-&gt; bar.foo.com s2:80</span></div></pre></td></tr></table></figure>
<p>可以这样来定义Ingress：</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><div class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></div><div class="line"><span class="attr">kind:</span> <span class="string">Ingress</span></div><div class="line"><span class="attr">metadata:</span></div><div class="line"><span class="attr">  name:</span> <span class="string">test</span></div><div class="line"><span class="attr">spec:</span></div><div class="line"><span class="attr">  rules:</span></div><div class="line"><span class="attr">  - host:</span> <span class="string">foo.bar.com</span></div><div class="line"><span class="attr">    http:</span></div><div class="line"><span class="attr">      paths:</span></div><div class="line"><span class="attr">      - backend:</span></div><div class="line"><span class="attr">          serviceName:</span> <span class="string">s1</span></div><div class="line"><span class="attr">          servicePort:</span> <span class="number">80</span></div><div class="line"><span class="attr">  - host:</span> <span class="string">bar.foo.com</span></div><div class="line"><span class="attr">    http:</span></div><div class="line"><span class="attr">      paths:</span></div><div class="line"><span class="attr">      - backend:</span></div><div class="line"><span class="attr">          serviceName:</span> <span class="string">s2</span></div><div class="line"><span class="attr">          servicePort:</span> <span class="number">80</span></div></pre></td></tr></table></figure>
<p>注意Ingress本身并不会自动创建负载均衡器，cluster中需要运行一个ingress controller来根据Ingress的定义来管理负载均衡器。目前社区提供了nginx和gce的参考实现。</p>
<h2 id="Service-Load-Balancer"><a href="#Service-Load-Balancer" class="headerlink" title="Service Load Balancer"></a>Service Load Balancer</h2><p>在Ingress出现以前，Service Load Balancer是推荐的解决Service局限性的方式。Service Load Balancer将haproxy跑在容器中，并监控service和endpoint的变化，通过容器IP对外提供4层和7层负载均衡服务。</p>
<p>社区提供的Service Load Balancer支持四种负载均衡协议：TCP、HTTP、HTTPS和SSL TERMINATION，并支持ACL访问控制。</p>
<h2 id="Custom-Load-Balancer"><a href="#Custom-Load-Balancer" class="headerlink" title="Custom Load Balancer"></a>Custom Load Balancer</h2><p>虽然Kubernetes提供了丰富的负载均衡机制，但在实际使用的时候，还是会碰到一些复杂的场景是它不能支持的，比如</p>
<ul>
<li>接入已有的负载均衡设备</li>
<li>多租户网络情况下，容器网络和主机网络是隔离的，这样<code>kube-proxy</code>就不能正常工作</li>
</ul>
<p>这个时候就可以自定义组件，并代替kube-proxy来做负载均衡。基本的思路是监控kubernetes中service和endpoints的变化，并根据这些变化来配置负载均衡器。比如weave flux、nginx plus、kube2haproxy等</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a href="http://kubernetes.io/docs/user-guide/services/" target="_blank" rel="external">http://kubernetes.io/docs/user-guide/services/</a></li>
<li><a href="http://kubernetes.io/docs/user-guide/ingress/" target="_blank" rel="external">http://kubernetes.io/docs/user-guide/ingress/</a></li>
<li><a href="https://github.com/kubernetes/contrib/tree/master/service-loadbalancer" target="_blank" rel="external">https://github.com/kubernetes/contrib/tree/master/service-loadbalancer</a></li>
<li><a href="https://www.nginx.com/blog/load-balancing-kubernetes-services-nginx-plus/" target="_blank" rel="external">https://www.nginx.com/blog/load-balancing-kubernetes-services-nginx-plus/</a></li>
<li><a href="https://github.com/weaveworks/flux" target="_blank" rel="external">https://github.com/weaveworks/flux</a></li>
<li><a href="https://github.com/AdoHe/kube2haproxy" target="_blank" rel="external">https://github.com/AdoHe/kube2haproxy</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Kubernetes在设计之初就充分考虑了针对容器的服务发现与负载均衡机制，提供了Service资源，并通过kube-proxy配合cloud provider来适应不同的应用场景。随着kubernetes用户的激增，用户场景的不断丰富，又产生了一些新的负载均衡机制。目前，
    
    </summary>
    
    
      <category term="kubernetes" scheme="http://feisky.xyz/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>如何快速启动一个Kubernetes集群</title>
    <link href="http://feisky.xyz/2016/08/24/%E5%A6%82%E4%BD%95%E5%BF%AB%E9%80%9F%E5%90%AF%E5%8A%A8%E4%B8%80%E4%B8%AAKubernetes%E9%9B%86%E7%BE%A4/"/>
    <id>http://feisky.xyz/2016/08/24/如何快速启动一个Kubernetes集群/</id>
    <published>2016-08-24T06:48:44.000Z</published>
    <updated>2017-03-17T01:40:56.314Z</updated>
    
    <content type="html"><![CDATA[<p>相比Docker一个二进制文件解决所有问题，Kubernetes则为不同的服务提供了不同的二进制文件，并将一些服务放到了addons中。故而，Kubernetes的部署相对要麻烦的多。借助<a href="https://github.com/kubernetes/minikube" target="_blank" rel="external">minikube</a>项目，现在可以很方便的在本机快速启动一个单节点的Kubernetes集群。</p>
<h2 id="安装minikube"><a href="#安装minikube" class="headerlink" title="安装minikube"></a>安装minikube</h2><p>minikube最新release版本为v0.8.0，支持Kubernetes v1.3.0到v1.3.5的各个版本，默认启动Kubernetes v1.3.5。</p>
<p>OSX</p>
<figure class="highlight awk"><table><tr><td class="code"><pre><div class="line">curl -Lo minikube https:<span class="regexp">//</span>storage.googleapis.com<span class="regexp">/minikube/</span>releases<span class="regexp">/v0.8.0/mi</span>nikube-darwin-amd64 &amp;&amp; chmod +x minikube &amp;&amp; sudo mv minikube <span class="regexp">/usr/</span>local<span class="regexp">/bin/</span></div></pre></td></tr></table></figure>
<p>Linux</p>
<figure class="highlight awk"><table><tr><td class="code"><pre><div class="line">curl -Lo minikube https:<span class="regexp">//</span>storage.googleapis.com<span class="regexp">/minikube/</span>releases<span class="regexp">/v0.8.0/mi</span>nikube-linux-amd64 &amp;&amp; chmod +x minikube &amp;&amp; sudo mv minikube <span class="regexp">/usr/</span>local<span class="regexp">/bin/</span></div></pre></td></tr></table></figure>
<p>Windows</p>
<figure class="highlight vim"><table><tr><td class="code"><pre><div class="line">下载http<span class="variable">s:</span>//storage.googleapis.<span class="keyword">com</span>/minikube/releases/v0.<span class="number">8.0</span>/minikube-windows-amd64.<span class="keyword">exe</span>，并重命名为minikube.<span class="keyword">exe</span></div></pre></td></tr></table></figure>
<p>minikube支持xhyve(on OSX)、VirtualBox、VMWare Fusion等多种不同的driver，这些driver也需要单独安装，比如在OSX上安装xhyve driver:</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><div class="line">brew install docker-machine-driver-xhyve</div><div class="line"><span class="comment"># docker-machine-driver-xhyve need root owner and uid</span></div><div class="line">sudo chown root:wheel $(brew --prefix)/opt/docker-machine-driver-xhyve/bin/docker-machine-driver-xhyve</div><div class="line">sudo chmod u+s $(brew --prefix)/opt/docker-machine-driver-xhyve/bin/docker-machine-driver-xhyve</div></pre></td></tr></table></figure>
<p>另外，还需要安装一个<code>kubectl</code>客户端，用来跟kubernetes交互：</p>
<figure class="highlight cmake"><table><tr><td class="code"><pre><div class="line">gcloud components <span class="keyword">install</span> kubectl</div></pre></td></tr></table></figure>
<h2 id="启动Kubernetes-Cluster"><a href="#启动Kubernetes-Cluster" class="headerlink" title="启动Kubernetes Cluster"></a>启动Kubernetes Cluster</h2><p>启动Kubernetes Cluster就非常简单了，一个命令即可：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><div class="line">$ minikube <span class="keyword">start</span></div><div class="line"><span class="keyword">Starting</span> <span class="keyword">local</span> Kubernetes cluster...</div><div class="line">Kubectl <span class="keyword">is</span> <span class="keyword">now</span> configured <span class="keyword">to</span> <span class="keyword">use</span> the cluster.</div></pre></td></tr></table></figure>
<p>当然了，国内环境下，最好加上代理：</p>
<figure class="highlight armasm"><table><tr><td class="code"><pre><div class="line"><span class="symbol">minikube</span> start --docker-env HTTP_PROXY<span class="symbol">=http</span>://proxy-<span class="built_in">ip</span>:port --docker-env HTTPS_PROXY<span class="symbol">=http</span>://proxy-<span class="built_in">ip</span>:port</div></pre></td></tr></table></figure>
<p>然后就可以通过kubectl来玩Kubernetes了，比如启动一个简单的nginx服务：</p>
<figure class="highlight livecodeserver"><table><tr><td class="code"><pre><div class="line">$ kubectl run nginx <span class="comment">--image=nginx --port=80</span></div><div class="line">deployment <span class="string">"nginx"</span> created</div><div class="line">$ kubectl expose deployment nginx <span class="comment">--port=80 --type=NodePort --name=nginx-http</span></div><div class="line">service <span class="string">"nginx-http"</span> exposed</div><div class="line">$ kubectl <span class="built_in">get</span> pods</div><div class="line">NAME                     READY     STATUS    RESTARTS   AGE</div><div class="line">nginx<span class="number">-2032906785</span><span class="number">-81</span>t56   <span class="number">1</span>/<span class="number">1</span>       Running   <span class="number">0</span>          <span class="number">2</span>m</div><div class="line">$ kubectl <span class="built_in">get</span> services</div><div class="line">NAME         CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE</div><div class="line">kubernetes   <span class="number">10.0</span><span class="number">.0</span><span class="number">.1</span>     &lt;<span class="literal">none</span>&gt;        <span class="number">443</span>/TCP   <span class="number">20</span>m</div><div class="line">nginx-<span class="keyword">http</span>   <span class="number">10.0</span><span class="number">.0</span><span class="number">.146</span>   &lt;<span class="literal">none</span>&gt;        <span class="number">80</span>/TCP    <span class="number">2</span>m</div><div class="line">$ minikube service nginx-<span class="keyword">http</span> <span class="comment">--url</span></div><div class="line"><span class="keyword">http</span>://<span class="number">192.168</span><span class="number">.64</span><span class="number">.10</span>:<span class="number">30569</span></div></pre></td></tr></table></figure>
<p>这样就可以通过<code>http://192.168.64.10:30569</code>来直接访问nginx服务。</p>
<p>minikube默认还部署了最新的dashboard，可以通过<code>minikube dashboard</code>命令在默认浏览器中打开：</p>
<p><img src="/images/k8s-dashboard.png" alt="k8s-dashboard"></p>
<p>更多的玩法可以参考minikube的帮助文档：</p>
<pre><code>Usage:
  minikube [command]

Available Commands:
  dashboard        Opens/displays the kubernetes dashboard URL for your local cluster
  delete           Deletes a local kubernetes cluster.
  docker-env       sets up docker env variables; similar to &#39;$(docker-machine env)&#39;
  get-k8s-versions Gets the list of available kubernetes versions available for minikube.
  ip               Retrieve the IP address of the running cluster.
  logs             Gets the logs of the running localkube instance, used for debugging minikube, not user code.
  service          Gets the kubernetes URL for the specified service in your local cluster
  ssh              Log into or run a command on a machine with SSH; similar to &#39;docker-machine ssh&#39;
  start            Starts a local kubernetes cluster.
  status           Gets the status of a local kubernetes cluster.
  stop             Stops a running local kubernetes cluster.
  version          Print the version of minikube.
</code></pre><p>更多请参考<a href="https://github.com/kubernetes/minikube。" target="_blank" rel="external">https://github.com/kubernetes/minikube。</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;相比Docker一个二进制文件解决所有问题，Kubernetes则为不同的服务提供了不同的二进制文件，并将一些服务放到了addons中。故而，Kubernetes的部署相对要麻烦的多。借助&lt;a href=&quot;https://github.com/kubernetes/mini
    
    </summary>
    
    
      <category term="docker" scheme="http://feisky.xyz/tags/docker/"/>
    
      <category term="Kubernetes" scheme="http://feisky.xyz/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Setup hyperd with flannel network</title>
    <link href="http://feisky.xyz/2016/07/19/Setup-hyperd-with-flannel-network/"/>
    <id>http://feisky.xyz/2016/07/19/Setup-hyperd-with-flannel-network/</id>
    <published>2016-07-19T07:58:26.000Z</published>
    <updated>2017-03-17T01:40:56.314Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Flannel"><a href="#Flannel" class="headerlink" title="Flannel"></a>Flannel</h1><p>Flannel is a virtual network that gives a subnet to each host for use with container runtimes.</p>
<p>Platforms like Google’s Kubernetes assume that each container (pod) has a unique, routable IP inside the cluster. The advantage of this model is that it reduces the complexity of doing port mapping.</p>
<p>flannel runs an agent, flanneld, on each host and is responsible for allocating a subnet lease out of a preconfigured address space. flannel uses etcd to store the network configuration, allocated subnets, and auxiliary data (such as host’s IP). The forwarding of packets is achieved using one of several strategies that are known as backends. The simplest backend is udp and uses a TUN device to encapsulate every IP fragment in a UDP packet, forming an overlay network. The following diagram demonstrates the path a packet takes as it traverses the overlay network:</p>
<p><img src="/images/14689151388980.jpg" alt=""></p>
<h1 id="Flannel-install"><a href="#Flannel-install" class="headerlink" title="Flannel install"></a>Flannel install</h1><p>First install etcd:</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><div class="line">curl -L https://github.com/coreos/etcd/releases/download/v3.0.3/etcd-v3.0.3-linux-amd64.tar.gz -o etcd-v3.0.3-linux-amd64.tar.gz</div><div class="line">tar xzvf etcd-v3.0.3-linux-amd64.tar.gz</div><div class="line">cp etcd-v3.0.3-linux-amd64/&#123;etcd,etcdctl&#125; /usr/bin</div><div class="line">rm -rf etcd-v3.0.3-linux-amd64 etcd-v3.0.3-linux-amd64.tar.gz</div></pre></td></tr></table></figure>
<p>Then, install flannel:</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><div class="line">curl -L https://github.com/coreos/flannel/releases/download/v0.5.5/flannel-0.5.5-linux-amd64.tar.gz -o flannel-0.5.5-linux-amd64.tar.gz</div><div class="line">tar zxvf flannel-0.5.5-linux-amd64.tar.gz</div><div class="line">cp flannel-0.5.5/flanneld /usr/bin</div><div class="line">rm -rf flannel-0.5.5*</div></pre></td></tr></table></figure>
<p>Start etcd and setup default network:</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><div class="line">nohup etcd --advertise-client-urls <span class="string">'http://192.168.33.10:2379'</span> --listen-client-urls <span class="string">'http://192.168.33.10:2379'</span> &amp;</div><div class="line">etcdctl --endpoints=192.168.33.10:2379 <span class="built_in">set</span> /coreos.com/network/config  <span class="string">'&#123; "Network": "172.168.0.0/16", "Backend": &#123; "Type": "vxlan", "VNI": 2000 &#125; &#125;'</span></div></pre></td></tr></table></figure>
<p>Start flanneld on all nodes:</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><div class="line">nohup flanneld -etcd-endpoints=http://192.168.33.10:2379 -iface=eth1 &amp;</div></pre></td></tr></table></figure>
<h1 id="Hyperd-install"><a href="#Hyperd-install" class="headerlink" title="Hyperd install"></a>Hyperd install</h1><figure class="highlight sh"><table><tr><td class="code"><pre><div class="line">apt-get install qemu-system-x86 -y</div><div class="line">curl <span class="_">-s</span>SL http://hypercontainer.io/install | bash</div></pre></td></tr></table></figure>
<p>Configure hyperd to use subnet provided by flannel：</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><div class="line"><span class="built_in">source</span> /run/flannel/subnet.env</div><div class="line">brctl addbr docker0</div><div class="line">ip addr add dev docker0 <span class="variable">$&#123;FLANNEL_SUBNET&#125;</span></div><div class="line">ip link <span class="built_in">set</span> docker0 up</div><div class="line"></div><div class="line">cat &gt;/etc/hyper/config &lt;&lt;EOF</div><div class="line">Kernel=/var/lib/hyper/kernel</div><div class="line">Initrd=/var/lib/hyper/hyper-initrd.img</div><div class="line">Hypervisor=qemu</div><div class="line">StorageDriver=devicemapper</div><div class="line">Bridge=docker0</div><div class="line">BridgeIP=<span class="variable">$&#123;FLANNEL_SUBNET&#125;</span></div><div class="line">EOF</div><div class="line"></div><div class="line">nohup hyperd --nondaemon --v=3 &amp;</div></pre></td></tr></table></figure>
<h1 id="Test"><a href="#Test" class="headerlink" title="Test"></a>Test</h1><figure class="highlight sh"><table><tr><td class="code"><pre><div class="line">root@s2:~<span class="comment"># hyper run -d busybox</span></div><div class="line">POD id is pod-hZviZLulsb</div><div class="line">Time to run a POD is 3648 ms</div><div class="line">root@s2:~<span class="comment"># hyper exec pod-hZviZLulsb ip addr</span></div><div class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue</div><div class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</div><div class="line">    inet 127.0.0.1/8 scope host lo</div><div class="line">       valid_lft forever preferred_lft forever</div><div class="line">    inet6 ::1/128 scope host</div><div class="line">       valid_lft forever preferred_lft forever</div><div class="line">2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast qlen 1000</div><div class="line">    link/ether 52:54:51:e5:db:2f brd ff:ff:ff:ff:ff:ff</div><div class="line">    inet 172.168.12.3/24 scope global eth0</div><div class="line">       valid_lft forever preferred_lft forever</div><div class="line">    inet6 fe80::5054:51ff:fee5:db2f/64 scope link</div><div class="line">       valid_lft forever preferred_lft forever</div><div class="line"></div><div class="line">root@s1:~<span class="comment"># hyper run -d busybox</span></div><div class="line">POD id is pod-GbccOdYKjK</div><div class="line">Time to run a POD is 3631 ms</div><div class="line">root@s1:~<span class="comment"># hyper exec pod-GbccOdYKjK ip addr</span></div><div class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue</div><div class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</div><div class="line">    inet 127.0.0.1/8 scope host lo</div><div class="line">       valid_lft forever preferred_lft forever</div><div class="line">    inet6 ::1/128 scope host</div><div class="line">       valid_lft forever preferred_lft forever</div><div class="line">2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast qlen 1000</div><div class="line">    link/ether 52:54:da:0c:b6:<span class="built_in">cd</span> brd ff:ff:ff:ff:ff:ff</div><div class="line">    inet 172.168.95.3/24 scope global eth0</div><div class="line">       valid_lft forever preferred_lft forever</div><div class="line">    inet6 fe80::5054:daff:fe0c:b6<span class="built_in">cd</span>/64 scope link</div><div class="line">       valid_lft forever preferred_lft forever</div><div class="line">root@s1:~<span class="comment"># hyper exec pod-GbccOdYKjK ping -c3 172.168.12.3</span></div><div class="line">PING 172.168.12.3 (172.168.12.3): 56 data bytes</div><div class="line">64 bytes from 172.168.12.3: seq=0 ttl=62 time=57.400 ms</div><div class="line">64 bytes from 172.168.12.3: seq=1 ttl=62 time=6.563 ms</div><div class="line">64 bytes from 172.168.12.3: seq=2 ttl=62 time=1.580 ms</div><div class="line"></div><div class="line">--- 172.168.12.3 ping statistics ---</div><div class="line">3 packets transmitted, 3 packets received, 0% packet loss</div><div class="line">round-trip min/avg/max = 1.580/21.847/57.400 ms</div></pre></td></tr></table></figure>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul>
<li><a href="https://github.com/coreos/flannel" target="_blank" rel="external">https://github.com/coreos/flannel</a></li>
<li><a href="http://docs.hypercontainer.io/get_started/install/linux.html" target="_blank" rel="external">http://docs.hypercontainer.io/get_started/install/linux.html</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Flannel&quot;&gt;&lt;a href=&quot;#Flannel&quot; class=&quot;headerlink&quot; title=&quot;Flannel&quot;&gt;&lt;/a&gt;Flannel&lt;/h1&gt;&lt;p&gt;Flannel is a virtual network that gives a subnet t
    
    </summary>
    
    
      <category term="hyper" scheme="http://feisky.xyz/tags/hyper/"/>
    
      <category term="container" scheme="http://feisky.xyz/tags/container/"/>
    
      <category term="flannel" scheme="http://feisky.xyz/tags/flannel/"/>
    
  </entry>
  
  <entry>
    <title>Play with docker v1.12</title>
    <link href="http://feisky.xyz/2016/06/24/Play-with-docker-v1-12/"/>
    <id>http://feisky.xyz/2016/06/24/Play-with-docker-v1-12/</id>
    <published>2016-06-24T04:39:49.000Z</published>
    <updated>2017-03-17T01:40:56.314Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p>
<p>Docker v1.12 brings in its integrated orchestration into docker engine.</p>
<blockquote>
<p>Starting with Docker 1.12, we have added features to the core Docker Engine to make multi-host and multi-container orchestration easy.  We’ve added new API objects, like Service and Node, that will let you use the Docker API to deploy and manage apps on a group of Docker Engines called a swarm. With Docker 1.12, the best way to orchestrate Docker is Docker!</p>
</blockquote>
<p><img src="https://cloud.githubusercontent.com/assets/676637/16327966/a4f34346-3a07-11e6-8d21-153509596cec.png" alt="docker-v1 12"></p>
<h2 id="Playing-on-GCE"><a href="#Playing-on-GCE" class="headerlink" title="Playing on GCE"></a>Playing on GCE</h2><p>Create swarm-manager:</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><div class="line">gcloud init</div><div class="line">docker-machine create swarm-manager --engine-install-url experimental.docker.com <span class="_">-d</span> google --google-machine-type n1-standard-1 --google-zone us-central1<span class="_">-f</span> --google-disk-size <span class="string">"500"</span> --google-tags swarm-cluster --google-project k8s-dev-prj</div></pre></td></tr></table></figure>
<p>Check what version has been installed:</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><div class="line"><span class="string">$</span> <span class="string">eval</span> <span class="string">$(docker-machine</span> <span class="string">env</span> <span class="string">swarm-manager)</span></div><div class="line"><span class="string">$</span> <span class="string">docker</span> <span class="string">version</span></div><div class="line"><span class="attr">Client:</span></div><div class="line"><span class="attr"> Version:</span>      <span class="number">1.12</span><span class="number">.0</span><span class="bullet">-rc2</span></div><div class="line"> <span class="string">API</span> <span class="attr">version:</span>  <span class="number">1.24</span></div><div class="line"> <span class="string">Go</span> <span class="attr">version:</span>   <span class="string">go1.6.2</span></div><div class="line"> <span class="string">Git</span> <span class="attr">commit:</span>   <span class="number">906</span><span class="string">eacd</span></div><div class="line"><span class="attr"> Built:</span>        <span class="string">Fri</span> <span class="string">Jun</span> <span class="number">17</span> <span class="number">20</span><span class="string">:35:33</span> <span class="number">2016</span></div><div class="line"> <span class="string">OS/Arch:</span>      <span class="string">darwin/amd64</span></div><div class="line"><span class="attr"> Experimental:</span> <span class="literal">true</span></div><div class="line"></div><div class="line"><span class="attr">Server:</span></div><div class="line"><span class="attr"> Version:</span>      <span class="number">1.12</span><span class="number">.0</span><span class="bullet">-rc2</span></div><div class="line"> <span class="string">API</span> <span class="attr">version:</span>  <span class="number">1.24</span></div><div class="line"> <span class="string">Go</span> <span class="attr">version:</span>   <span class="string">go1.6.2</span></div><div class="line"> <span class="string">Git</span> <span class="attr">commit:</span>   <span class="number">906</span><span class="string">eacd</span></div><div class="line"><span class="attr"> Built:</span>        <span class="string">Fri</span> <span class="string">Jun</span> <span class="number">17</span> <span class="number">21</span><span class="string">:07:35</span> <span class="number">2016</span></div><div class="line"> <span class="string">OS/Arch:</span>      <span class="string">linux/amd64</span></div><div class="line"><span class="attr"> Experimental:</span> <span class="literal">true</span></div></pre></td></tr></table></figure>
<p>Create worker node:</p>
<figure class="highlight haml"><table><tr><td class="code"><pre><div class="line">docker-machine create swarm-worker-1 \</div><div class="line">    -<span class="ruby">-engine-install-url experimental.docker.com \</span></div><div class="line">    -<span class="ruby">d google \</span></div><div class="line">    -<span class="ruby">-google-machine-type n1-standard-<span class="number">1</span> \</span></div><div class="line">    -<span class="ruby">-google-zone us-central1-f \</span></div><div class="line">    -<span class="ruby">-google-disk-size <span class="string">"500"</span> \</span></div><div class="line">    -<span class="ruby">-google-tags swarm-cluster \</span></div><div class="line">    -<span class="ruby">-google-project k8s-dev-prj</span></div></pre></td></tr></table></figure>
<p>Initialize swarm</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><div class="line"><span class="comment"># init manager</span></div><div class="line"><span class="built_in">eval</span> $(docker-machine env swarm-manager)</div><div class="line">docker swarm init</div></pre></td></tr></table></figure>
<blockquote>
<p>Under the hood this creates a Raft consensus group of one node.  This first node has the role of manager, meaning it accepts commands and schedule tasks.  As you join more nodes to the swarm, they will by default be workers, which simply execute containers dispatched by the manager.  You can optionally add additional manager nodes.  The manager nodes will be part of the Raft consensus group. We use an optimized Raft store in which reads are serviced directly from memory which makes scheduling performance fast.</p>
</blockquote>
<figure class="highlight mel"><table><tr><td class="code"><pre><div class="line"># join worker</div><div class="line"><span class="keyword">eval</span> $(docker-machine <span class="keyword">env</span> swarm-worker<span class="number">-1</span>)</div><div class="line">manager_ip=$(gcloud compute instances list | awk <span class="string">'/swarm-manager/&#123;print $4&#125;'</span>)</div><div class="line">docker swarm join $&#123;manager_ip&#125;:<span class="number">2377</span></div></pre></td></tr></table></figure>
<p>List all nodes:</p>
<figure class="highlight crmsh"><table><tr><td class="code"><pre><div class="line">$ eval $(docker-machine env swarm-manager)</div><div class="line">$ docker <span class="keyword">node</span> <span class="title">ls</span></div><div class="line">ID                           NAME            MEMBERSHIP  STATUS  AVAILABILITY  MANAGER STATUS</div><div class="line"><span class="number">0m</span>2qy40ch1nqfpmhnsvj8jzch *  swarm-manager   Accepted    Ready   Active        Leader</div><div class="line"><span class="number">4</span>v1oo055unqiz9fy14u8wg3fn    swarm-worker-<span class="number">1</span>  Accepted    Ready   Active</div></pre></td></tr></table></figure>
<h2 id="Playing-with-service"><a href="#Playing-with-service" class="headerlink" title="Playing with service"></a>Playing with service</h2><figure class="highlight sh"><table><tr><td class="code"><pre><div class="line"><span class="built_in">eval</span> $(docker-machine env swarm-manager)</div><div class="line">docker service create --replicas 2 -p 80:80/tcp --name nginx nginx</div></pre></td></tr></table></figure>
<blockquote>
<p>This command declares a desired state on your swarm of 2 nginx containers, reachable as a single, internally load balanced service on port 80 of any node in your swarm.  Internally, we make this work using Linux IPVS, an in-kernel Layer 4 multi-protocol load balancer that’s been in the Linux kernel for more than 15 years.  With IPVS routing packets inside the kernel, swarm’s routing mesh delivers high performance container-aware load-balancing.</p>
<p>When you create services, can optionally create replicated or global services.  Replicated services mean any number of containers that you define will be spread across the available hosts.  Global services, by contrast, schedule one instance the same container on every host in the swarm.</p>
<p>Let’s turn to how Docker provides resiliency.  Swarm mode enabled engines are self-healing, meaning that they are aware of the application you defined and will continuously check and reconcile the environment when things go awry.  For example, if you unplug one of the machines running an nginx instance, a new container will come up on another node.  Unplug the network switch for half the machines in your swarm, and the other half will take over, redistributing the containers amongst themselves.  For updates, you now have flexibility in how you re-deploy services once you make a change. You can set a rolling or parallel update of the containers on your swarm.</p>
</blockquote>
<figure class="highlight sh"><table><tr><td class="code"><pre><div class="line">docker service scale nginx=3</div><div class="line"></div><div class="line">$ docker ps</div><div class="line">CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS               NAMES</div><div class="line">b51a902db8bc        nginx:latest        <span class="string">"nginx -g 'daemon off"</span>   2 minutes ago       Up 2 minutes        80/tcp, 443/tcp     nginx.1.8yvwxbquvz1ptuqsc8hewwbau</div></pre></td></tr></table></figure>
<figure class="highlight vbscript"><table><tr><td class="code"><pre><div class="line"># switch <span class="keyword">to</span> worker</div><div class="line">$ <span class="built_in">eval</span> $(docker-machine env swarm-worker<span class="number">-1</span>)</div><div class="line">$ docker ps</div><div class="line">CONTAINER ID        IMAGE               COMMAND                  CREATED              STATUS              PORTS               NAMES</div><div class="line">da6a8250bef4        nginx:latest        <span class="string">"nginx -g 'daemon off"</span>   About a <span class="built_in">minute</span> ago   Up About a <span class="built_in">minute</span>   <span class="number">80</span>/tcp, <span class="number">443</span>/tcp     nginx<span class="number">.2</span>.bqko7fyj1nowwj1flxva3ur0g</div><div class="line"><span class="number">54</span>d9ffd07894        nginx:latest        <span class="string">"nginx -g 'daemon off"</span>   About a <span class="built_in">minute</span> ago   Up About a <span class="built_in">minute</span>   <span class="number">80</span>/tcp, <span class="number">443</span>/tcp     nginx<span class="number">.3</span><span class="number">.02</span>k4d34gjooa9f8m6yhfi5hyu</div></pre></td></tr></table></figure>
<p>As seen above, one container runs on swarm-manager, and the others run on swarm-worker-1.</p>
<h2 id="Expose-services"><a href="#Expose-services" class="headerlink" title="Expose services"></a>Expose services</h2><h3 id="Visit-by-node-node-ip"><a href="#Visit-by-node-node-ip" class="headerlink" title="Visit by node node ip"></a>Visit by node node ip</h3><figure class="highlight haml"><table><tr><td class="code"><pre><div class="line">gcloud compute firewall-rules create nginx-swarm \</div><div class="line">  -<span class="ruby">-allow <span class="symbol">tcp:</span><span class="number">80</span> \</span></div><div class="line">  -<span class="ruby">-description <span class="string">"nginx swarm service"</span> \</span></div><div class="line">  -<span class="ruby">-target-tags swarm-cluster</span></div></pre></td></tr></table></figure>
<p>Then use external IP (get by exec <code>gcloud compute instances list</code>) to visit nginx service.</p>
<h3 id="GCP-Load-Balancer-tcp"><a href="#GCP-Load-Balancer-tcp" class="headerlink" title="GCP Load Balancer (tcp)"></a>GCP Load Balancer (tcp)</h3><figure class="highlight sql"><table><tr><td class="code"><pre><div class="line">gcloud compute addresses <span class="keyword">create</span> network-lb-ip<span class="number">-1</span> <span class="comment">--region us-central1</span></div><div class="line">gcloud <span class="keyword">compute</span> <span class="keyword">http</span>-health-checks <span class="keyword">create</span> basic-<span class="keyword">check</span></div><div class="line">gcloud <span class="keyword">compute</span> target-pools <span class="keyword">create</span> www-pool <span class="comment">--region us-central1 --health-check basic-check</span></div><div class="line">gcloud <span class="keyword">compute</span> target-pools <span class="keyword">add</span>-instances www-pool <span class="comment">--instances swarm-manager,swarm-worker-1 --zone us-central1-f</span></div><div class="line"></div><div class="line"># <span class="keyword">Get</span> lb addresses</div><div class="line">STATIC_EXTERNAL_IP=$(gcloud <span class="keyword">compute</span> addresses <span class="keyword">list</span> | awk <span class="string">'/network-lb-ip-1/&#123;print $3&#125;'</span>)</div><div class="line"># <span class="keyword">create</span> forwarding <span class="keyword">rules</span></div><div class="line">gcloud <span class="keyword">compute</span> forwarding-<span class="keyword">rules</span> <span class="keyword">create</span> www-rule <span class="comment">--region us-central1 --port-range 80 --address $&#123;STATIC_EXTERNAL_IP&#125; --target-pool www-pool</span></div></pre></td></tr></table></figure>
<p>Now you could visit <a href="http://${STATIC_EXTERNAL_IP}" target="_blank" rel="external">http://${STATIC_EXTERNAL_IP}</a> for nginx service.</p>
<p>BTW, <a href="https://blog.docker.com/2016/06/azure-aws-beta/" target="_blank" rel="external">Docker for aws and azure</a> will do this more easily as integrated:</p>
<ul>
<li>Use an SSH key already associated with your IaaS account for access control</li>
<li>Provision infrastructure load balancers and update them dynamically as apps are created and updated</li>
<li>Configure security groups and virtual networks to create secure Docker setups that are easy for operations to understand and manage</li>
</ul>
<blockquote>
<p>By default, apps deployed with bundles do not have ports publicly exposed. Update port mappings for services, and Docker will automatically wire up the underlying platform loadbalancers:<code>docker service update -p 80:80 &lt;example-service&gt;</code></p>
</blockquote>
<h3 id="Networking"><a href="#Networking" class="headerlink" title="Networking"></a>Networking</h3><h4 id="Local-networking"><a href="#Local-networking" class="headerlink" title="Local networking"></a>Local networking</h4><p><img width="1239" alt="2016-06-24 12 05 13" src="https://cloud.githubusercontent.com/assets/676637/16327964/9dfd842a-3a07-11e6-9031-c79c9c43ec83.png"></p>
<p>Create local scope network and place containers in existing vlans:</p>
<figure class="highlight lsl"><table><tr><td class="code"><pre><div class="line">docker network create -d macvlan --subnet=<span class="number">192.168</span><span class="number">.0</span><span class="number">.0</span>/<span class="number">16</span> --ip-range=<span class="number">192.168</span><span class="number">.41</span><span class="number">.0</span>/<span class="number">24</span> --aux-address=<span class="string">"favoriate_ip_ever=192.168.41.2"</span> --gateway=<span class="number">192.168</span><span class="number">.41</span><span class="number">.1</span> -o parent=eth0<span class="number">.41</span> macnet41</div><div class="line">docker run --net=macnet41 -it --rm alpine /bin/sh</div></pre></td></tr></table></figure>
<h4 id="Multi-host-networking"><a href="#Multi-host-networking" class="headerlink" title="Multi-host networking"></a>Multi-host networking</h4><p>A typical two-tier (web+db) application runs on swarm scope network would be created like this:</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><div class="line">docker network <span class="keyword">create</span> -d overlay mynet</div><div class="line">docker service <span class="keyword">create</span> –<span class="keyword">name</span> frontend –replicas <span class="number">5</span> -p <span class="number">80</span>:<span class="number">80</span>/tcp –network mynet mywebapp</div><div class="line">docker service <span class="keyword">create</span> –<span class="keyword">name</span> redis –network mynet redis:latest</div></pre></td></tr></table></figure>
<p><img width="1133" alt="2016-06-26 10 27 20" src="https://cloud.githubusercontent.com/assets/676637/16362920/7dbd339e-3bed-11e6-9987-8d425480ba59.png"></p>
<p><img width="1169" alt="2016-06-24 12 05 30" src="https://cloud.githubusercontent.com/assets/676637/16327980/c4af9432-3a07-11e6-93ed-9e94d12f0c9b.png"><br><img width="1228" alt="2016-06-24 12 05 58" src="https://cloud.githubusercontent.com/assets/676637/16327982/c4b2a906-3a07-11e6-8e97-70a26c5fc701.png"><br><img width="1235" alt="2016-06-24 12 06 11" src="https://cloud.githubusercontent.com/assets/676637/16327981/c4b14fc0-3a07-11e6-84f0-260a716044cb.png"></p>
<h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>Docker v1.12 indeeds introduced easy-of-use interface for orchestrating containers, but I’m concerned whether this way could scale for large clusters. Maybe we could see it on Docker’s further iterations.</p>
<h3 id="Further-more"><a href="#Further-more" class="headerlink" title="Further more"></a>Further more</h3><ul>
<li><a href="https://blog.docker.com/2016/06/docker-1-12-built-in-orchestration/" target="_blank" rel="external">https://blog.docker.com/2016/06/docker-1-12-built-in-orchestration/</a></li>
<li><a href="http://www.slideshare.net/MadhuVenugopal2/dockercon-us-2016-docker-networking-deep-dive" target="_blank" rel="external">http://www.slideshare.net/MadhuVenugopal2/dockercon-us-2016-docker-networking-deep-dive</a></li>
<li><a href="https://medium.com/google-cloud/docker-swarm-on-google-cloud-platform-c9925bd7863c#.3plkwmxss" target="_blank" rel="external">https://medium.com/google-cloud/docker-swarm-on-google-cloud-platform-c9925bd7863c#.3plkwmxss</a></li>
<li><a href="https://beta.docker.com/docs/" target="_blank" rel="external">https://beta.docker.com/docs/</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;[TOC]&lt;/p&gt;
&lt;p&gt;Docker v1.12 brings in its integrated orchestration into docker engine.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Starting with Docker 1.12, we ha
    
    </summary>
    
    
      <category term="docker" scheme="http://feisky.xyz/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>Playing docker with hypervisor container runtime runV</title>
    <link href="http://feisky.xyz/2016/06/17/Playing-docker-with-hypervisor-container-runtime-runV/"/>
    <id>http://feisky.xyz/2016/06/17/Playing-docker-with-hypervisor-container-runtime-runV/</id>
    <published>2016-06-17T09:12:38.000Z</published>
    <updated>2017-03-17T01:40:56.314Z</updated>
    
    <content type="html"><![CDATA[<p>Table of contents:</p>
<p>[TOC]</p>
<hr>
<p>The latest master branch of <a href="https://github.com/hyperhq/runv" target="_blank" rel="external">runV</a> has already supported running as an runtime in docker. Since v1.11, docker introduced OCI contain runtime (runc) integration via containerd. Since runc and runV are both <a href="https://github.com/opencontainers/runtime-spec/blob/master/implementations.md" target="_blank" rel="external">recommended implementation of OCI</a>, it is natural to make runV working with containerd. </p>
<p>Now let’s have a try.</p>
<h3 id="Install-runv-and-docker"><a href="#Install-runv-and-docker" class="headerlink" title="Install runv and docker"></a>Install runv and docker</h3><p>Docker could be installed via <a href="https://docs.docker.com/engine/installation/" target="_blank" rel="external">https://docs.docker.com/engine/installation/</a>.</p>
<p>Since only master branch of runV supports running integrated with docker, we should compile runV by source.</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><div class="line">sudo apt-get install -y autoconf automake pkg-config libdevmapper-dev libsqlite3-dev libvirt-dev qemu libvirt-bin</div><div class="line">mkdir -p <span class="variable">$GOPATH</span>/src/github.com/hyperhq</div><div class="line"><span class="built_in">cd</span> <span class="variable">$GOPATH</span>/src/github.com/hyperhq</div><div class="line">git <span class="built_in">clone</span> https://github.com/hyperhq/runv</div><div class="line"><span class="built_in">cd</span> runv</div><div class="line">./autogen.sh</div><div class="line">./configure</div><div class="line">make</div><div class="line">make install</div></pre></td></tr></table></figure>
<h3 id="Start-docker-with-runV-runtime"><a href="#Start-docker-with-runV-runtime" class="headerlink" title="Start docker with runV runtime"></a>Start docker with runV runtime</h3><p>Stop docker first since it is running with runc by default.</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><div class="line">systemctl stop docker</div></pre></td></tr></table></figure>
<p>Now start docker with runV:</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><div class="line"><span class="comment"># start containerd</span></div><div class="line">systemd-run --unit=containerd-runv docker-containerd --debug <span class="_">-l</span> /var/run/docker/libcontainerd/docker-containerd.sock --runtime /usr/<span class="built_in">local</span>/bin/runv --runtime-args --debug --runtime-args --driver=libvirt --runtime-args --kernel=/var/lib/hyper/kernel --runtime-args --initrd=/var/lib/hyper/hyper-initrd.img --start-timeout 2m</div><div class="line"></div><div class="line"><span class="comment"># start docker</span></div><div class="line">systemd-run --unit=docker-runv docker daemon -D <span class="_">-l</span> debug --containerd=/var/run/docker/libcontainerd/docker-containerd.sock</div><div class="line"></div><div class="line"><span class="comment"># check status</span></div><div class="line">[root@linux ~]<span class="comment"># systemctl status containerd-runv</span></div><div class="line">● containerd-runv.service - /usr/bin/docker-containerd --debug <span class="_">-l</span> /var/run/docker/libcontainerd/docker-containerd.sock --runtime /usr/<span class="built_in">local</span>/bin/runv --runtime-args --debug --runtime-args --driver=libvirt --runtime-args --kernel=/var/lib/hyper/kernel --runtime-args --initrd=/var/lib/hyper/hyper-initrd.img --start-timeout 2m</div><div class="line">   Loaded: loaded (/run/systemd/system/containerd-runv.service; static; vendor preset: disabled)</div><div class="line">  Drop-In: /run/systemd/system/containerd-runv.service.d</div><div class="line">           └─50-Description.conf, 50-ExecStart.conf</div><div class="line">   Active: active (running) since 五 2016-06-17 09:47:57 UTC; 10s ago</div><div class="line"> Main PID: 12650 (docker-containe)</div><div class="line">   Memory: 1.8M</div><div class="line">   CGroup: /system.slice/containerd-runv.service</div><div class="line">           └─12650 /usr/bin/docker-containerd --debug <span class="_">-l</span> /var/run/docker/libcontainerd/docker-containerd.sock --run...</div><div class="line"></div><div class="line">6月 17 09:47:57 linux systemd[1]: Started /usr/bin/docker-containerd --debug <span class="_">-l</span> /var/run/docker/libcontainerd/docker...</div><div class="line">6月 17 09:47:57 linux systemd[1]: Starting /usr/bin/docker-containerd --debug <span class="_">-l</span> /var/run/docker/libcontainerd/docke...</div><div class="line">6月 17 09:47:57 linux docker-containerd[12650]: time=<span class="string">"2016-06-17T09:47:57Z"</span> level=warning msg=<span class="string">"containerd: low ...=4096</span></div><div class="line">6月 17 09:47:57 linux docker-containerd[12650]: time="2016-06-17T09:47:57Z<span class="string">" level=debug msg="</span>containerd: <span class="built_in">read</span> p...unt=0</div><div class="line">6月 17 09:47:57 linux docker-containerd[12650]: time=<span class="string">"2016-06-17T09:47:57Z"</span> level=debug msg=<span class="string">"containerd: superv...nerd"</span></div><div class="line">6月 17 09:47:57 linux docker-containerd[12650]: time=<span class="string">"2016-06-17T09:47:57Z"</span> level=debug msg=<span class="string">"containerd: grpc a...sock"</span></div><div class="line">Hint: Some lines were ellipsized, use <span class="_">-l</span> to show <span class="keyword">in</span> full.</div><div class="line"></div><div class="line">[root@linux ~]<span class="comment"># systemctl status docker-runv</span></div><div class="line">● docker-runv.service - /usr/bin/docker daemon -D <span class="_">-l</span> debug --containerd=/var/run/docker/libcontainerd/docker-containerd.sock</div><div class="line">   Loaded: loaded (/run/systemd/system/docker-runv.service; static; vendor preset: disabled)</div><div class="line">  Drop-In: /run/systemd/system/docker-runv.service.d</div><div class="line">           └─50-Description.conf, 50-ExecStart.conf</div><div class="line">   Active: active (running) since 五 2016-06-17 09:34:11 UTC; 25s ago</div><div class="line"> Main PID: 11120 (docker)</div><div class="line">   Memory: 20.8M</div><div class="line">   CGroup: /system.slice/docker-runv.service</div><div class="line">           └─11120 /usr/bin/docker daemon -D <span class="_">-l</span> debug --containerd=/var/run/docker/libcontainerd/docker-containerd.sock</div><div class="line"></div><div class="line">6月 17 09:34:13 linux docker[11120]: time=<span class="string">"2016-06-17T09:34:13.019309548Z"</span> level=debug msg=<span class="string">"Registering POST, /volumes/create"</span></div><div class="line">6月 17 09:34:13 linux docker[11120]: time=<span class="string">"2016-06-17T09:34:13.019448115Z"</span> level=debug msg=<span class="string">"Registering DELETE, /volumes/&#123;name:.*&#125;"</span></div><div class="line">6月 17 09:34:13 linux docker[11120]: time=<span class="string">"2016-06-17T09:34:13.019551244Z"</span> level=debug msg=<span class="string">"Registering POST, /build"</span></div><div class="line">6月 17 09:34:13 linux docker[11120]: time=<span class="string">"2016-06-17T09:34:13.019607895Z"</span> level=debug msg=<span class="string">"Registering GET, /networks"</span></div><div class="line">6月 17 09:34:13 linux docker[11120]: time=<span class="string">"2016-06-17T09:34:13.019675700Z"</span> level=debug msg=<span class="string">"Registering GET, /networks/&#123;id:.*&#125;"</span></div><div class="line">6月 17 09:34:13 linux docker[11120]: time=<span class="string">"2016-06-17T09:34:13.019771551Z"</span> level=debug msg=<span class="string">"Registering POST, /networks/create"</span></div><div class="line">6月 17 09:34:13 linux docker[11120]: time=<span class="string">"2016-06-17T09:34:13.020256142Z"</span> level=debug msg=<span class="string">"Registering POST, /networks/&#123;id:.*&#125;/connect"</span></div><div class="line">6月 17 09:34:13 linux docker[11120]: time=<span class="string">"2016-06-17T09:34:13.020369131Z"</span> level=debug msg=<span class="string">"Registering POST, /networks/&#123;id:.*&#125;/disconnect"</span></div><div class="line">6月 17 09:34:13 linux docker[11120]: time=<span class="string">"2016-06-17T09:34:13.020463042Z"</span> level=debug msg=<span class="string">"Registering DELETE, /networks/&#123;id:.*&#125;"</span></div><div class="line">6月 17 09:34:13 linux docker[11120]: time=<span class="string">"2016-06-17T09:34:13.021491071Z"</span> level=info msg=<span class="string">"API listen on /var/run/docker.sock"</span></div></pre></td></tr></table></figure>
<h3 id="Create-container"><a href="#Create-container" class="headerlink" title="Create container"></a>Create container</h3><p>Let’s create a nginx container.</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><div class="line">[root@linux ~]<span class="comment"># docker run -i -d  nginx</span></div><div class="line">6a34a0513ebbdb2c57d828bf4e814773c8a5cf6af8c35e4376f2028769a7c35c</div><div class="line">[root@linux ~]<span class="comment"># docker ps</span></div><div class="line">CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS               NAMES</div><div class="line">6a34a0513ebb        nginx               <span class="string">"nginx -g 'daemon off"</span>   9 seconds ago       Up 3 seconds        80/tcp, 443/tcp     berserk_mcnulty</div><div class="line"></div><div class="line"><span class="comment"># Is it working</span></div><div class="line">[root@linux ~]<span class="comment"># docker inspect --format '&#123;&#123; .NetworkSettings.IPAddress &#125;&#125;' 6a34a0513ebb</span></div><div class="line">172.17.0.2</div><div class="line">[root@linux ~]<span class="comment"># curl -I 172.17.0.2</span></div><div class="line">HTTP/1.1 200 OK</div><div class="line">Server: nginx/1.11.1</div><div class="line">Date: Fri, 17 Jun 2016 09:52:37 GMT</div><div class="line">Content-Type: text/html</div><div class="line">Content-Length: 612</div><div class="line">Last-Modified: Tue, 31 May 2016 14:40:22 GMT</div><div class="line">Connection: keep-alive</div><div class="line">ETag: <span class="string">"574da256-264"</span></div><div class="line">Accept-Ranges: bytes</div></pre></td></tr></table></figure>
<p>Is the container really running in runV with hypervisor?</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><div class="line">root@linux ~]<span class="comment"># runv list</span></div><div class="line">ID                                                                 PID         STATUS      BUNDLE                                                                                           CREATED</div><div class="line">6a34a0513ebbdb2c57d828bf4e814773c8a5cf6af8c35e4376f2028769a7c35c   12756       running     /var/run/docker/libcontainerd/6a34a0513ebbdb2c57d828bf4e814773c8a5cf6af8c35e4376f2028769a7c35c   2016-06-17T09:48:38.324839156Z</div><div class="line"></div><div class="line">[root@linux ~]<span class="comment"># runv state 6a34a0513ebbdb2c57d828bf4e814773c8a5cf6af8c35e4376f2028769a7c35c</span></div><div class="line">&#123;</div><div class="line">  <span class="string">"ociVersion"</span>: <span class="string">"0.6.0-dev"</span>,</div><div class="line">  <span class="string">"id"</span>: <span class="string">"6a34a0513ebbdb2c57d828bf4e814773c8a5cf6af8c35e4376f2028769a7c35c"</span>,</div><div class="line">  <span class="string">"pid"</span>: 12756,</div><div class="line">  <span class="string">"bundlePath"</span>: <span class="string">"/var/run/docker/libcontainerd/6a34a0513ebbdb2c57d828bf4e814773c8a5cf6af8c35e4376f2028769a7c35c"</span>,</div><div class="line">  <span class="string">"rootfsPath"</span>: <span class="string">"/var/run/docker/libcontainerd/6a34a0513ebbdb2c57d828bf4e814773c8a5cf6af8c35e4376f2028769a7c35c/rootfs"</span>,</div><div class="line">  <span class="string">"status"</span>: <span class="string">"running"</span>,</div><div class="line">  <span class="string">"created"</span>: <span class="string">"2016-06-17T09:48:38.324839156Z"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">[root@linux ~]<span class="comment"># virsh list</span></div><div class="line"> Id    名称                         状态</div><div class="line">----------------------------------------------------</div><div class="line"> 919   vm-CeaKLvbPEg                  running</div><div class="line"></div><div class="line">[root@linux ~]<span class="comment"># ps -ef | grep vm-CeaKLvbPEg | grep -v grep</span></div><div class="line">root     12743     1  1 09:48 ?        00:00:06 /usr/bin/qemu-system-x86_64 -name vm-CeaKLvbPEg -S -machine pc-i440fx-2.0,accel=tcg,usb=off -cpu Haswell-noTSX,+abm,+hypervisor,+rdrand,+f16c,+osxsave,+ht,+vme -m 128 -realtime mlock=off -smp 1,sockets=1,cores=1,threads=1 -uuid 4f158103-bd5e-4fd1<span class="_">-a</span>62f-9e18093ceaf4 -nographic -no-user-config -nodefaults -chardev socket,id=charmonitor,path=/var/lib/libvirt/qemu/domain-vm-CeaKLvbPEg/monitor.sock,server,nowait -mon chardev=charmonitor,id=monitor,mode=control -rtc base=utc -no-reboot -boot strict=on -kernel /var/lib/hyper/kernel -initrd /var/lib/hyper/hyper-initrd.img -append console=ttyS0 panic=1 no_timer_check -device virtio-scsi-pci,id=scsi0,bus=pci.0,addr=0x3 -device virtio-serial-pci,id=virtio-serial0,bus=pci.0,addr=0x2 -fsdev <span class="built_in">local</span>,security_model=none,id=fsdev-fs0,path=/var/run/hyper/vm-CeaKLvbPEg/share_dir -device virtio-9p-pci,id=fs0,fsdev=fsdev-fs0,mount_tag=share_dir,bus=pci.0,addr=0x4 -chardev socket,id=charserial0,path=/var/run/hyper/vm-CeaKLvbPEg/console.sock,server,nowait -device isa-serial,chardev=charserial0,id=serial0 -chardev socket,id=charchannel0,path=/var/run/hyper/vm-CeaKLvbPEg/hyper.sock,server,nowait -device virtserialport,bus=virtio-serial0.0,nr=1,chardev=charchannel0,id=channel0,name=sh.hyper.channel.0 -chardev socket,id=charchannel1,path=/var/run/hyper/vm-CeaKLvbPEg/tty.sock,server,nowait -device virtserialport,bus=virtio-serial0.0,nr=2,chardev=charchannel1,id=channel1,name=sh.hyper.channel.1 -device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x5 -msg timestamp=on</div></pre></td></tr></table></figure>
<h3 id="Is-it-stable"><a href="#Is-it-stable" class="headerlink" title="Is it stable?"></a>Is it stable?</h3><p>Of course not, runV is still under quick development and features are not complete now. For example, there are a lot of commands not supported now:</p>
<ul>
<li><code>docker stop</code></li>
<li><code>docker stats</code></li>
<li><code>docker exec</code></li>
</ul>
<p>Although there are still problems, I’m exited by what runV has done. Looking forward to its release.</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Table of contents:&lt;/p&gt;
&lt;p&gt;[TOC]&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;The latest master branch of &lt;a href=&quot;https://github.com/hyperhq/runv&quot; target=&quot;_blank&quot; rel=&quot;ex
    
    </summary>
    
    
      <category term="docker" scheme="http://feisky.xyz/tags/docker/"/>
    
      <category term="runV" scheme="http://feisky.xyz/tags/runV/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes-mesos architecture</title>
    <link href="http://feisky.xyz/2016/06/07/Kubernetes-mesos-architecture/"/>
    <id>http://feisky.xyz/2016/06/07/Kubernetes-mesos-architecture/</id>
    <published>2016-06-07T05:21:07.000Z</published>
    <updated>2017-03-17T01:40:56.314Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/kubernetes_mesos_architecture.png" alt=""></p>
<p>From <a href="http://cdn.yongbok.net/ruo91/architecture/k8s/kubernetes_mesos_architecture_v1.x.png" target="_blank" rel="external">http://cdn.yongbok.net/ruo91/architecture/k8s/kubernetes_mesos_architecture_v1.x.png</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/images/kubernetes_mesos_architecture.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;From &lt;a href=&quot;http://cdn.yongbok.net/ruo91/architecture/k8s/kubernete
    
    </summary>
    
    
      <category term="mesos" scheme="http://feisky.xyz/tags/mesos/"/>
    
      <category term="kubernetes" scheme="http://feisky.xyz/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Hypernetes: Bringing Security and Multi-tenancy to Kubernetes</title>
    <link href="http://feisky.xyz/2016/06/06/Hypernetes-Bringing-Security-and-Multi-tenancy-to-Kubernetes/"/>
    <id>http://feisky.xyz/2016/06/06/Hypernetes-Bringing-Security-and-Multi-tenancy-to-Kubernetes/</id>
    <published>2016-06-06T08:10:25.000Z</published>
    <updated>2017-03-17T01:40:56.314Z</updated>
    
    <content type="html"><![CDATA[<blockquote>
<p>Notes: this post is copied from <a href="http://blog.kubernetes.io/2016/05/hypernetes-security-and-multi-tenancy-in-kubernetes.html" target="_blank" rel="external">http://blog.kubernetes.io/2016/05/hypernetes-security-and-multi-tenancy-in-kubernetes.html</a>.</p>
</blockquote>
<p><em>Today’s guest post is written by Harry Zhang and Pengfei Ni, engineers at HyperHQ, describing a new hypervisor based container called HyperContainer</em>  </p>
<p>While many developers and security professionals are comfortable with Linux containers as an effective boundary, many users need a stronger degree of isolation, particularly for those running in a multi-tenant environment. Sadly, today, those users are forced to run their containers inside virtual machines, even one VM per container.  </p>
<p>Unfortunately, this results in the loss of many of the benefits of a cloud-native deployment: slow startup time of VMs; a memory tax for every container; low utilization resulting in wasting resources.  </p>
<p>In this post, we will introduce HyperContainer, a hypervisor based container and see how it naturally fits into the Kubernetes design, and enables users to serve their customers directly with virtualized containers, instead of wrapping them inside of full blown VMs.  </p>
<p><strong>HyperContainer</strong>  </p>
<p><a href="http://hypercontainer.io/" target="_blank" rel="external">HyperContainer</a> is a hypervisor-based container, which allows you to launch Docker images with standard hypervisors (KVM, Xen, etc.). As an open-source project, HyperContainer consists of an <a href="https://github.com/opencontainers/runtime-spec" target="_blank" rel="external">OCI</a> compatible runtime implementation, named <a href="https://github.com/hyperhq/runv/" target="_blank" rel="external">runV</a>, and a management daemon named <a href="https://github.com/hyperhq/hyperd" target="_blank" rel="external">hyperd</a>. The idea behind HyperContainer is quite straightforward: to combine the best of both virtualization and container.  </p>
<p>We can consider containers as two parts (as Kubernetes does). The first part is the container runtime, where HyperContainer uses virtualization to achieve execution isolation and resource limitation instead of namespaces and cgroups. The second part is the application data, where HyperContainer leverages Docker images. So in HyperContainer, virtualization technology makes it possible to build a fully isolated sandbox with an independent guest kernel (so things like <code>top</code> and /proc all work), but from developer’s view, it’s portable and behaves like a standard container.  </p>
<p><strong>HyperContainer as Pod</strong>  </p>
<p>The interesting part of HyperContainer is not only that it is secure enough for multi-tenant environments (such as a public cloud), but also how well it fits into the Kubernetes philosophy.  </p>
<p>One of the most important concepts in Kubernetes is Pods. The design of Pods is a lesson learned (<a href="http://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/43438.pdf" target="_blank" rel="external">Borg paper section 8.1</a>) from real world workloads, where in many cases people want an atomic scheduling unit composed of multiple containers (please check this <a href="https://github.com/kubernetes/kubernetes/tree/master/examples/javaweb-tomcat-sidecar" target="_blank" rel="external">example</a> for further information). In the context of Linux containers, a Pod wraps and encapsulates several containers into a logical group. But in HyperContainer, the hypervisor serves as a natural boundary, and Pods are introduced as first-class objects:  </p>
<p><img src="https://lh6.googleusercontent.com/8DjNb9IE0HjinFxkaoGbPaaKbts5_Osbj-8NVWQMgY_8D32643Aum0SaMc2OedV2gECG3EXov8qj_f8XDe0IfpptZt61HxfJEonLo3RA5xkr5zSmd2nxqVc8yESc423nPEZTj1H3" alt=""></p>
<p>HyperContainer wraps a Pod of light-weight application containers and exposes the container interface at Pod level. Inside the Pod, a minimalist Linux kernel called HyperKernel is booted. This HyperKernel is built with a tiny Init service called HyperStart. It will act as the PID 1 process and creates the Pod, setup Mount namespace, and launch apps from the loaded images. </p>
<p>This model works nicely with Kubernetes. The integration of HyperContainer with Kubernetes, as we indicated in the title, is what makes up the <a href="https://github.com/hyperhq/hypernetes" target="_blank" rel="external">Hypernetes</a> project. </p>
<p><strong>Hypernetes</strong></p>
<p>One of the best parts of Kubernetes is that it is designed to support multiple container runtimes, meaning users are not locked-in to a single vendor. We are very pleased to announce that we have already begun working with the Kubernetes team to integrate HyperContainer into Kubernetes upstream. This integration involves:</p>
<ol>
<li>container runtime optimizing and refactoring</li>
<li>new client-server mode runtime interface</li>
<li>containerd integration to support runV</li>
</ol>
<p>The OCI standard and kubelet’s multiple runtime architecture make this integration much easier even though HyperContainer is not based on Linux container technology stack.</p>
<p>On the other hand, in order to run HyperContainers in multi-tenant environment, we also created a new network plugin and modified an existing volume plugin. Since Hypernetes runs Pod as their own VMs, it can make use of your existing IaaS layer technologies for multi-tenant network and persistent volumes. The current Hypernetes implementation uses standard Openstack components.</p>
<p>Below we go into further details about how all those above are implemented.</p>
<p><strong>Identity and Authentication</strong></p>
<hr>
<p>In Hypernetes we chose <a href="http://docs.openstack.org/developer/keystone/" target="_blank" rel="external">Keystone</a> to manage different tenants and perform identification and authentication for tenants during any administrative operation. Since Keystone comes from the OpenStack ecosystem, it works seamlessly with the network and storage plugins we used in Hypernetes.</p>
<p><strong>Multi-tenant Network Model</strong></p>
<p>For a multi-tenant container cluster, each tenant needs to have strong network isolation from each other tenant. In Hypernetes, each tenant has its own Network. Instead of configuring a new network using OpenStack, which is complex, with Hypernetes, you just create a Network object like below.  </p>
<p>apiVersion: v1  kind: Network  metadata:    name: net1  spec:    tenantID: 065f210a2ca9442aad898ab129426350    subnets:      subnet1:        cidr: 192.168.0.0/24        gateway: 192.168.0.1</p>
<p>Note that the tenantID is supplied by Keystone. This yaml will automatically create a new Neutron network with a default router and a subnet 192.168.0.0/24. </p>
<p>A Network controller will be responsible for the life-cycle management of any Network instance created by the user. This Network can be assigned to one or more Namespaces, and any Pods belonging to the same Network can reach each other directly through IP address.  </p>
<p>apiVersion: v1  kind: Namespace  metadata:    name: ns1  spec:    network: net1  </p>
<p>If a Namespace does not have a Network spec, it will use the default Kubernetes network model instead, including the default kube-proxy. So if a user creates a Pod in a Namespace with an associated Network, Hypernetes will follow the <a href="http://kubernetes.io/docs/admin/network-plugins/" target="_blank" rel="external">Kubernetes Network Plugin Model</a> to set up a Neutron network for this Pod. Here is a high level example:</p>
<p><img src="https://lh4.googleusercontent.com/ij88fHWT3wDSxDh4W7S0sARfjdRd5oTJTZGT_r8oQoqoGGjZWmHLJtPG8TT3U_tZ2rFqK7lwK56l3UIq3csSUxSdgGvfzORaAEAkl9fChxiLzVgz-mExTMi8sxUlfsesS59G0Fsa" alt="A Hypernetes Network Workflow.png"></p>
<p>Hypernetes uses a standalone gRPC handler named kubestack to translate the Kubernetes Pod request into the Neutron network API. Moreover, kubestack is also responsible for handling another important networking feature: a multi-tenant Service proxy.</p>
<p>In a multi-tenant environment, the default iptables-based kube-proxy can not reach the individual Pods, because they are isolated into different networks. Instead, Hypernetes uses a <a href="https://github.com/hyperhq/hyperd/blob/2072dd8e28a02a25ae6a819f81029b47a579e683/servicediscovery/servicediscovery.go" target="_blank" rel="external">built-in HAproxy in every HyperContainer</a> as the portal. This HAproxy will proxy all the Service instances in the namespace of that Pod. Kube-proxy will be responsible for updating these backend servers by following the standard OnServiceUpdate and OnEndpointsUpdate processes, so that users will not notice any difference. A downside of this method is that HAproxy has to listen to some specific ports which may conflicts with user’s containers.That’s why we are planning to use LVS to replace this proxy in the next release.</p>
<p>With the help of the Neutron based network plugin, the Hypernetes Service is able to provide an OpenStack load balancer, just like how the “external” load balancer does on GCE. When user creates a Service with external IPs, an OpenStack load balancer will be created and endpoints will be automatically updated through the kubestack workflow above.</p>
<p><strong>Persistent Storage</strong></p>
<p>When considering storage, we are actually building a tenant-aware persistent volume in Kubernetes. The reason we decided not to use existing Cinder volume plugin of Kubernetes is that its model does not work in the virtualization case. Specifically:</p>
<p>The Cinder volume plugin requires OpenStack as the Kubernetes provider.</p>
<p>The OpenStack provider will find on which VM the target Pod is running on</p>
<p>Cinder volume plugin will mount a Cinder volume to a path inside the host VM of Kubernetes.</p>
<p>The kubelet will bind mount this path as a volume into containers of target Pod.</p>
<p>But in Hypernetes, things become much simpler. Thanks to the physical boundary of Pods, HyperContainer can mount Cinder volumes directly as block devices into Pods, just like a normal VM. This mechanism eliminates extra time to query Nova to find out the VM of target Pod in the existing Cinder volume workflow listed above.</p>
<p>The current implementation of the Cinder plugin in Hypernetes is based on Ceph RBD backend, and it works the same as all other Kubernetes volume plugins, one just needs to remember to create the Cinder volume (referenced by volumeID below) beforehand.  </p>
<p>apiVersion: v1  kind: Pod  metadata:    name: nginx    labels:      app: nginx  spec:    containers:    - name: nginx      image: nginx      ports:      - containerPort: 80      volumeMounts:      - name: nginx-persistent-storage        mountPath: /var/lib/nginx    volumes:    - name: nginx-persistent-storage      cinder:        volumeID: 651b2a7b-683e-47e1-bdd6-e3c62e8f91c0        fsType: ext4  </p>
<p>So when the user provides a Pod yaml with a Cinder volume, Hypernetes will check if kubelet is using the Hyper container runtime. If so, the Cinder volume can be mounted directly to the Pod without any extra path mapping. Then the volume metadata will be passed to the Kubelet RunPod process as part of HyperContainer spec. Done!</p>
<p>Thanks to the plugin model of Kubernetes network and volume, we can easily build our own solutions above for HyperContainer though it is essentially different from the traditional Linux container. We also plan to propose these solutions to Kubernetes upstream by following the CNI model and volume plugin standard after the runtime integration is completed.</p>
<p>We believe all of these <a href="https://github.com/hyperhq/" target="_blank" rel="external">open source projects</a> are important components of the container ecosystem, and their growth depends greatly on the open source spirit and technical vision of the Kubernetes team. </p>
<p><strong>Conclusion</strong></p>
<p>This post introduces some of the technical details about HyperContainer and the Hypernetes project. We hope that people will be interested in this new category of secure container and its integration with Kubernetes. If you are looking to try out Hypernetes and HyperContainer, we have just announced the public beta of our new secure container cloud service (<a href="https://hyper.sh/" target="_blank" rel="external">Hyper_</a>), which is built on these technologies. But even if you are running on-premise, we believe that Hypernetes and HyperContainer will let you run Kubernetes in a more secure way.</p>
<p><em>~Harry Zhang and Pengfei Ni, engineers at HyperHQ</em></p>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;Notes: this post is copied from &lt;a href=&quot;http://blog.kubernetes.io/2016/05/hypernetes-security-and-multi-tenancy-in-kubernet
    
    </summary>
    
    
      <category term="Kubernetes" scheme="http://feisky.xyz/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>How docker 1.11 share network accross containers</title>
    <link href="http://feisky.xyz/2016/05/11/How-docker-1-11-share-network-accross-containers/"/>
    <id>http://feisky.xyz/2016/05/11/How-docker-1-11-share-network-accross-containers/</id>
    <published>2016-05-11T02:25:06.000Z</published>
    <updated>2017-03-17T01:40:56.314Z</updated>
    
    <content type="html"><![CDATA[<p>Docker 1.11 has moved to runc with containerd, I am interested in how it processing shared netns accross containers.</p>
<p>For example, I have already running a container 75599a6f387b7842c6da57efd38f9742b2ca621782f891402f83852c66dbd706. A new container within same netns can be created with cmd:</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><div class="line">docker run -itd --net=container:75599a6f387b alpine sh</div></pre></td></tr></table></figure>
<p>This will generate a runc <code>config.json</code> as follows:</p>
<figure class="highlight json"><table><tr><td class="code"><pre><div class="line">&#123;</div><div class="line">    <span class="attr">"ociVersion"</span>: <span class="string">"0.6.0-dev"</span>,</div><div class="line">    <span class="attr">"platform"</span>: &#123;</div><div class="line">        <span class="attr">"os"</span>: <span class="string">"linux"</span>,</div><div class="line">        <span class="attr">"arch"</span>: <span class="string">"amd64"</span></div><div class="line">    &#125;,</div><div class="line">    <span class="attr">"process"</span>: &#123;</div><div class="line">        <span class="attr">"terminal"</span>: <span class="literal">true</span>,</div><div class="line">        <span class="attr">"user"</span>: &#123;</div><div class="line">            <span class="attr">"additionalGids"</span>: [</div><div class="line">                <span class="number">0</span>,</div><div class="line">                <span class="number">1</span>,</div><div class="line">                <span class="number">2</span>,</div><div class="line">                <span class="number">3</span>,</div><div class="line">                <span class="number">4</span>,</div><div class="line">                <span class="number">6</span>,</div><div class="line">                <span class="number">10</span>,</div><div class="line">                <span class="number">11</span>,</div><div class="line">                <span class="number">20</span>,</div><div class="line">                <span class="number">26</span>,</div><div class="line">                <span class="number">27</span></div><div class="line">            ]</div><div class="line">        &#125;,</div><div class="line">        <span class="attr">"args"</span>: [</div><div class="line">            <span class="string">"sh"</span></div><div class="line">        ],</div><div class="line">        <span class="attr">"env"</span>: [</div><div class="line">            <span class="string">"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"</span>,</div><div class="line">            <span class="string">"HOSTNAME=75599a6f387b"</span>,</div><div class="line">            <span class="string">"TERM=xterm"</span></div><div class="line">        ],</div><div class="line">        <span class="attr">"cwd"</span>: <span class="string">"/"</span>,</div><div class="line">        <span class="attr">"capabilities"</span>: [</div><div class="line">            <span class="string">"CAP_CHOWN"</span>,</div><div class="line">            <span class="string">"CAP_DAC_OVERRIDE"</span>,</div><div class="line">            <span class="string">"CAP_FSETID"</span>,</div><div class="line">            <span class="string">"CAP_FOWNER"</span>,</div><div class="line">            <span class="string">"CAP_MKNOD"</span>,</div><div class="line">            <span class="string">"CAP_NET_RAW"</span>,</div><div class="line">            <span class="string">"CAP_SETGID"</span>,</div><div class="line">            <span class="string">"CAP_SETUID"</span>,</div><div class="line">            <span class="string">"CAP_SETFCAP"</span>,</div><div class="line">            <span class="string">"CAP_SETPCAP"</span>,</div><div class="line">            <span class="string">"CAP_NET_BIND_SERVICE"</span>,</div><div class="line">            <span class="string">"CAP_SYS_CHROOT"</span>,</div><div class="line">            <span class="string">"CAP_KILL"</span>,</div><div class="line">            <span class="string">"CAP_AUDIT_WRITE"</span></div><div class="line">        ]</div><div class="line">    &#125;,</div><div class="line">    <span class="attr">"root"</span>: &#123;</div><div class="line">        <span class="attr">"path"</span>: <span class="string">"/var/lib/docker/devicemapper/mnt/d33c7932917e64bde482b437fc3ccaad9a00a04e0cf49e39f9d3be5d71991db6/rootfs"</span>,</div><div class="line">        <span class="attr">"readonly"</span>: <span class="literal">false</span></div><div class="line">    &#125;,</div><div class="line">    <span class="attr">"hostname"</span>: <span class="string">"75599a6f387b"</span>,</div><div class="line">    <span class="attr">"mounts"</span>: [</div><div class="line">        &#123;</div><div class="line">            <span class="attr">"destination"</span>: <span class="string">"/proc"</span>,</div><div class="line">            <span class="attr">"type"</span>: <span class="string">"proc"</span>,</div><div class="line">            <span class="attr">"source"</span>: <span class="string">"proc"</span>,</div><div class="line">            <span class="attr">"options"</span>: [</div><div class="line">                <span class="string">"nosuid"</span>,</div><div class="line">                <span class="string">"noexec"</span>,</div><div class="line">                <span class="string">"nodev"</span></div><div class="line">            ]</div><div class="line">        &#125;,</div><div class="line">        &#123;</div><div class="line">            <span class="attr">"destination"</span>: <span class="string">"/dev"</span>,</div><div class="line">            <span class="attr">"type"</span>: <span class="string">"tmpfs"</span>,</div><div class="line">            <span class="attr">"source"</span>: <span class="string">"tmpfs"</span>,</div><div class="line">            <span class="attr">"options"</span>: [</div><div class="line">                <span class="string">"nosuid"</span>,</div><div class="line">                <span class="string">"strictatime"</span>,</div><div class="line">                <span class="string">"mode=755"</span></div><div class="line">            ]</div><div class="line">        &#125;,</div><div class="line">        &#123;</div><div class="line">            <span class="attr">"destination"</span>: <span class="string">"/dev/pts"</span>,</div><div class="line">            <span class="attr">"type"</span>: <span class="string">"devpts"</span>,</div><div class="line">            <span class="attr">"source"</span>: <span class="string">"devpts"</span>,</div><div class="line">            <span class="attr">"options"</span>: [</div><div class="line">                <span class="string">"nosuid"</span>,</div><div class="line">                <span class="string">"noexec"</span>,</div><div class="line">                <span class="string">"newinstance"</span>,</div><div class="line">                <span class="string">"ptmxmode=0666"</span>,</div><div class="line">                <span class="string">"mode=0620"</span>,</div><div class="line">                <span class="string">"gid=5"</span></div><div class="line">            ]</div><div class="line">        &#125;,</div><div class="line">        &#123;</div><div class="line">            <span class="attr">"destination"</span>: <span class="string">"/sys"</span>,</div><div class="line">            <span class="attr">"type"</span>: <span class="string">"sysfs"</span>,</div><div class="line">            <span class="attr">"source"</span>: <span class="string">"sysfs"</span>,</div><div class="line">            <span class="attr">"options"</span>: [</div><div class="line">                <span class="string">"nosuid"</span>,</div><div class="line">                <span class="string">"noexec"</span>,</div><div class="line">                <span class="string">"nodev"</span>,</div><div class="line">                <span class="string">"ro"</span></div><div class="line">            ]</div><div class="line">        &#125;,</div><div class="line">        &#123;</div><div class="line">            <span class="attr">"destination"</span>: <span class="string">"/sys/fs/cgroup"</span>,</div><div class="line">            <span class="attr">"type"</span>: <span class="string">"cgroup"</span>,</div><div class="line">            <span class="attr">"source"</span>: <span class="string">"cgroup"</span>,</div><div class="line">            <span class="attr">"options"</span>: [</div><div class="line">                <span class="string">"ro"</span>,</div><div class="line">                <span class="string">"nosuid"</span>,</div><div class="line">                <span class="string">"noexec"</span>,</div><div class="line">                <span class="string">"nodev"</span></div><div class="line">            ]</div><div class="line">        &#125;,</div><div class="line">        &#123;</div><div class="line">            <span class="attr">"destination"</span>: <span class="string">"/dev/mqueue"</span>,</div><div class="line">            <span class="attr">"type"</span>: <span class="string">"mqueue"</span>,</div><div class="line">            <span class="attr">"source"</span>: <span class="string">"mqueue"</span>,</div><div class="line">            <span class="attr">"options"</span>: [</div><div class="line">                <span class="string">"nosuid"</span>,</div><div class="line">                <span class="string">"noexec"</span>,</div><div class="line">                <span class="string">"nodev"</span></div><div class="line">            ]</div><div class="line">        &#125;,</div><div class="line">        &#123;</div><div class="line">            <span class="attr">"destination"</span>: <span class="string">"/etc/resolv.conf"</span>,</div><div class="line">            <span class="attr">"type"</span>: <span class="string">"bind"</span>,</div><div class="line">            <span class="attr">"source"</span>: <span class="string">"/var/lib/docker/containers/75599a6f387b7842c6da57efd38f9742b2ca621782f891402f83852c66dbd706/resolv.conf"</span>,</div><div class="line">            <span class="attr">"options"</span>: [</div><div class="line">                <span class="string">"rbind"</span>,</div><div class="line">                <span class="string">"rprivate"</span></div><div class="line">            ]</div><div class="line">        &#125;,</div><div class="line">        &#123;</div><div class="line">            <span class="attr">"destination"</span>: <span class="string">"/etc/hostname"</span>,</div><div class="line">            <span class="attr">"type"</span>: <span class="string">"bind"</span>,</div><div class="line">            <span class="attr">"source"</span>: <span class="string">"/var/lib/docker/containers/75599a6f387b7842c6da57efd38f9742b2ca621782f891402f83852c66dbd706/hostname"</span>,</div><div class="line">            <span class="attr">"options"</span>: [</div><div class="line">                <span class="string">"rbind"</span>,</div><div class="line">                <span class="string">"rprivate"</span></div><div class="line">            ]</div><div class="line">        &#125;,</div><div class="line">        &#123;</div><div class="line">            <span class="attr">"destination"</span>: <span class="string">"/etc/hosts"</span>,</div><div class="line">            <span class="attr">"type"</span>: <span class="string">"bind"</span>,</div><div class="line">            <span class="attr">"source"</span>: <span class="string">"/var/lib/docker/containers/75599a6f387b7842c6da57efd38f9742b2ca621782f891402f83852c66dbd706/hosts"</span>,</div><div class="line">            <span class="attr">"options"</span>: [</div><div class="line">                <span class="string">"rbind"</span>,</div><div class="line">                <span class="string">"rprivate"</span></div><div class="line">            ]</div><div class="line">        &#125;,</div><div class="line">        &#123;</div><div class="line">            <span class="attr">"destination"</span>: <span class="string">"/dev/shm"</span>,</div><div class="line">            <span class="attr">"type"</span>: <span class="string">"bind"</span>,</div><div class="line">            <span class="attr">"source"</span>: <span class="string">"/var/lib/docker/containers/d8230e57e88d15515a94138ef512a4271e31d03bb6fb257b3d57a847e70b5c68/shm"</span>,</div><div class="line">            <span class="attr">"options"</span>: [</div><div class="line">                <span class="string">"rbind"</span>,</div><div class="line">                <span class="string">"rprivate"</span></div><div class="line">            ]</div><div class="line">        &#125;</div><div class="line">    ],</div><div class="line">    <span class="attr">"hooks"</span>: &#123;&#125;,</div><div class="line">    <span class="attr">"linux"</span>: &#123;</div><div class="line">        <span class="attr">"resources"</span>: &#123;</div><div class="line">            <span class="attr">"devices"</span>: [</div><div class="line">                &#123;</div><div class="line">                    <span class="attr">"allow"</span>: <span class="literal">false</span>,</div><div class="line">                    <span class="attr">"access"</span>: <span class="string">"rwm"</span></div><div class="line">                &#125;,</div><div class="line">                &#123;</div><div class="line">                    <span class="attr">"allow"</span>: <span class="literal">true</span>,</div><div class="line">                    <span class="attr">"type"</span>: <span class="string">"c"</span>,</div><div class="line">                    <span class="attr">"major"</span>: <span class="number">1</span>,</div><div class="line">                    <span class="attr">"minor"</span>: <span class="number">5</span>,</div><div class="line">                    <span class="attr">"access"</span>: <span class="string">"rwm"</span></div><div class="line">                &#125;,</div><div class="line">                &#123;</div><div class="line">                    <span class="attr">"allow"</span>: <span class="literal">true</span>,</div><div class="line">                    <span class="attr">"type"</span>: <span class="string">"c"</span>,</div><div class="line">                    <span class="attr">"major"</span>: <span class="number">1</span>,</div><div class="line">                    <span class="attr">"minor"</span>: <span class="number">3</span>,</div><div class="line">                    <span class="attr">"access"</span>: <span class="string">"rwm"</span></div><div class="line">                &#125;,</div><div class="line">                &#123;</div><div class="line">                    <span class="attr">"allow"</span>: <span class="literal">true</span>,</div><div class="line">                    <span class="attr">"type"</span>: <span class="string">"c"</span>,</div><div class="line">                    <span class="attr">"major"</span>: <span class="number">1</span>,</div><div class="line">                    <span class="attr">"minor"</span>: <span class="number">9</span>,</div><div class="line">                    <span class="attr">"access"</span>: <span class="string">"rwm"</span></div><div class="line">                &#125;,</div><div class="line">                &#123;</div><div class="line">                    <span class="attr">"allow"</span>: <span class="literal">true</span>,</div><div class="line">                    <span class="attr">"type"</span>: <span class="string">"c"</span>,</div><div class="line">                    <span class="attr">"major"</span>: <span class="number">1</span>,</div><div class="line">                    <span class="attr">"minor"</span>: <span class="number">8</span>,</div><div class="line">                    <span class="attr">"access"</span>: <span class="string">"rwm"</span></div><div class="line">                &#125;,</div><div class="line">                &#123;</div><div class="line">                    <span class="attr">"allow"</span>: <span class="literal">true</span>,</div><div class="line">                    <span class="attr">"type"</span>: <span class="string">"c"</span>,</div><div class="line">                    <span class="attr">"major"</span>: <span class="number">5</span>,</div><div class="line">                    <span class="attr">"minor"</span>: <span class="number">0</span>,</div><div class="line">                    <span class="attr">"access"</span>: <span class="string">"rwm"</span></div><div class="line">                &#125;,</div><div class="line">                &#123;</div><div class="line">                    <span class="attr">"allow"</span>: <span class="literal">true</span>,</div><div class="line">                    <span class="attr">"type"</span>: <span class="string">"c"</span>,</div><div class="line">                    <span class="attr">"major"</span>: <span class="number">5</span>,</div><div class="line">                    <span class="attr">"minor"</span>: <span class="number">1</span>,</div><div class="line">                    <span class="attr">"access"</span>: <span class="string">"rwm"</span></div><div class="line">                &#125;,</div><div class="line">                &#123;</div><div class="line">                    <span class="attr">"allow"</span>: <span class="literal">false</span>,</div><div class="line">                    <span class="attr">"type"</span>: <span class="string">"c"</span>,</div><div class="line">                    <span class="attr">"major"</span>: <span class="number">10</span>,</div><div class="line">                    <span class="attr">"minor"</span>: <span class="number">229</span>,</div><div class="line">                    <span class="attr">"access"</span>: <span class="string">"rwm"</span></div><div class="line">                &#125;</div><div class="line">            ],</div><div class="line">            <span class="attr">"disableOOMKiller"</span>: <span class="literal">false</span>,</div><div class="line">            <span class="attr">"oomScoreAdj"</span>: <span class="number">0</span>,</div><div class="line">            <span class="attr">"memory"</span>: &#123;</div><div class="line">                <span class="attr">"kernelTCP"</span>: <span class="literal">null</span>,</div><div class="line">                <span class="attr">"swappiness"</span>: <span class="number">18446744073709551615</span></div><div class="line">            &#125;,</div><div class="line">            <span class="attr">"cpu"</span>: &#123;&#125;,</div><div class="line">            <span class="attr">"pids"</span>: &#123;</div><div class="line">                <span class="attr">"limit"</span>: <span class="number">0</span></div><div class="line">            &#125;,</div><div class="line">            <span class="attr">"blockIO"</span>: &#123;</div><div class="line">                <span class="attr">"blkioWeight"</span>: <span class="number">0</span></div><div class="line">            &#125;</div><div class="line">        &#125;,</div><div class="line">        <span class="attr">"cgroupsPath"</span>: <span class="string">"/docker/d8230e57e88d15515a94138ef512a4271e31d03bb6fb257b3d57a847e70b5c68"</span>,</div><div class="line">        <span class="attr">"namespaces"</span>: [</div><div class="line">            &#123;</div><div class="line">                <span class="attr">"type"</span>: <span class="string">"mount"</span></div><div class="line">            &#125;,</div><div class="line">            &#123;</div><div class="line">                <span class="attr">"type"</span>: <span class="string">"network"</span>,</div><div class="line">                <span class="attr">"path"</span>: <span class="string">"/proc/14702/ns/net"</span></div><div class="line">            &#125;,</div><div class="line">            &#123;</div><div class="line">                <span class="attr">"type"</span>: <span class="string">"uts"</span></div><div class="line">            &#125;,</div><div class="line">            &#123;</div><div class="line">                <span class="attr">"type"</span>: <span class="string">"pid"</span></div><div class="line">            &#125;,</div><div class="line">            &#123;</div><div class="line">                <span class="attr">"type"</span>: <span class="string">"ipc"</span></div><div class="line">            &#125;</div><div class="line">        ],</div><div class="line">        <span class="attr">"devices"</span>: [</div><div class="line">            &#123;</div><div class="line">                <span class="attr">"path"</span>: <span class="string">"/dev/zero"</span>,</div><div class="line">                <span class="attr">"type"</span>: <span class="string">"c"</span>,</div><div class="line">                <span class="attr">"major"</span>: <span class="number">1</span>,</div><div class="line">                <span class="attr">"minor"</span>: <span class="number">5</span>,</div><div class="line">                <span class="attr">"fileMode"</span>: <span class="number">438</span>,</div><div class="line">                <span class="attr">"uid"</span>: <span class="number">0</span>,</div><div class="line">                <span class="attr">"gid"</span>: <span class="number">0</span></div><div class="line">            &#125;,</div><div class="line">            &#123;</div><div class="line">                <span class="attr">"path"</span>: <span class="string">"/dev/null"</span>,</div><div class="line">                <span class="attr">"type"</span>: <span class="string">"c"</span>,</div><div class="line">                <span class="attr">"major"</span>: <span class="number">1</span>,</div><div class="line">                <span class="attr">"minor"</span>: <span class="number">3</span>,</div><div class="line">                <span class="attr">"fileMode"</span>: <span class="number">438</span>,</div><div class="line">                <span class="attr">"uid"</span>: <span class="number">0</span>,</div><div class="line">                <span class="attr">"gid"</span>: <span class="number">0</span></div><div class="line">            &#125;,</div><div class="line">            &#123;</div><div class="line">                <span class="attr">"path"</span>: <span class="string">"/dev/urandom"</span>,</div><div class="line">                <span class="attr">"type"</span>: <span class="string">"c"</span>,</div><div class="line">                <span class="attr">"major"</span>: <span class="number">1</span>,</div><div class="line">                <span class="attr">"minor"</span>: <span class="number">9</span>,</div><div class="line">                <span class="attr">"fileMode"</span>: <span class="number">438</span>,</div><div class="line">                <span class="attr">"uid"</span>: <span class="number">0</span>,</div><div class="line">                <span class="attr">"gid"</span>: <span class="number">0</span></div><div class="line">            &#125;,</div><div class="line">            &#123;</div><div class="line">                <span class="attr">"path"</span>: <span class="string">"/dev/random"</span>,</div><div class="line">                <span class="attr">"type"</span>: <span class="string">"c"</span>,</div><div class="line">                <span class="attr">"major"</span>: <span class="number">1</span>,</div><div class="line">                <span class="attr">"minor"</span>: <span class="number">8</span>,</div><div class="line">                <span class="attr">"fileMode"</span>: <span class="number">438</span>,</div><div class="line">                <span class="attr">"uid"</span>: <span class="number">0</span>,</div><div class="line">                <span class="attr">"gid"</span>: <span class="number">0</span></div><div class="line">            &#125;,</div><div class="line">            &#123;</div><div class="line">                <span class="attr">"path"</span>: <span class="string">"/dev/fuse"</span>,</div><div class="line">                <span class="attr">"type"</span>: <span class="string">"c"</span>,</div><div class="line">                <span class="attr">"major"</span>: <span class="number">10</span>,</div><div class="line">                <span class="attr">"minor"</span>: <span class="number">229</span>,</div><div class="line">                <span class="attr">"fileMode"</span>: <span class="number">438</span>,</div><div class="line">                <span class="attr">"uid"</span>: <span class="number">0</span>,</div><div class="line">                <span class="attr">"gid"</span>: <span class="number">0</span></div><div class="line">            &#125;</div><div class="line">        ],</div><div class="line">        <span class="attr">"maskedPaths"</span>: [</div><div class="line">            <span class="string">"/proc/kcore"</span>,</div><div class="line">            <span class="string">"/proc/latency_stats"</span>,</div><div class="line">            <span class="string">"/proc/timer_stats"</span>,</div><div class="line">            <span class="string">"/proc/sched_debug"</span></div><div class="line">        ],</div><div class="line">        <span class="attr">"readonlyPaths"</span>: [</div><div class="line">            <span class="string">"/proc/asound"</span>,</div><div class="line">            <span class="string">"/proc/bus"</span>,</div><div class="line">            <span class="string">"/proc/fs"</span>,</div><div class="line">            <span class="string">"/proc/irq"</span>,</div><div class="line">            <span class="string">"/proc/sys"</span>,</div><div class="line">            <span class="string">"/proc/sysrq-trigger"</span></div><div class="line">        ]</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>So, it is very clear how it works:</p>
<ul>
<li>New container mounts same network namespace <code>/proc/14702/ns/net</code></li>
<li>New container mounts same network related configs, such as <code>/etc/resolv.conf</code>, <code>/etc/hosts</code> and <code>/etc/hostname</code></li>
</ul>
<p>There is still a little problem when first container is deleted: it could be deleted without any warning, but after delete operation, the second container will become not functional:</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><div class="line">[~]<span class="comment"># docker ps</span></div><div class="line">CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES</div><div class="line">d8230e57e88d        alpine              <span class="string">"sh"</span>                14 minutes ago      Up 14 minutes                           focused_spence</div><div class="line">[~]<span class="comment"># docker exec d8230e57e88d echo aaa</span></div><div class="line">rpc error: code = 2 desc = <span class="string">"oci runtime error: exec failed: lstat /proc/14702/ns/net: no such file or directory"</span></div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Docker 1.11 has moved to runc with containerd, I am interested in how it processing shared netns accross containers.&lt;/p&gt;
&lt;p&gt;For example, 
    
    </summary>
    
    
      <category term="docker" scheme="http://feisky.xyz/tags/docker/"/>
    
  </entry>
  
</feed>
